HIBERNATE -> Java Persistance with Hibernate, Second EDITION
    (by Christian Bauer is a member of the Hibernate developer team and a trainer and consultant.
        Gavin King is the founder of the Hibernate project and a member of the Java Persistence expert group
    )

http://file.allitebooks.com/20160115/Java%20Persistence%20with%20Hibernate.pdf#page=88&zoom=100,0,190

book covers hibernate 5 with Java Persistance 2.1 standard JSR 338

============================================================================================
Part 1 - Getting started with ORM
============================================================================================	

You use SQL as a data definition language (DDL) when creating, altering, and dropping artifacts such as tables and constraints
in the catalog of the DBMS. When this schema is ready, you use SQL as a data manipulation language (DML) to perform operations
on data, including insertions, updates, and deletions.
You retrieve data by executing queries with restrictions, projections, and Cartesian products. For efficient reporting,
you use SQL to join, aggregate, and group data as necessary. You can even nest SQL statements inside each other—a
technique that uses subselects.

For several decades, developers have spoken of a paradigm mismatch. This mismatch explains why every enterprise project
expends so much effort on persistence-related concerns. The paradigms referred to are object modeling and relational
modeling, or, more practically, object-oriented programming and SQL.

	The problem of granularity
		you’ll find various problems if you check the support for userdefined data types (UDTs) in today’s SQL database
		management systems....You can consider the failure to standardize such an important piece of functionality as
		fallout from the object-relational database wars between vendors in the mid-1990s. Today, most engineers accept
		that SQL products have limited type systems—no questions asked .
	
	The problem of subtypes / inheritance
		In Java, you implement type inheritance using superclasses and subclasses. To illustrate why this can present a
		mismatch problem, let’s add to your e-commerce application so that you now can accept not only bank account
		billing, but also credit and debit cards.
		
		We aren’t finished with inheritance. As soon as we introduce inheritance into the model, we have the possibility
		of polymorphism.
		The User class has an association to the BillingDetails superclass. This is a polymorphic association. At
		runtime, a User instance may reference an instance of any of the subclasses of BillingDetails (CreditCard,
		BankAccount, and so on). Similarly, you want to be able to write polymorphic queries that refer to the
		BillingDetails class, and have the query return instances of its subclasses. SQL databases also lack an obvious
		way (or at least a standardized way) to represent a polymorphic association. A foreign key constraint refers to
		exactly one target table; it isn’t straightforward to define a foreign key that refers to multiple tables.
		
		The result of this mismatch of subtypes is that the inheritance structure in a model must be persisted in an
		SQL database that doesn’t offer an inheritance mechanism. (Fortunately, this problem is now well understood in
		the community, and most solutions support approximately the same functionality.)
	
	The problem of identity
		 Java defines two different notions of sameness:
 			Instance identity (roughly equivalent to memory location, checked with a == b)
 			Instance equality, as determined by the implementation of the equals() method (also called equality by value)
			
		Let’s use an example to discuss another problem related to database identity. In the table definition for USERS,
		USERNAME is the primary key. Unfortunately, this decision makes it difficult to change a user’s name; you need
		to update not only the row in USERS, but also the foreign key values in (many) rows of BILLINGDETAILS.
			
		To solve this problem, later in this book we recommend that you use surrogate keys whenever you can’t find a
		good natural key. We also discuss what makes a good primary key. A surrogate key column is a primary key column
		with no meaning to the application user—in other words, a key that isn’t presented to the application user.
		Its only purpose is identifying data inside the application.  (good old fashioned bigint ID :) THAT contains
		system-generated values )
			create table USERS (
				ID bigint not null primary key,
				USERNAME varchar(15) not null unique,
				...
			);
			
	Problems relating to associations 
		 Java associations can have many-to-many multiplicity. For example, the classes could look like this:
			public class User {
				Set billingDetails;
			}
			public class BillingDetails {
				Set users;
			}
		However, if you want this type of relationship in SQL, you need a new table, usually called a link table...which
		doesn't usually appear in the domain model
			create table USER_BILLINGDETAILS (
			 USER_ID bigint,
			 BILLINGDETAILS_ID bigint,
			 primary key (USER_ID, BILLINGDETAILS_ID),
			 foreign key (USER_ID) references USERS,
			 foreign key (BILLINGDETAILS_ID) references BILLINGDETAILS
			);
			
	The problem of data navigation		
		In Java, when you access a user’s billing information, you call someUser.getBillingDetails().iterator().next()
		or something similar.
		
		The single most important thing you can do to improve the performance of data access code is to minimize the
		number of requests to the database. The most obvious way to do this is to minimize the number of SQL queries.
		(Of course, other, more sophisticated, ways—such as extensive caching—follow as a second step.)
	
		Any object persistence solution worth its salt provides functionality for fetching the data of associated
		instances only when the association is first accessed in Java code.
		This is known as lazy loading: retrieving data on demand only. This piecemeal style of data access is
		fundamentally inefficient in the context of an SQL database, because it requires executing one statement for
		each node or collection of the object network that is accessed. This is the dreaded n+1 selects problem.


!!!!!	This mismatch in the way you access data in Java and in a relational database is perhaps the single most common
		source of performance problems in Java information systems. Yet although we’ve been blessed with innumerable
		books and articles advising us to use StringBuffer for string concatenation, avoiding the Cartesian product and
		n+1 selects problems is still a mystery for many Java programmers. (Admit it: you just thought StringBuilder
		would be much better than StringBuffer.)
	
        In a nutshell, object/relational mapping is the automated (and transparent) persistence of objects in a Java application
        to the tables in an SQL database, using metadata that describes the mapping between the classes of the application and
        the schema of the SQL database. In essence, ORM works by transforming (reversibly) data from one representation to another.

        we believe that Java developers must have a sufficient level of familiarity with—and appreciation of—relational modeling
        and SQL in order to work with Hibernate. ORM is an advanced technique used by developers who have already done it the
        hard way.
!!!!    To use Hibernate effectively, you must be able to view and interpret the SQL statements it issues and
        understand their performance implications.

 Let’s look at some of the benefits of Hibernate:
	 Productivity—Hibernate eliminates much of the grunt work (more than you’d
	expect) and lets you concentrate on the business problem. No matter which
	application-development strategy you prefer—top-down, starting with a domain
	model, or bottom-up, starting with an existing database schema—Hibernate,
	used together with the appropriate tools, will significantly reduce development time.
	
	 Maintainability —Automated ORM with Hibernate reduces lines of code (LOC),
	making the system more understandable and easier to refactor. Hibernate provides a
	buffer between the domain model and the SQL schema, insulating each model
	from minor changes to the other.
	
	 Performance—Although hand-coded persistence might be faster in the same
	sense that assembly code can be faster than Java code, automated solutions like
	Hibernate allow the use of many optimizations at all times. One example of this
	is efficient and easily tunable caching in the application tier. This means developers
	can spend more energy hand-optimizing the few remaining real bottlenecks
	instead of prematurely optimizing everything.
	
	 Vendor independence—Hibernate can help mitigate some of the risks associated
	with vendor lock-in. Even if you plan never to change your DBMS product, ORM
	tools that support a number of different DBMSs enable a certain level of portability.
	In addition, DBMS independence helps in development scenarios where engineers
	use a lightweight local database but deploy for testing and production on a
	different system. 


We should be clear up front that neither Java Persistence nor Hibernate are limited to the Java EE environment; they’re
general-purpose solutions to the persistence problem that any type of Java (or Groovy, or Scala) application can use.
 The JPA specification defines the following:
	 A facility for specifying mapping metadata—how persistent classes and their
	properties relate to the database schema. JPA relies heavily on Java annotations
	in domain model classes, but you can also write mappings in XML files.
	
	 APIs for performing basic CRUD operations on instances of persistent classes,
	most prominently javax.persistence.EntityManager to store and load data.
	
	 A language and APIs for specifying queries that refer to classes and properties of
	classes. This language is the Java Persistence Query Language (JPQL) and looks
	similar to SQL. The standardized API allows for programmatic creation of criteria
	queries without string manipulation.
	
	 How the persistence engine interacts with transactional instances to perform
	dirty checking, association fetching, and other optimization functions. The latest
	JPA specification covers some basic caching strategies.

Hibernate implements JPA and supports all the standardized mappings, queries, and programming interfaces. 


Introducing Hibernate
Today, Hibernate is not only an ORM service, but also a collection of data management tools extending well beyond ORM.
		
	Hibernate ORM—Hibernate ORM consists of a core, a base service for persistence with SQL databases, and a native proprietary API.
	Hibernate ORM is the foundation for several of the other projects and is the oldest Hibernate project. You can use Hibernate
	ORM on its own, independent of any framework or any particular runtime environment with all JDKs. It works in every
	Java EE/J2EE application server, in Swing applications, in a simple servlet container, and so on. As long as you can
	configure a data source for Hibernate, it works.
	
	Hibernate EntityManager—This is Hibernate’s implementation of the standard Java Persistence APIs, an optional module you can
	stack on top of Hibernate ORM. You can fall back to Hibernate when a plain Hibernate interface or even a JDBC Connection
	is needed. Hibernate’s native features are a superset of the JPA persistence features in every respect.
	
	Hibernate Validator—Hibernate provides the reference implementation of the Bean Validation (JSR 303) specification. Independent
	of other Hibernate projects, it provides declarative validation for your domain model (or any other) classes
	
	Hibernate Envers—Envers is dedicated to audit logging and keeping multiple versions of data in your SQL database. This helps you add
	data history and audit trails to your application, similar to version control systems you might already be familiar with such as
	Subversion and Git
	
	Hibernate Search—Hibernate Search keeps an index of your domain model data up to date in an Apache Lucene database. It lets you query
	this database with a powerful and naturally integrated API. Many projects use Hibernate Search in addition to Hibernate ORM, adding
	full-text search capabilities. If you have a free text search form in your application’s user interface, and you want happy users, work
	with Hibernate Search. Hibernate Search isn’t covered in this book; you can find more information in Hibernate Search in Action
	by Emmanuel Bernard (Bernard, 2008).
	
	Hibernate OGM—The most recent Hibernate project is the object/grid mapper. It provides JPA support for NoSQL solutions, reusing the Hibernate core engine
	but persisting mapped entities into a key/value-, document-, or graph-oriented data store. Hibernate OGM isn’t covered in this book
	
	
Hello world example

	Your starting point in JPA is the persistence unit. A persistence unit is a pairing of your domain model class mappings with a
	database connection, plus some other configuration settings. Every application has at least one persistence unit; some applications
	have several if they’re talking to several (logical or physical) databases. Hence, your first step is setting up a persistence unit
	in your application’s configuration.
		
	1.Every persistent entity class must have at least the @Entity annotation. Hibernatemaps this class to a table called MESSAGE.
	2.Every persistent entity class must have an identifier attribute annotated with @Id. Hibernate maps this attribute to a column named ID.
	3.Someone must generate identifier values; this annotation enables automatic generation of IDs (@GeneratedValue)
		
	Instances of the Message class (our @Entity example class) can be managed (made persistent) by Hibernate, but they don’t have to be.
	Because the Message object doesn’t implement any persistence specific classes or interfaces, you can use it just like any other
	Java class:
		Message msg = new Message();
		msg.setText("Hello!");
		System.out.println(msg.getText());	
		
	Every interaction with your database should occur within explicit transaction boundaries, even if you’re only reading data.
		
	// SELECT * from MESSAGE	
	List<Message> emails = em.createQuery("select m from Message m").getResultList();
	assertEquals(emails.size(), 1);
	assertEquals(emails.get(0).getText(), "Hello World!");
	emails.get(0).setText("Take me to your leader!");

	You can change the value of a property. Hibernate detects this automatically because the loaded Message is still
	attached to the persistence context it was loaded in.
	On commit, Hibernate checks the persistence context for dirty state and executes the SQL UPDATE automatically to
	synchronize in-memory with the database state.
		
	The query language you’ve seen in this example isn’t SQL, it’s the Java Persistence Query Language (JPQL). Although
	there is syntactically no difference in this trivial example, the Message in the query string doesn’t refer to the
	database table name, but to the persistent class name. If you map the class to a different table, the query
	will still work. 
	
	
Native Hibernate configuration

!!!!! This SessionFactory approach is used in our TNG app as well !!!!

	The native equivalent of the standard JPA EntityManagerFactory is the org.hibernate.SessionFactory. You have usually
	one per application, and it’s the same pairing of class mappings with database connection configuration.
	
	 In its most compact form, building a SessionFactory looks like this: 
	 
		SessionFactory sessionFactory = new MetadataSources(
			 new StandardServiceRegistryBuilder()
			 .configure("hibernate.cfg.xml").build()
			).buildMetadata().buildSessionFactory();
			
	This loads all settings from a Hibernate configuration file. If you have an existing Hibernate project, you most
	likely have this file on your classpath. Similar to persistence.xml, this configuration file contains database
	connection details, as well as a list of persistent classes and other configuration properties.
	
	Storing and loading a item with Hibernate’s native equivalent of EntityManager is done with the org.hibernate.Session.
	You can create a Session with the SessionFactory, and you must close it just as you have to close your own EntityManager.

	!!!	Or, using another Hibernate feature, you can let Hibernate take care of creating and closing the Session with
	SessionFactory.getCurrentSession() .Whenever you call getCurrentSession() in the same thread, you get the same
	org.hibernate.Session. It’s bound automatically to the ongoing transaction and is closed for you automatically when
	that transaction commits or rolls back .
	
	The native Hibernate API is very similar to the standard Java Persistence API, and most methods have the same names.
	Hibernate synchronizes the session with the database and automatically closes the “current” session on commit of the
	bound transaction.
	
	A Hibernate criteria query is a type-safe programmatic way to express queries, automatically translated into SQL.
	
	Most of the examples in this book don’t use the SessionFactory or Session API.
	From time to time, when a particular feature is only available in Hibernate, we show you how to unwrap() the native
	interface given a standard API.
	
-------------------------------------
Domain models and metadata	
------------------------------------

Bean Validation -> helps to automatically verify the integrity of the domain model data not only for persistent
information but all business logic.
	
A layered architecture
	With any nontrivial application, it usually makes sense to organize classes by concern. Persistence is one concern; others
	include presentation, workflow, and business logic. A typical object-oriented architecture includes layers of code that represent the concerns.
	
	A layered architecture defines interfaces between code that implements the various concerns, allowing changes to be made to
	the way one concern is implemented without significant disruption to code in the other layers. Layering determines the
	kinds of inter-layer dependencies that occur. The rules are as follows:
		 Layers communicate from top to bottom. A layer is dependent only on the interface of the layer directly below it.
		 Each layer is unaware of any other layers except for the layer just below it.
		
	Figure of a typical high level application, and its 3 layers  (I flipped it horizontally here, so that it can easily be seen)
								_____________________________________________
								|		Domain Model(all 3 layers use this)	|
						    	|___________________________________________| 		
							  /                     |                     \
							 /                      |					   \
							/						|						\
		Database <---- Persistence Layer <---- Business Layer <---- Presentation Layer
							\						|						/
							 \						|					   /
							  \						|					  /
							   |------------------------------------------|
							   |	     (all 3 layers use these)		  |
							   |   Interceptors, utility & helper classes |
							   --------------------------------------------	 	
	
	
	
	Presentation layer — The user interface logic is topmost. Code responsible for the presentation and control of page and screen navigation is in the presentation
		layer. The user interface code may directly access business entities of the shared domain model and render them on the screen, along with controls to execute
		actions. In some architectures, business entity instances might not be directly accessible by user interface code: for example, if the presentation layer isn’t
		running on the same machine as the rest of the system. In such cases, the presentation layer may require its own special data-transfer model, representing
		only a transmittable subset of the domain model. 
	
	Business layer—The exact form of the next layer varies widely between applications. It’s generally agreed that the business layer is responsible for implementing
		any business rules or system requirements that would be understood by users as part of the problem domain. This layer usually includes some kind of
		controlling component—code that knows when to invoke which business rule. In some systems, this layer has its own internal representation of the business
		domain entities. Alternatively, it relies on a domain model implementation, shared with the other layers of the application.
		
	Persistence layer—The persistence layer is a group of classes and components responsible for storing data to, and retrieving it from, one or more data stores.
		This layer needs a model of the business domain entities for which you’d like to keep persistent state. The persistence layer is where the bulk of JPA and Hibernate
		use takes place.
	
	Database—The database is usually external, shared by many applications. It’s the actual, persistent representation of the system state. If an SQL database is
		used, the database includes a schema and possibly stored procedures for execution of business logic close to the data.
	
	Helper and utility classes—Every application has a set of infrastructural helper or utility classes that are used in every layer of the application (such as Exception
		classes for error handling). These shared infrastructural elements don’t form a layer because they don’t obey the rules for inter-layer dependency in a layered
		architecture. 	
		
Analyzing the business domain

	Now that you have a high-level architecture, you can focus on the business problem. 

	At this stage, you, with the help of domain experts, analyze the business problems your software system needs to solve,
	identifying the relevant main entities and their interactions. The motivating goal behind the analysis and design of
	a domain model is to capture the essence of the business information for the application’s purpose.
		
	Entities are usually notions understood by users of the system: payment, customer, order, item, bid, and so forth. Some
	entities may be abstractions of less concrete things the user thinks about, such as a pricing algorithm, but even these
	are usually understandable to the user. You can find all these entities in the conceptual view of the business, sometimes
	called a BUSINESS MODEL.
	From this business model, engineers and architects of object-oriented software create an object-oriented model, still at
	the conceptual level (no Java code). This model may be as simple as a mental image existing only in the mind of the
	developer, or it may be as elaborate as a UML class diagram.
		
	Entities that are of interest to the user represent the DOMAIN MODEL. It’s an abstract view of the real world. 
	
	In the end, what modeling language you use is secondary (object-oriented model or entity-relationship diagram); we’re
	most interested in the structure and relationships of the business entities.
	
	
	ORM without a domain model :
	Object persistence with full ORM is most suitable for applications based on a rich domain model. If your application
	doesn’t implement complex business rules or complex interactions between entities (or if you have few entities),
	you may not need a domain model. Many simple and some not-so-simple problems are perfectly suited to table-oriented
	solutions, where the application is designed around the database data model instead of around an object-oriented domain
	model, often with logic executed in the database (stored procedures). Another aspect to consider is the learning
	curve: once you’re proficient with Hibernate, you’ll use it for all applications, even as a simple SQL query generator
	and result mapper. If you’re just learning ORM, a trivial use case may not justify your invested time and overhead.
	(remmber the nail and hammer problem ?:) )
	

Addressing leakage of concerns

!!!!
	When concerns such as persistence, transaction management, or authorization start to appear in the domain model classes,
	this is an example of leakage of concerns. The domain model implementation is such an important piece of code that
	it shouldn’t depend on orthogonal Java APIs. For example, code in the domain model shouldn’t
	perform JNDI lookups or call the database via the JDBC API, not directly and not through an intermediate abstraction.
	This allows you to reuse the domain model classes virtually anywhere:
		 The presentation layer can access instances and attributes of domain model entities when rendering views.
		 The controller components in the business layer can also access the state of domain model entities and call
		 methods of the entities to execute business logic.
		 The persistence layer can load and store instances of domain model entities from and to the database, preserving
		their state.

	Most important, preventing leakage of concerns makes it easy to unit-test the domain model without the need for a
	particular runtime environment or container, or the need for mocking any service dependencies. You can write unit
	tests that verify the correct behavior of your domain model classes without any special test harness. (We
	aren’t talking about testing “load from the database” and “store in the database” aspects, but “calculate the shipping
	cost and tax” behavior.)

Transparent and automated persistence

	JPA defines the entity class as the primary programming artifact. This programming model enables transparent persistence,
	and a JPA provider such as Hibernate also offers automated persistence.
	
	We use transparent to mean a complete separation of concerns between the persistent classes of the domain model and the
	persistence layer. The persistent classes are unaware of—and have no dependency on—the persistence mechanism. We use
	automatic to refer to a persistence solution (your annotated domain, the layer, and mechanism) that relieves you of
	handling low-level mechanical details, such as writing most SQL statements and working with the JDBC API.
	
	 The Item class of the CaveatEmptor domain model( our book example) shouldn’t have any runtime dependency on any Java
	 Persistence or Hibernate API. Furthermore:
		 JPA doesn’t require that any special superclasses or interfaces be inherited or implemented by persistent classes.
		Nor are any special classes used to implement attributes and associations. (Of course, the option to use both
		techniques is always there.)
		 You can reuse persistent classes outside the context of persistence, in unit tests or in the presentation layer,
		for example. You can create instances in any runtime environment with the regular Java "new" operator, preserving
		testability and reusability.
		 In a system with transparent persistence, instances of entities aren’t aware of the underlying data store; they
		need not even be aware that they’re being persisted or retrieved. JPA externalizes persistence concerns to a generic
		persistence manager API.
		 Hence, most of your code, and certainly your complex business logic, doesn’t have to concern itself with the
		current state of a domain model entity instance in a single thread of execution
	
	We regard transparency as a requirement because it makes an application easier to build and maintain. Transparent
	persistence should be one of the primary goals of any ORM solution.

	JPA requires that collectionvalued attributes be typed to an interface such as java.util.Set or java.util.List
    and not to an actual implementation such as java.util.HashSet (this is a good practice anyway).
	
	Our preferred programming model to archive this is POJO.
	

Writing persistence-capable classes

	JPA doesn’t require that persistent classes implement java.io.Serializable. But when instances are stored in an
	HttpSession or passed by value using RMI, serialization is necessary. Although this might not occur in your application,
	 the class will be serializable without any additional work, and there are no downsides to declaring that.
	(We aren’t going to declare it on every example, assuming that you know when it will be necessary.) 

!!!!!
	The class can be "abstract" and, if needed, extend a non-persistent class or implement an interface. 
	It must be a top-level class, not nested within another class. 
	The persistence-capable class and any of its methods can’t be final (a requirement of the JPA specification). 
	 
	Hibernate (and JPA) require a constructor with no arguments for every persistent class. Alternatively, you might not
	write a constructor at all; Hibernate will then use the Java default constructor. Hibernate calls classes using the
	Java reflection API on such a noargument constructor to create instances.
!!!!!
	
	Hibernate doesn’t require accessor methods (getters & setters). You can choose how the state of an instance of your
	persistent classes should be persisted. Hibernate will either directly access fields or call accessor methods. Your
	class design isn’t disturbed much by these considerations. You can make some accessor methods non-public or completely
	remove them—then configure Hibernate to rely on field access for these properties.
	Although trivial accessor methods are common, one of the reasons we like to use JavaBeans-style accessor methods is that
	they provide encapsulation: you can change the hidden internal implementation of an attribute without any changes to
	the public interface. If you configure Hibernate to access attributes through methods, you
	abstract the internal data structure of the class—the instance variables—from the design of the database.
 
	For example, if your database stores the name of a user as a single NAME column, but your User class has firstname
	and lastname fields, you can add the following persistent name property to the class.

		public class User {
		 protected String firstname;
		 protected String lastname;
		 
		 public String getName() {
			return firstname + ' ' + lastname;
		 }
		 
		 public void setName(String name) {
			StringTokenizer t = new StringTokenizer(name);
			firstname = t.nextToken();
			lastname = t.nextToken();
		 }
		}

!!!!!
	DIRTY CHECKING is very important to consider. Hibernate automatically detects state changes in order to synchronize
	the updated state with the database. It’s usually safe to return a different instance from the getter method than
	the instance passed by Hibernate to the setter. Hibernate compares them by value—not by object identity—
	to determine whether the attribute’s persistent state needs to be updated. For example, the following getter method
	doesn’t result in unnecessary SQL UPDATEs:
		
		public String getFirstname() {
			return new String(firstname);		// this is ok :)
		}
	There is one important exception to this: collections are COMPARED BY IDENTITY ! For a property mapped as a persistent
	collection, you should return exactly the same collection instance from the getter method that Hibernate passed to the
	setter method. If you don’t, Hibernate will update the database, even if no update is necessary, every time the state
	held in memory is synchronized with the database. You should usually avoid this kind of code in accessor methods:
		
		protected String[] names = new String[0];
		public void setNames(List<String> names) {
		 this.names = names.toArray(new String[names.size()]);
		}
		public List<String> getNames() {
		 return Arrays.asList(names);		// THIS IS NOT Ok if hibernate accesses the methods...!!!!
		}	
		
	Of course, this won’t be a problem if Hibernate is accessing the names field directly, ignoring your getter and setter methods. 
!!!!!

	How does Hibernate handle exceptions when your accessor methods throw them? If Hibernate uses accessor methods when
	loading and storing instances and a RuntimeException (unchecked) is thrown, the current transaction is rolled back,
	and the exception is yours to handle in the code that called the Java Persistence (or Hibernate native) API. If you
	throw a checked application exception, Hibernate wraps the exception into a RuntimeException.
		
	
Implementing POJO associations	
	
	You’ll now see how to associate and create different kinds of relationships between objects: one-to-many, many-to-one,
	and bidirectional relationships.

		ITEM
	name : String
	description : String
	createdOn : Date
	verified : boolean
	auctionType : AuctionType
	initialPrice : BigDecimal
	auctionStart : Date
	auctionEnd : Date

		(0 to many relationship)
		BID
	amount : BigDecimal
	createdOn : Date

	As with all of our UML class diagrams, we left out the association-related attributes, Item#bids and Bid#item. These
	properties and the methods that manipulate their values are called scaffolding code.
	This is what the scaffolding code for the Bid class looks like:	
	
	public class Bid {
	 protected Item item;
	 
	 public Item getItem() {
		return item;
	 }
	 
	 public void setItem(Item item) {
		this.item = item;
	 }
	}
	The item property allows navigation from a Bid to the related Item. This is an association with many-to-one multiplicity;
	users can make many bids for each item. Here is the Item class’s scaffolding code:

	public class Item {
	 protected Set<Bid> bids = new HashSet<Bid>();			// at page 72 (as numbered in the PDF, not written on the page) in the book we get an explanation why a Set and not a List was used 
															// in a few words, the order(obtained from the List) is not important when persisting data in the DB..we can always sort data by
	 public Set<Bid> getBids() {							// the bid's date & time when displaying it to the user or when doing the retrieve from the DB...
		return bids;
	 }
	 
	 public void setBids(Set<Bid> bids) {
		this.bids = bids;
	 }
	}

	This association between the two classes allows bidirectional navigation: the many-to-one is from this perspective a
	one-to-many multiplicity (again, one item can have many bids). The scaffolding code for the bids property uses a
	collection interface type, java.util.Set. JPA requires interfaces for collection-typed properties, where you
	must use java.util.Set, java.util.List, or java.util.Collection rather than HashSet, for example. 

!!!!
	It’s good practice to program to collection interfaces anyway, rather than concrete implementations, so this restriction
	shouldn’t bother you.
	
!!!!
	The JPA provider (for example Hibernate) is also required to set a non-empty value on any mapped collection valued
	property (for "bids" in our example above): for example, when an Item without bids is loaded from the database.
	(It doesn’t have to use a HashSet; the implementation is up to the provider. Hibernate has its own collection
	implementations with additional capabilities—for example, dirty checking.)
	
	managing the link between an Item and a Bid is much more complicated in Java code than it is in an SQL database, with
	declarative foreign key constraints. In our experience, engineers are often unaware of this complication arising from a
	 network object model with bidirectional references (pointers)
	
	 The basic procedure for linking a Bid with an Item looks like this:
		anItem.getBids().add(aBid);
		aBid.setItem(anItem);
	Whenever you create this bidirectional link, two actions are required:
	 You must add the Bid to the bids collection of the Item.
	 The item property of the Bid must be set. 
	
!!!!
	JPA doesn’t manage persistent associations. If you want to manipulate an association, you must write exactly the same
	code you would write without Hibernate. If an association is bidirectional, you must consider both sides of the
	relationship. If you ever have problems understanding the behavior of associations in JPA, just ask yourself,
	“What would I do without Hibernate?” Hibernate doesn’t change the regular Java semantics. 
		
	public void addBid(Bid bid) {
	 if (bid == null)
		throw new NullPointerException("Can't add null Bid");
	 if (bid.getItem() != null)
		throw new IllegalStateException("Bid is already assigned to an Item");
	 
	 getBids().add(bid);
	 bid.setItem(this);
	 
	}
	
	The addBid() method not only reduces the lines of code when dealing with Item and Bid instances, but also enforces the
	cardinality of the association. You avoid errors that arise from leaving out one of the two required actions.  If you
	compare this with the relational model of foreign keys in an SQL database, you can easily see how a network
	and pointer model complicates a simple operation: instead of a declarative constraint, you need procedural code to
	guarantee data integrity.
	
	The Item#getBids() getter method still returns a modifiable collection, so clients can use it to make changes that aren’t
	reflected on the inverse side. Bids added directly to the collection wouldn’t have a reference to an item—an inconsistent
	state, according to your database constraints. To prevent this, you can wrap the internal collection before returning
	it from the getter method, with Collections.unmodifiableCollection(c) and Collections.unmodifiableSet(s). The client
	then gets an exception if it tries to modify the collection; you therefore force every modification to go through the
	relationship management method that guarantees integrity. Note that in this case you’ll have to configure Hibernate for
	field access, because the collection returned by the getter method is then not the same as the one given to the setter
	method.
	
	In the examples in this book, we’ll sometimes write scaffolding methods such as the Item#addBid() shown earlier, or we
	may have additional constructors for required values. It’s up to you how many convenience methods and layers you want
	to wrap around the persistent association properties and/or fields, but we recommend being consistent and applying the
	same strategy to all your domain model classes.
	
	
Domain model metadata	
	
	Metadata is data about data, so domain model metadata is information about your domain model. For example, when you use
	the Java reflection API to discover the names of classes of your domain model or the names of their attributes, you’re
	accessing domain model metadata. ORM tools also require metadata, to specify the mapping between classes and tables,
	properties and columns, associations and foreign keys, Java types and SQL types, and so on.
	
	JPA standardizes two metadata options: annotations in Java code and externalized XML descriptor files. Hibernate has
	some extensions for native functionality, also available as annotations and/or XML descriptors. Usually we prefer
	EITHER annotations OR XML files as the primary source of mapping metadata. After reading this section, you’ll have the
	background information to make an educated decision for your own project.

!!	Most engineers today prefer Java annotations as the primary mechanism for declaring metadata.
	
	
Annotation-based metadata

	The big advantage of annotations is to put metadata next to the information it describes, instead of separating it
	physically into a different file

	import javax.persistence.Entity;
	@Entity
	public class Item {
	}
	
	This example declares the Item class as a persistent entity using the @javax.persistence.Entity annotation. All of
	its attributes are now automatically persistent with a default strategy. That means you can load and store instances
	of Item, and all properties of the class are part of the managed state. (YOU ALSO NEED to add an identifier property...)

	The JPA metadata is included in the compiled class files. Hibernate then reads the classes and metadata with Java
	reflection when the application starts. The IDE can also easily validate and highlight annotations—they’re regular Java
	types, after all. Most development tools and editors can’t refactor XML element and attribute values, but annotations
	are part of the Java language and are included in all refactoring operations.
	
	Is my class now dependent on JPA?
	Yes, but it’s a compile-time only dependency(SEE Question 2). You need JPA libraries on your classpath when compiling
	the source of your domain model class. The Java Persistence API isn’t required on the classpath when you create an
	instance of the class: for example, in a desktop client application that doesn’t execute any JPA code. Only when you
	access the annotations through reflection at runtime (as Hibernate does internally when it reads your metadata) will
	you need the packages on the classpath.

	When the standardized Java Persistence annotations are insufficient, a JPA provider may offer additional annotations.
	...for example some performance-tuning options you’d expect to be available in high-quality persistence software are
	only available as Hibernate-specific annotations.
	
	
USING VENDOR EXTENSIONS	
	
	import javax.persistence.Entity;
	@Entity
	@org.hibernate.annotations.Cache(
	 usage = org.hibernate.annotations.CacheConcurrencyStrategy.READ_WRITE
	)
	public class Item {
	}
	
	We prefer to prefix Hibernate annotations with the full org.hibernate.annotations package name. Consider this good
	practice, because you can easily see what metadata for this class is from the JPA specification and which is
	vendor-specific. You can also easily search your source code for “org.hibernate.annotations” and get a complete
	overview of all nonstandard annotations in your application in a single search result.
	
	
GLOBAL ANNOTATION METADATA

	Annotations on classes only cover metadata that is applicable to that particular class. You often need metadata at a
	higher level, for an entire package or even the whole application (example : the @Entity annotation maps a particular class.)
	
	JPA and Hibernate also have annotations for global metadata. For example, a @NamedQuery has global scope; you
	don’t apply it to a particular class. Where should you place this annotation?
	Although it’s possible to place such global annotations in the source file of a class (any class, really, at the top),
	we’d rather keep global metadata in a separate file. Package-level annotations are a good choice; they’re in a file
	called package-info.java in a particular package directory.
	
	@org.hibernate.annotations.NamedQueries({
		@org.hibernate.annotations.NamedQuery(
		 name = "findItemsOrderByName",
		 query = "select i from Item i order by i.name asc"
		)
		,
		@org.hibernate.annotations.NamedQuery(
		 name = "findItemBuyNowPriceGreaterThan",
		 query = "select i from Item i where i.buyNowPrice > :price",
		 timeout = 60,
		 comment = "Custom SQL comment"
		)
	})
	
	package org.jpwh.model.querying;
	
	The syntax of this file with the package and import declarations at the bottom is probably new to you. There is a reason
	the previous code example only includes annotations from the Hibernate package and no Java Persistence annotations.
	We ignored the standardized JPA @org.javax.persistence.NamedQuery annotation and used the Hibernate alternative.
	The JPA annotations don’t have package applicability—we don’t know why. In fact, JPA doesn’t allow annotations in a
	package-info.java file. The native Hibernate annotations offer the same, and sometimes even more, functionality, so this
	shouldn’t be too much of a problem. If you don’t want to use the Hibernate annotations, you’ll have to either put the
	JPA annotations at the top of any class (you could have an otherwise empty MyNamedQueries class as part of your domain model)
	
	
Applying Bean Validation Rules

!!!!
	The idea behind Bean Validation is that declaring rules such as “This property can’t be null” or “This number has to be
	in the given range” is much easier and less error-prone than writing if-then-else procedures repeatedly. Furthermore,
	declaring these rules on the central component of your application, the domain model implementation, enables integrity
	checks in every layer of the system. The rules are then available to the presentation and persistence layers.
	
	import javax.validation.constraints.Future;
	import javax.validation.constraints.NotNull;
	import javax.validation.constraints.Size;

	@Entity
	public class Item {
		@NotNull
		@Size(
			min = 2,
			max = 255,
			item = "Name is required, maximum 255 characters."
		)
		protected String name;
	
		@Future
		protected Date auctionEnd;
	}
	
	You want to guarantee that the name is always present and human readable (one-character item names don’t make much sense),
	but it shouldn’t be too long—your SQL database will be most efficient with variable-length strings up to 255 characters,
	and your user interface also has some constraints on visible label space. The ending time of an auction
	obviously should be in the future. If you don’t provide an error item, a default item will be used.
	The validation engine will access the fields directly if you annotate the fields. If you prefer calls through accessor
	methods, annotate the getter method with validation constraints, not the setter.
	
	Bean Validation isn’t limited to the built-in annotations; you can create your own constraints and annotations. With a
	custom constraint, you can even use class-level annotations and validate several attribute values at the same time on
	an instance of the class.
	
	If you want to manually check the integrity of an Item instance, you can use this tutorial : https://www.baeldung.com/javax-validation
	You’ll rarely write this kind of validation code; most of the time, this aspect is automatically handled by your user
	interface and persistence framework.  It’s therefore important to look for Bean Validation integration when selecting
	a UI framework. JSF version 2 and newer automatically integrates with Bean Validation, for example.
		ValidatorFactory factory = Validation.buildDefaultValidatorFactory();
		Validator validator = factory.getValidator();

		Item item = new Item();
		item.setName("Some Item");
		item.setAuctionEnd(new Date());
		Set<ConstraintViolation<Item>> violations = validator.validate(item);
	
	Hibernate will only validate if it finds a Bean Validation provider (such as Hibernate Validator) on the classpath of the
	running application. You can control this behavior of Hibernate with the <validation-mode> element in your persistence.xml
	configuration file. The default mode is AUTO.
		 You don’t have to manually validate instances (like in the code from above with the ValidatorFactory) before passing
		 them to Hibernate for storage.
		 Hibernate recognizes constraints on persistent domain model classes and triggers validation before database insert
		or update operations. When validation fails, Hibernate throws a ConstraintViolationException, containing the failure
		details, to the code calling persistence-management operations.
		 The Hibernate toolset for automatic SQL schema generation understands many constraints and generates SQL
		DDL-equivalent constraints for you. For example, an @NotNull annotation translates into an SQL NOT NULL constraint,
		and an @Size(n) rule defines the number of characters in a VARCHAR(n)-typed column.


Externalizing metadata with XML files

	You can replace or override every annotation in JPA with an XML descriptor element. In other words, you don’t have to
	use annotations if you don’t want to, or if keeping mapping metadata separate from source code is for whatever reason
	advantageous to your system design.
	
	The following listing shows a JPA XML descriptor for a particular persistence unit
	
	The JPA provider automatically picks up this descriptor if you place it in a METAINF/orm.xml file on the classpath of
	the persistence unit. If you prefer to use a different name or several files, you’ll have to change the configuration
	of the persistence unit in your META-INF/persistence.xml file:
			<persistence-unit name="SimpleXMLCompletePU">
			 ...
			 <mapping-file>simple/Mappings.xml</mapping-file>
			 <mapping-file>simple/Queries.xml</mapping-file>
			 ...
			</persistence-unit>
	If you include the <xml-mapping-metadata-complete> element, the JPA provider ignores all annotations on your domain model
	classes in this persistence unit and relies only on the mappings as defined in the XML descriptor(s).
	
	 Instead, if you don’t want to ignore but override the annotation metadata, don’t mark the XML descriptors as
	 “complete”, and name the class and property to override:
		<entity class="org.jpwh.model.simple.Item">
		 <attributes>
		 <basic name="name">
		 <column name="ITEM_NAME"/>
		 </basic>
		 </attributes>
		</entity>
	We won’t talk much about JPA XML descriptors in this book. The syntax of these documents is a 1:1 mirror of the JPA
	annotation syntax, so you shouldn’t have any problems writing them. We’ll focus on the important aspect: the mapping
	strategies. The syntax used to write down metadata is secondary.
	
HIBERNATE XML MAPPING FILES

	The native Hibernate XML mapping file format was the original metadata option before JDK 5 introduced annotations.
	By convention, you name these files with the suffix .hbm.xml.
	
		<?xml version="1.0"?>
		<hibernate-mapping xmlns="http://www.hibernate.org/xsd/orm/hbm" package="org.jpwh.model.simple" default-access="field">
		 <class name="Item">
			<id name="id">
				<generator class="native"/>
			</id>
			<property name="name"/>
			<property name="auctionEnd" type="timestamp"/>
		 </class>
		 
		 <query name="findItemsHibernate">select i from Item i</query>
		 
		 <database-object>
			<create>create index ITEM_NAME_IDX on ITEM(NAME)</create>
			<drop>drop index if exists ITEM_NAME_IDX</drop>
		 </database-object>
		</hibernate-mapping>
		
		1. Metadata is declared in a <hibernate-mapping> root element. Attributes such as package name and default-access
		apply to all mappings in this file. You may include as many entity class mappings as you like.
!!!!	Although it’s possible to declare mappings for multiple classes in one mapping file by using multiple <class>
        elements, many older Hibernate projects are organized with one mapping file per persistent class. The convention is
        to give the file the same name and package as the mapped class: for example, my/model/Item.hbm.xml for the
        my.model.Item class.
			
!!		A class mapping in a Hibernate XML document is a “complete” mapping; that is, any other mapping metadata for that
        class, whether in annotations or JPA XML files, will trigger a “duplicate mapping” error on startup. If you map a
        class in a Hibernate XML file, this declaration has to include all mapping details. You can’t override individual
        properties or extend an existing mapping. In addition, you have to list and map all persistent properties of an
        entity class in a Hibernate XML file. If you don’t map a property, Hibernate considers it transient state.
        Compare this with JPA mappings, where the @Entity annotation alone will make all properties of a class persistent.
			
	    Hibernate native XML files are no longer the primary choice for declaring the bulk of a project’s ORM metadata.
	    Most engineers now prefer annotations. Native XML metadata files are mostly used to gain access to special
	    Hibernate features that aren’t available as annotations or are easier to maintain in XML files (for example,
	    because it’s deployment-dependent configuration metadata). When we later discuss such advanced and native
	    Hibernate features, we’ll show you how to declare them in Hibernate XML files.
	
	
Accessing metadata at runtime

	The JPA specification provides programming interfaces for accessing the metamodel of persistent classes. There are two
	flavors of the API. One is more dynamic in nature and similar to basic Java reflection. The second option is a static
	metamodel, typically produced by a Java 6 annotation processor. For both options, access is read-only; you
	can’t modify the metadata at runtime.

	Sometimes—for example, when you want to write some custom validation or generic UI code—you’d like to get programmatic
	access to the persistent attributes of an entity. You’d like to know what persistent classes and attributes your
	domain model has dynamically.  (see examples at page 55 in the book..)

	What is the annotation processing tool (apt)?
		Java includes the command-line utility apt, or annotation processing tool, which finds and executes annotation
		processors based on annotations in source code. An annotation processor uses reflection APIs to process program
		annotations (JSR 175). The apt APIs provide a build-time, source file, and read-only view of programs to model
		the Java type system. Annotation processors may first produce new source code and files, which apt can then
		compile along with the original source.
	
	

============================================================================================
Part 2 - Mapping Strategies
============================================================================================	
	This part is all about actual ORM, from classes and properties to tables and columns. 
		Chapter 4 starts with regular class and property mappings and explains how you can map fine-grained Java domain models
		Chapter 5, you’ll see how to map basic properties and embeddable components, and how to control mapping between
		Java and SQL types
		Chapter 6, you’ll map inheritance hierarchies of entities to the database using four basic inheritance-mapping
		strategies; you’ll also map polymorphic associations
		Chapter 7 is all about mapping collections and entity associations: you map persistent collections, collections of
		basic and embeddable types, and simple many-to-one and one-to-many entity associations.
		Chapter 8 dives deeper with advanced entity association mappings like mapping one-to-one entity associations,
		one-to-many mapping options, and many-to-many and ternary entity relationships
		Chapter 9 is the most interesting if you need to introduce Hibernate in an existing application, or if you have to
		work with legacy database schemas and handwritten SQL. We’ll also talk about customized SQL DDL for schema generation
		in this chapter.


Mapping persistend classes

Fine-grained domain models	
	A major objective of Hibernate is support for fine-grained and rich domain models. It’s one reason we work with POJOs.
	In crude terms, fine-grained means more classes than tables.
	
	For example, a user may have a home address in your domain model. In the database, you may have a single USERS table with
	the columns HOME_STREET, HOME_CITY, and HOME_ZIPCODE. (Remember the problem of SQL types we discussed in section 1.2.1?)
	In the domain model, you could use the same approach, representing the address as three string-valued properties of the
	User class. But it’s much better to model this using an Address class, where User has a homeAddress property. This domain
	model achieves improved cohesion and greater code reuse, and it’s more understandable than SQL with inflexible type systems.
	JPA emphasizes the usefulness of fine-grained classes for implementing type safety and behavior.
	
Defining application concepts

    Two people live in the same house, and they both register user accounts in CaveatEmptor (your app). Let’s call them John
    and Jane. An instance of User represents each account.
!!! Because you want to load, save, and delete these User instances independently, User is an entity class and not a value type.
    Finding entity classes is easy

	If you have 2 users...each corresponds to a row in your table, and they share the same address, then  If Address is
	supposed to support shared runtime references, it’s an entity type. The Address instance has its own life, you can’t
	delete it when John removes his User account—Jane might still have a reference to the Address.

	Now let’s look at the alternative model where each User has a reference to its own homeAddress instance, as shown in
	figure 4.2. In this case, you can make an instance of Address dependent on an instance of User: you make it a value
	type. When John removes his User account, you can safely delete his Address instance. Nobody else will hold a reference.


!!!	 Hence, we make the following essential distinction:
		 You can retrieve an instance of entity type using its persistent identity: for example, a User, Item, or Category
		 instance. A reference to an entity instance (a pointer in the JVM) is persisted as a reference in the database
		 (a foreign key– constrained value). An entity instance has its own life cycle; it may exist independently of
		 any other entity. You map selected classes of your domain model as entity types.
		 An instance of value type has no persistent identifier property; it belongs to an entity instance. Its lifespan
		is bound to the owning entity instance. A value type instance doesn’t support shared references. The most
		obvious value types are all JDK-defined classes such as String, Integer, and even primitives. You can
		also map your own domain model classes as value types: for example, Address and MonetaryAmount.

	If you read the JPA specification, you’ll find the same concept. But value types in JPA are called basic property
	types or embeddable classes.

Distinguishing entities and value types

!!!!!! your first reaction should be to make everything a value typed class and promote it to an entity only when absolutely necessary

	Next, take your domain model diagram and implement POJOs for all entities and value types. You’ll have to take care
	of three things:
		 Shared references—Avoid shared references to value type instances when you
		write your POJO classes. For example, make sure only one User can reference
		an Address. You can make Address immutable with no public setUser()
		method and enforce the relationship with a public constructor that has a User
		argument. Of course, you still need a no-argument, probably protected constructor, as we discussed in the previous
		chapter, so Hibernate can also create an instance.
		 Life cycle dependencies—If a User is deleted, its Address dependency has to be
		deleted as well. Persistence metadata will include the cascading rules for all
		such dependencies, so Hibernate (or the database) can take care of removing
		the obsolete Address. You must design your application procedures and user
		interface to respect and expect such dependencies—write your domain model
		POJOs accordingly.
		 Identity—Entity classes need an identifier property in almost all cases. Value type
		classes (and of course JDK classes such as String and Integer) don’t have an
		identifier property, because instances are identified through the owning entity
			
			
Understanding Java identity and equality			
		Mapping entities with identity requires you to understand Java identity and equality before we can walk through
		 an entity class example and its mapping. After that, we’ll be able to dig in deeper and select a primary key,
		  configure key generators, and finally go through identifier generator strategies.
			
		Java developers understand the difference between Java object identity and equality. Object identity (==) is a
		notion defined by the Java virtual machine. Two references are identical if they point to the same memory
		location.  On the other hand, object equality is a notion defined by a class’s equals()
		method, sometimes also referred to as equivalence. Equivalence means two different (non-identical) instances
		have the same value—the same state. Two different instances of String are equal if they represent the same
		sequence of characters, even though each has its own location in the memory space of the virtual machine.
		(If you’re a Java guru, we acknowledge that String is a special case. Assume we used a different class to make the same point.)
		
!!!!	Persistence complicates this picture.
		Along with Java identity and equality, we define database identity. You now have three methods for distinguishing references:
		 Objects are identical if they occupy the same memory location in the JVM. This can be checked with the
		a == b operator. This concept is known as object identity.
		 Objects are equal if they have the same state, as defined by the a.equals(Object b) method. Classes that
		don’t explicitly override this method inherit the implementation defined by java.lang.Object, which compares
		object identity with ==. This concept is known as object equality.
		 Objects stored in a relational database are identical if they share the same table and primary key value.
		This concept, mapped into the Java space, is known as database identity.


A first entity class and mapping	
	We weren’t completely honest in the previous chapter: the @Entity annotation isn’t enough to map a persistent class.
	You also need an @Id annotation, as shown in the following listing.
		@Entity
		public class Item {
		 @Id
		 @GeneratedValue(generator = "ID_GENERATOR")
		 protected Long id;
		 public Long getId() {											// optional but useful
		 return id;
		 }
		}
	This is the most basic entity class, marked as “persistence capable” with the @Entity annotation, and with an @Id
	mapping for the database identifier property. The class maps by default to a table named ITEM in the database schema.

	Every entity class has to have an @Id property; it’s how JPA exposes database identity to the application. We
	don’t show the identifier property in our diagrams; we assume that each entity class has one. In our examples, we
	always name the identifier property id. This is a good practice for your own project; use the same identifier
	property name for all your domain model entity classes.
	
	Should you have a setter method? Primary key values never change, so you shouldn’t allow modification of the
	identifier property value. Hibernate won’t update a primary key column, and you shouldn’t expose a public identifier
	setter method on an entity.


Selecting a primary key
	A candidate key is a column or set of columns that you could use to identify a particular row in a table. To become
	the primary key, a candidate key must satisfy the following requirements:
		 The value of any candidate key column is never null. You can’t identify something with data that is unknown,
		and there are no nulls in the relational model. Some SQL products allow defining (composite) primary keys with
		nullable columns, so you must be careful.
		 The value of the candidate key column(s) is a unique value for any row.
		 The value of the candidate key column(s) never changes; it’s immutable.
	
	Hibernate doesn’t support updating primary key values with an API; if you try to work around this requirement, you’ll
	run into problems with Hibernate’s caching and dirty-checking engine.

!!!	Many legacy SQL data models use natural primary keys. A natural key is a key with business meaning: an attribute or
	combination of attributes that is unique by virtue of its business semantics. Examples of natural keys are the US
	Social Security Number and Australian Tax File Number.
	Experience has shown that natural primary keys usually cause problems in the end. A good primary key must be unique,
	immutable, and never null. Few entity attributes satisfy these requirements...

	In addition, you should make certain that a candidate key definition never changes throughout the lifetime of the
	database. Changing the value (or even definition) of a primary key, and all foreign keys that refer to it, is a
	frustrating task. Expect your database schema to survive decades, even if your application won’t.
	
	For these reasons, we strongly recommend that you add synthetic identifiers, also called surrogate keys. Surrogate
	keys have no business meaning—they have unique values generated by the database or application. Application users
	ideally don’t see or refer to these key values; they’re part of the system internals.
	There are a number of well-known approaches to generating surrogate key values.
	The aforementioned @GeneratedValue annotation is how you configure this.


Configuring key generators
	Usually you want the system to generate a primary key value when you save an entity instance, so you write the
	@GeneratedValue annotation next to @Id. JPA standardizes several value-generation strategies with the
	javax.persistence.GenerationType enum, which you select with @GeneratedValue(strategy = ...):

		 GenerationType.AUTO—Hibernate picks an appropriate strategy, asking the
		SQL dialect of your configured database what is best. This is equivalent to
		@GeneratedValue() without any settings.
		 GenerationType.SEQUENCE—Hibernate expects (and creates, if you use the
		tools) a sequence named HIBERNATE_SEQUENCE in your database. The sequence
		will be called separately before every INSERT, producing sequential numeric
		values.
		 GenerationType.IDENTITY—Hibernate expects (and creates in table DDL) a
		special auto-incremented primary key column that automatically generates a
		numeric value on INSERT, in the database.
		 GenerationType.TABLE—Hibernate will use an extra table in your database
		schema that holds the next numeric primary key value, one row for each entity
		class. This table will be read and updated accordingly, before INSERTs. The
		default table name is HIBERNATE_SEQUENCES with columns SEQUENCE_NAME and
		SEQUENCE_NEXT_HI_VALUE. (The internal implementation uses  a more complex but efficient hi/lo generation
		algorithm; more on this later)

	Therefore, instead of picking one of the JPA strategies, we recommend a mapping of the identifier with
	@GeneratedValue(generator = "ID_GENERATOR"), as shown in the previous example. This is a named identifier
	generator; you are now free to set up the ID_GENERATOR configuration independently from your entity classes.

	(see example in com.testehan.hibernate.basics.first2)

	This Hibernate-specific generator configuration has the following advantages:
		 The enhanced-sequence B strategy produces sequential numeric values. If your SQL dialect supports sequences,
		Hibernate will use an actual database sequence. If your DBMS doesn’t support native sequences, Hibernate will
		manage and use an extra “sequence table,” simulating the behavior of a sequence.
		This gives you real portability: the generator can always be called before performing an SQL INSERT, unlike,
		for example, auto-increment identity columns, which produce a value on INSERT that has to be returned to the application
		afterward.
		 You can configure the sequence_name C. Hibernate will either use an existing sequence or create it when you
		generate the SQL schema automatically. If your DBMS doesn’t support sequences, this will be the special
		“sequence table” name.
		 You can start with an initial_value D that gives you room for test data. For
		example, when your integration test runs, Hibernate will make any new data
		insertions from test code with identifier values greater than 1000. Any test data
		you want to import before the test can use numbers 1 to 999, and you can refer
		to the stable identifier values in your tests: “Load item with id 123 and run some
		tests on it.” This is applied when Hibernate generates the SQL schema and
		sequence; it’s a DDL option


!!!	You can share the same database sequence among all your domain model classes. There is no harm in specifying
    @GeneratedValue(generator = "ID_GENERATOR") in all your entity classes. It doesn’t matter if primary key values
    aren’t contiguous for a particular entity, as long as they’re unique within one table.

!!!	Finally, you use java.lang.Long as the type of the identifier property in the entity class, which maps perfectly
    to a numeric database sequence generator.
	An Integer would work for almost two months if you generated a new identifier each millisecond with no gaps, and
	a Long would last for about 300 million years.


Identifier generator strategies
	We also show the relationship between each standard JPA strategy and its native Hibernate equivalent. Hibernate
	has been growing organically, so there are now two sets of mappings between standard and native strategies;
	 we call them Old and New in the list.
	
	You can find the list starting from page 72 or 101 in PDF numbering....the 2 below are the one that I found more interesting:
		 uuid2—Produces a unique 128-bit UUID in the application layer. Useful when
		you need globally unique identifiers across databases (say, you merge data from
		several distinct production databases in batch runs every night into an archive).
		The UUID can be encoded either as a java.lang.String, a byte[16], or a java
		.util.UUID property in your entity class. Replaces the legacy uuid and uuid
		.hex strategies. You configure it with an org.hibernate.id.UUIDGenerationStrategy; see the Javadoc for the
		class org.hibernate.id.UUIDGenerator for more details.
		 guid—Uses a globally unique identifier produced by the database, with an SQL
		function available on Oracle, Ingres, MS SQL Server, and MySQL. Hibernate
		calls the database function before an INSERT. Maps to a java.lang.String
		identifier property. If you need full control over identifier generation, configure the strategy of
		@GenericGenerator with the fully qualified name of a class 	that implements the org.hibernate.id.IdentityGenerator
		interface.
	
!!	To summarize, our recommendations on identifier generator strategies are as follows:
		 In general, we prefer pre-insert generation strategies that produce identifier values independently before INSERT.
		 Use enhanced-sequence, which uses a native database sequence when supported and otherwise falls back to an
		extra database table with a single column and row, emulating a sequence.
		( Dan : I did not find info on enhanced-sequence STUFF...book example works and is in
		is in com.testehan.hibernate.basics.first3 )


4.3.1 Controlling names (refers to entities)
	If you only specify @Entity on the persistence-capable class, the default mapped table name is the same as the class
	name. Note that we write SQL artifact names in UPPERCASE to make them easier to distinguish—SQL is actually case
	insensitive. So the Java entity class Item maps to the ITEM table. You can override the table name with
	the JPA @Table annotation, as shown next
		@Entity
		@Table(name = "USERS")
		public class User implements Serializable {
		 // ...
		}
		
	The User entity would map to the USER table; this is a reserved keyword in most SQL DBMSs. You can’t have a table with
	that name, so you instead map it to USERS.

	Hibernate provides a feature that allows you to enforce naming standards automatically. Suppose that all table names
	in CaveatEmptor should follow the pattern CE_<table name>. One solution is to manually specify an @Table annotation on
	all entity classes. This approach is time-consuming and easily forgotten. Instead, you can
	implement Hibernate’s PhysicalNamingStrategy interface or override an existing implementation, as in the following listing.

	public class CENamingStrategy extends org.hibernate.boot.model.naming.PhysicalNamingStrategyStandardImpl {
		 @Override
		 public Identifier toPhysicalTableName(Identifier name, JdbcEnvironment context) {
			return new Identifier("CE_" + name.getText(), name.isQuoted());
		 }
	}

	The overridden method toPhysicalTableName() prepends CE_ to all generated table names in your schema. Look at the
	Javadoc of the PhysicalNamingStrategy interface; it offers methods for custom naming of columns, sequences, and other
	artifacts. You have to enable the naming-strategy implementation in persistence.xml:
	<persistence-unit name="CaveatEmptorPU">
		...
		<properties>
			<property name="hibernate.physical_naming_strategy" value="org.jpwh.shared.CENamingStrategy"/>
		</properties>
	</persistence-unit>


4.3.2 Dynamic SQL generation !!!!!

!!!	By default, Hibernate creates SQL statements for each persistent class when the persistence unit is created, on
	startup. These statements are simple create, read, update, and delete (CRUD) operations for reading a single row,
	deleting a row, and so on. It’s cheaper to store these in memory up front, instead of generating SQL strings every
	time such a simple query has to be executed at runtime. In addition, prepared statement caching at the JDBC level is
	much more efficient if there are fewer statements.
	How can Hibernate create an UPDATE statement on startup? After all, the columns to be updated aren’t known at this
	time. The answer is that the generated SQL statement updates all columns, and if the value of a particular column
	isn’t modified, the statement sets it to its old value.

!!	In some situations, such as a legacy table with hundreds of columns where the SQL statements will be large for even
	the simplest operations (say, only one column needs updating), you should disable this startup SQL generation and
	switch to dynamic statements generated at runtime. An extremely large number of entities can also impact
	startup time, because Hibernate has to generate all SQL statements for CRUD up front.


4.3.3 Making an entity immutable
	Instances of a particular class may be immutable. For example, in CaveatEmptor, a Bid made for an item is immutable.
	Hence, Hibernate never needs to execute UPDATE statements on the BID table. Hibernate can also make a few other
	 optimizations, such as avoiding dirty checking, if you map an immutable class as shown in the next example. Here,
	  the Bid class is immutable and instances are never modified:
		@Entity
		@org.hibernate.annotations.Immutable
		public class Bid {
		 // ...
		}
	
	A POJO is immutable if no public setter methods for any properties of the class are exposed—all values are set in
	the constructor

	When you can’t create a view in your database schema, you can map an immutable entity class to an SQL SELECT query.
	Sometimes your DBA won’t allow you to change the database schema; even adding a new view might not be possible. Let’s
	say you want to create a view that contains the identifier of an auction Item and the number of bids made for that item.

		@Entity
		@org.hibernate.annotations.Immutable
		@org.hibernate.annotations.Subselect(
		 value = "select i.ID as ITEMID, i.ITEM_NAME as NAME, " +
		 "count(b.ID) as NUMBEROFBIDS " +
		 "from ITEM i left outer join BID b on i.ID = b.ITEM_ID " +
		 "group by i.ID, i.ITEM_NAME"
		)
		
		@org.hibernate.annotations.Synchronize({"Item", "Bid"})
		public class ItemBidSummary {
		 @Id
		 protected Long itemId;
		 protected String name;
		 protected long numberOfBids;
		 
		 public ItemBidSummary() {
		 }
		 // Getter methods...
		 // ...
		}

		When an instance of ItemBidSummary is loaded, Hibernate executes your custom SQL SELECT as a subselect:
			ItemBidSummary itemBidSummary = em.find(ItemBidSummary.class, ITEM_ID);
		
		You should list all table names referenced in your SELECT in the @org.hibernate.annotations.Synchronize annotation


4.4 Summary
	 Entities are the coarser-grained classes of your system. Their instances have an
	independent life cycle and their own identity, and many other instances can reference them.
	 Value types, on the other hand, are dependent on a particular entity class. A
	value type instance is bound to its owning entity instance, and only one entity
	instance can reference it—it has no individual identity.
	 We looked at Java identity, object equality, and database identity, and at what
	makes good primary keys. You learned which generators for primary key values
	Hibernate provides out of the box, and how to use and extend this identifier
	system.
	 We discussed some useful class mapping options, such as naming strategies and
	dynamic SQL generation. 



Mapping value types
	We split value types into two categories: basic value-typed classes that come with the JDK, such as String, Date,
	primitives, and their wrappers; and developer defined value-typed classes, such as Address and MonetaryAmount in
	CaveatEmptor.

	 The default JPA rules for properties of persistent classes are these:
		 If the property is a primitive or a primitive wrapper, or of type String, BigInteger,
		BigDecimal, java.util.Date, java.util.Calendar, java.sql.Date, java.sql
		.Time, java.sql.Timestamp, byte[], Byte[], char[], or Character[], it’s automatically persistent.
		Hibernate loads and stores the value of the property in a column with an appropriate SQL type and the same name
		as the property.
		 Otherwise, if you annotate the class of the property as @Embeddable, or you map
		the property itself as @Embedded, Hibernate maps the property as an embedded
		component of the owning class. We discuss embedding of components later in
		this chapter, with the Address and MonetaryAmount embeddable classes of
		CaveatEmptor.
		 Otherwise, if the type of the property is java.io.Serializable, its value is
		stored in its serialized form. This typically isn’t what you want, and you should
		always map Java classes instead of storing a heap of bytes in the database. Imagine maintaining a database with
		this binary information when the application is gone in a few years.
		 Otherwise, Hibernate will throw an exception on startup, complaining that it
		doesn’t understand the type of the property.
		
		!!!! This configuration by exception approach means you don’t have to annotate a property to make it persistent;
		you only have to configure the mapping in an exceptional case.


Overriding basic property defaults
	To exclude a property, mark the field or the getter method of the property with the @javax.persistence.Transient
	annotation or use the Java transient keyword. The transient keyword usually only excludes fields for Java
	serialization, but it’s also recognized by JPA providers.

!!!!!
	@Basic(optional = false)
	BigDecimal initialPrice;

	By default, all persistent properties are nullable and optional; an Item may have an unknown initialPrice. Mapping
	the initialPrice property as non-optional makes
	sense if you have a NOT NULL constraint on the INITIALPRICE column in your SQL schema. If Hibernate is generating
	the SQL schema, it will include a NOT NULL constraint automatically for non-optional properties. Now, when you store
	an Item and forget to set a value on the initialPrice field,
	Hibernate will complain with an exception before hitting the database with an SQL statement. Hibernate knows that a
	value is required to perform an INSERT or UPDATE.
!!!	If you don’t mark the property as optional and try to save a NULL, the database will reject the SQL statement,
    and Hibernate will throw a constraint-violation exception.
	There isn’t much difference in the end result, but it’s cleaner to avoid hitting the database with a statement that fails.

!!! Instead of @Basic, most engineers use the more versatile @Column annotation to declare nullability:
	@Column(nullable = false)
	BigDecimal initialPrice;

	We’ve now shown you three ways to declare whether a property value is required: with the @Basic annotation, the
	@Column annotation, and earlier with the Bean Validation
	@NotNull annotation in section 3.3.2. All have the same effect on the JPA provider: Hibernate does a null check when
	saving and generates a NOT NULL constraint in the
	database schema. We recommend the Bean Validation @NotNull annotation so you can manually validate an Item instance
	and/or have your user interface code in the presentation layer execute validation checks automatically.


Customizing property access
	Property annotations aren’t always on fields, and you may not want Hibernate to access fields directly. 

!!! The persistence engine accesses the properties of a class either directly through fields or indirectly through
    getter and setter methods. An annotated entity inherits the
!!	default from the position of the mandatory @Id annotation. For example, if you’ve declared @Id on a field, not a
    getter method, all other mapping annotations for that entity are expected on fields. Annotations are never on the
    setter methods.

	The JPA specification offers the @Access annotation for overriding the default behavior, with the parameters
	AccessType.FIELD and AccessType.PROPERTY. If you set @Access on the class/entity level, Hibernate accesses all
	properties of the class according to the selected strategy. You then set any other mapping annotations,
	including the @Id, on either fields or getter methods, respectively. 

!!!!
	You can also use the @Access annotation to override the access strategy of individual properties. Let’s explore
	this with an example.
			@Entity
			public class Item {
				@Id
				@GeneratedValue(generator = Constants.ID_GENERATOR)
				protected Long id;
			
				@Access(AccessType.PROPERTY)
				 @Column(name = "ITEM_NAME")
				 protected String name;

				 public String getName() {
					return name;
				 }
				public void setName(String name) {
					this.name = !name.startsWith("AUCTION: ") ? "AUCTION: " + name : name;
				}
			}

		1. The Item entity defaults to field access. The @Id is on a field. (You also move the brittle ID_GENERATOR
		string into a constant.)
		2. The @Access(AccessType.PROPERTY) setting on the name field switches this particular property to runtime access
		through getter/setter methods by the JPA provider
		3. Hibernate calls getName() and setName() when loading and storing items.

	Now turn it around: if the default (or explicit) access type of the entity would be through property getter and setter
	methods, @Access(AccessType.FIELD) on a getter method would tell Hibernate to access the field directly. All other
	mapping information would still have to be on the getter method, not the field.

	Fun stuff : Hibernate has a rarely needed extension: the noop property accessor. This sounds strange, but it lets you
	refer to a virtual property in queries. This is useful if you have a database column you’d like to use only in JPA
	queries. For example, let’s say the ITEM database table has a VALIDATED column and your Hibernate application won’t
	access this column through the domain model. It might be a legacy column or a column maintained by another application
	or database trigger. All you want is to refer to this
	column in a JPA query such as select i from Item i where i.validated = true or select i.id, i.validated from Item i. The
	Java Item class in your domain model doesn’t have this property; hence there is no place to put annotations. The only
	 way to map such a virtual property is with an hbm.xml native metadata file
		<hibernate-mapping>
			<class name="Item">
				 <id name="id">
				 ...
				 </id>
				<property name="validated" column="VALIDATED" access="noop"/>
			</class>
		</hibernate-mapping>
	Remember that  when using this hbm.xml aproach, means that the mapping file has to be complete: any annotations on the Item class are now ignored! 


Using derived properties
!!	The given SQL formulas are evaluated every time the Item entity is retrieved from the database and not at any other
    time, so the result may be outdated if other properties are modified. The properties never appear in an SQL
    INSERT or UPDATE, only in SELECTs. Evaluation occurs in the database;
		Example:
			@org.hibernate.annotations.Formula("(select avg(b.AMOUNT) from BID b where b.ITEM_ID = ID)")
			protected BigDecimal averageBidAmount;

	Formulas may refer to columns of the database table, they can call SQL functions,and they may even include SQL subselects. 
	The SQL expression is passed to the underlying database as is; if you aren’t careful, you may rely on vendor-specific
	operators or keywords and bind your mapping metadata to a particular database product. Notice that unqualified column
	names refer to columns of the table of the class to which the derived property belongs.


Transforming column values
!!	Let’s say you have a database column called IMPERIALWEIGHT, storing the weight of an Item in pounds. The application,
    however, has the property Item#metricWeight in kilograms, so you have to convert the value of the database column
    when reading and writing a row from and to the ITEM table.

	@Column(name = "IMPERIALWEIGHT")
	@org.hibernate.annotations.ColumnTransformer(
		read = "IMPERIALWEIGHT / 2.20462",
		write = "? * 2.20462"
	)
	protected double metricWeight;

	When reading a row from the ITEM table, Hibernate embeds the expression IMPERIALWEIGHT / 2.20462, so the calculation
	occurs in the database and Hibernate returns the metric value in the result to the application layer

	 Hibernate also applies column converters in query restrictions. For example, the following query retrieves all
	 items with a weight of two kilograms:
		List<Item> result = em.createQuery("select i from Item i where i.metricWeight = :w").setParameter("w", 2.0).getResultList();
	The actual SQL executed by Hibernate for this query contains the following restriction in the WHERE clause:
		// ... where i.IMPERIALWEIGHT / 2.20462=?				


Generated and default property values
	The database sometimes generates a property value, usually when you insert a row for the first time. Examples of
	database-generated values are a creation timestamp, a default price for an item, and a trigger that runs for every
	modification.

	Hibernate applications need to refresh instances that contain any properties for which the database generates
	values, after saving. This means you would have to make another round trip to the database to read the value after
	inserting or updating a row. Marking properties as generated, however, lets the application delegate this
	responsibility to Hibernate. Essentially, whenever Hibernate issues an SQL INSERT or UPDATE for an entity that has
	declared generated properties, it does a SELECT immediately afterward to retrieve the generated values.

		@Temporal(TemporalType.TIMESTAMP)                        could not make this example with date work
		@Column(insertable = false, updatable = false)           but in first3 I managed to add creation time and update time
		@org.hibernate.annotations.Generated(                    in a different way
		 org.hibernate.annotations.GenerationTime.ALWAYS
		)
		protected Date lastModified;

		@Column(insertable = false)                             this works
		@org.hibernate.annotations.ColumnDefault("1.00")
		@org.hibernate.annotations.Generated(
		 org.hibernate.annotations.GenerationTime.INSERT
		)
		protected BigDecimal initialPrice;

	Available settings for GenerationTime are ALWAYS and INSERT. With ALWAYS, Hibernate refreshes the entity instance
	after every SQL UPDATE or INSERT. The example assumes that a database trigger will keep the lastModified property
	current. The property should also be marked read-only, with the updatable and insertable parameters of @Column.
	If both are set to false, the property’s column(s) never appear in the INSERT or UPDATE statements, and you let
	the database generate the value.

	With GenerationTime.INSERT, refreshing only occurs after an SQL INSERT, to retrieve the default value provided by
	the database. Hibernate also maps the property as not insertable. The @ColumnDefault Hibernate annotation sets the
	default value of the column when Hibernate exports and generates the SQL schema DDL.


Temporal properties
	The lastModified property of the last example was of type java.util.Date, and a database trigger on SQL INSERT
	generated its value. The JPA specification requires that you annotate temporal properties with @Temporal to
	declare the accuracy of the SQL data type of the mapped column. The Java temporal types are java.util.Date,
	java.util.Calendar, java.sql.Date, java.sql.Time, and java.sql.Timestamp. Hibernate also supports the classes of the
	java.time package available in JDK 8.
		Example
			@Temporal(TemporalType.TIMESTAMP)
			@Column(updatable = false)
			@org.hibernate.annotations.CreationTimestamp                this works
			protected Date createdOn;

!!!	Furthermore, you’ve used the @CreationTimestamp Hibernate annotation to mark the property. This is a sibling of the
	@Generated annotation from the previous section: it tells Hibernate to generate the property value automatically.
	In this case, Hibernate sets the value to the current time before it inserts the entity instance into
	the database. A similar built-in annotation is @UpdateTimestamp. You can also write and configure custom value
	generators, running in the application or database. Have
	a look at org.hibernate.annotations.GeneratorType and ValueGenerationType. 


Mapping enumerations
	An enumeration type is a common Java idiom where a class has a constant (small) number of immutable instances. 
		public enum AuctionType {
		 HIGHEST_BID,
		 LOWEST_BID,
		 FIXED_PRICE
		}

	How to use example:
	@NotNull
	@Enumerated(EnumType.STRING)
	protected AuctionType auctionType = AuctionType.HIGHEST_BID;

	Without the @Enumerated annotation, Hibernate would store the ORDINAL position of the value. That is, it would store
	1 for HIGHEST_BID, 2 for LOWEST_BID, and 3 for FIXED_PRICE. This is a brittle default; if you make changes to the
	AuctionType enum, existing values may no longer map to the same position. The EnumType.STRING option
	is therefore a better choice; Hibernate stores the label of the enum value as is. 


Mapping embeddable components 
	The example that is presents is that of a User, that has 2 addresses, 1 for his home, and 1 for billing...it is a
	relationship of type Composition, where the life cycle of the part is fully dependent on the life cycle of the whole

	There is only one mapped table, USERS, for the User entity. This table embeds all details of the components, where a
	single row holds a particular User and their homeAddress and billingAddress. If another entity has a reference to
	an Address—for example, Shipment#deliveryAddress—then the SHIPMENT table will also have all columns needed to store
	an Address. (this is what we want to achieve in this moddeling example)

!!!	This schema reflects value type semantics: a particular Address can’t be shared; it doesn’t have its own identity.
    Its primary key is the mapped database identifier of the owning entity. An embedded component has a dependent life
    cycle: when the owning entity instance is saved, the component instance is saved. When the owning entity
	instance is deleted, the component instance is deleted. Hibernate doesn’t even have to execute any special SQL for
	this; all the data is in a single row.
	Having “more classes than tables” is how Hibernate supports fine-grained domain models. Let’s write the classes and
	mapping for this structure.

	A component class has no individual identity; hence, the component class requires no identifier property or
	identifier mapping. It’s a simple POJO, as you can see in the following listing.
			
			@Embeddable										// we have this instead of @Entity
			public class Address {
				 @NotNull
				 @Column(nullable = false)					// used for DDL generation...not null constraint
				 protected String street;
				 
				 @NotNull
				 @Column(nullable = false, length = 5)			// overrirdes Varchar(255)
				 protected String zipcode;
				 
				 @NotNull
				 @Column(nullable = false)
				 protected String city;
				 
				 protected Address() {						// no args constructor
				 }
				 
				 public Address(String street, String zipcode, String city) {			// convenience constructor
				 this.street = street;
				 this.zipcode = zipcode;
				 this.city = city;
				 }
				 
				 public String getStreet() {
				 return street;
				 }
				 
				 public void setStreet(String street) {
				 this.street = street;
				 }
				 
				 public String getZipcode() {
				 return zipcode;
				 }
				 
				 public void setZipcode(String zipcode) {
				 this.zipcode = zipcode;
				 }
				 
				 public String getCity() {
				 return city;
				 }
				 
				 public void setCity(String city) {
				 this.city = city;
				 }
			}

	!!	1. Instead of @Entity, this component POJO is marked with @Embeddable. It has no identifier property.
	!!	2. Hibernate calls this no-argument constructor to create an instance and then populates the fields directly.
	!!	3. You can have additional (public) constructors for convenience.

	The properties of the embeddable class are all by default persistent, just like the properties of a persistent
	entity class. You can configure the property mappings with the same annotations, such as @Column or @Basic.
	The properties of the Address class map to the columns STREET, ZIPCODE, and CITY and are constrained with NOT NULL.

		Below is an example of how to use Address in another entity
			@Entity
			@Table(name = "USERS")
			public class User implements Serializable {
				@Id
				@GeneratedValue(generator = Constants.ID_GENERATOR)
				protected Long id;
				
				public Long getId() {
					return id;
				}
				
!!!!			protected Address homeAddress;			//	The Address is @Embeddable; no annotation is needed here.
				
				public Address getHomeAddress() {
					return homeAddress;
				}
				
				public void setHomeAddress(Address homeAddress) {
					this.homeAddress = homeAddress;
				}
			 // ...
			}

	Hibernate detects that the Address class is annotated with @Embeddable; the STREET,ZIPCODE, and CITY columns are
!!	mapped on the USERS table, the owning entity’s table. When we talked about property access earlier in this chapter,
	we mentioned that embeddable components inherit their access strategy from their owning entity. This means Hibernate
	will access the properties of the Address class with the same strategy as for User properties.

		 If the owning @Entity of an embedded component is mapped with field access, either implicitly with @Id on a
		field or explicitly with @Access(AccessType.FIELD) on the class, all mapping annotations of the embedded
		 component class are expected on fields of the component class. Hibernate expects annotations on the fields of
		 the Address class and reads/writes the fields directly at 	runtime. Getter and setter methods on Address are
		 optional.
		 If the owning @Entity of an embedded component is mapped with property access, either implicitly with @Id on
		a getter method or explicitly with @Access(AccessType.PROPERTY) on the class, all mapping annotations of the
		 embedded component class are expected on getter methods of the component class. Hibernate then reads and
		 writes values by calling getter and setter methods on the embeddable component class.
		 If the embedded property of the owning entity class—User#homeAddress in the last example—is marked with
		 @Access(AccessType.FIELD), Hibernate expects annotations on the fields of the Address class and access
		 fields at runtime.
		 If the embedded property of the owning entity class—User#homeAddress in the last example—is marked with
		@Access(AccessType.PROPERTY), Hibernate expects annotations on getter methods of the Address class and access
		getter and setter methods at runtime.
		 If @Access annotates the embeddable class itself, Hibernate will use the selected strategy for reading mapping
		annotations on the embeddable class and runtime access.

	When Hibernate loads a User without any address information, what should be returned by someUser.getHomeAddress()?
	Hibernate returns a null in this case. Hibernate also stores a null embedded property as NULL values in all mapped
	columns of the component. Consequently, if you store a User with an “empty” Address (you have an Address instance
	but all its properties are null), no Address instance will be returned when you load the User. This can be
	counterintuitive; on the other hand, you probably shouldn’t have nullable columns anyway and avoid ternary logic.


Overriding embedded attributes
	The billingAddress is another embedded component property of the User class, so another Address has to be stored
	in the USERS table. This creates a mapping conflict: so far, you only have columns in the schema to store one
	Address in STREET, ZIPCODE, and CITY. You need additional columns to store another Address for each USERS row.
	When you map the billingAddress, override the column names:
		@Entity
		@Table(name = "USERS")
		public class User implements Serializable {
		
			@Embedded									// not necessary  PAY ATTENTION AS THIS IS DIFFERENT FROM @Embeddable
			@AttributeOverrides({
				@AttributeOverride(name = "street", column = @Column(name = "BILLING_STREET")),
				@AttributeOverride(name = "zipcode", column = @Column(name = "BILLING_ZIPCODE", length = 5)),
				@AttributeOverride(name = "city", column = @Column(name = "BILLING_CITY"))
			})
			protected Address billingAddress;
		 
			public Address getBillingAddress() {
				return billingAddress;
			}

			public void setBillingAddress(Address billingAddress) {
				this.billingAddress = billingAddress;
			}
			// ...
		}
	
	!!!	The @Embedded annotation actually isn’t necessary. It’s an alternative to @Embeddable:mark either the component
	class or the property in the owning	entity class (both doesn’t hurt but has no advantage).
	The @Embedded annotation is useful if you want to map a third-party component class without source and no annotations,
	but using the right getter/setter methods (like regular JavaBeans).

	The @AttributeOverrides selectively overrides property mappings of the embedded class; in this example, you
	override all three properties and provide different column names. Now you can store two Address instances in the
	USERS table, each instance in a different set of columns

	!!!	Each @AttributeOverride for a component property is “complete”: any JPA or Hibernate annotation on the
	overridden property is ignored. This means the @Column annotations on the Address class are ignored—all BILLING_*
	columns are NULLable!


Mapping nested embedded components
	An embeddable class can have an embedded property. Address has a city property:
		Example:
			@Embeddable
			public class Address {
				@NotNull
				@Column(nullable = false)
				protected String street;
				 
				@NotNull
				@AttributeOverrides(
					@AttributeOverride(
						name = "name",
						column = @Column(name = "CITY", nullable = false)
					)
				)
				protected City city;
				
				// ...
			}
		The embeddable City class has only basic properties:
			@Embeddable
			public class City {
				 @NotNull
				 @Column(nullable = false, length = 5)
				 protected String zipcode;
				
     			@NotNull
				 @Column(nullable = false)
				 protected String name;

				 @NotNull
				 @Column(nullable = false)
				 protected String country;
				 // ...
			}

	You could continue this kind of nesting by creating a Country class, for example. All embedded properties, no matter
	how deep they are in the composition, are mapped to columns of the owning entity’s table—here, the USERS table.


Mapping Java and SQL types with converters
	Until now, you’ve assumed that Hibernate selects the right SQL type when you map a java.lang.String property.
	Nevertheless, what is the correct mapping between the Java and SQL types, and how can you control it?


PRIMITIVE AND NUMERIC TYPES

	Name			Java Type				ANSI Sql type
	integer int, 	java.lang.Integer 		INTEGER
	long long, 		java.lang.Long 			BIGINT
	short short, 	java.lang.Short 		SMALLINT
	float float, 	java.lang.Float 		FLOAT
	double double, 	java.lang.Double 		DOUBLE
	byte byte, 		java.lang.Byte 			TINYINT
	boolean boolean,java.lang.Boolean 		BOOLEAN
	big_decimal 	java.math.BigDecimal 	NUMERIC
	big_integer 	java.math.BigInteger 	NUMERIC

	You probably noticed that your DBMS product doesn’t support some of the mentioned SQL types. These SQL type names
	are ANSI-standard type names. Most DBMS vendors ignore this part of the SQL standard, usually because their legacy
	type systems predate the standard.

	For product-specific schema generation, Hibernate translates from the ANSI-standard type to an appropriate
	vendor-specific type using the configured SQL dialect. This means you usually don’t have to worry about SQL data
	types if you let Hibernate create the schema for you.
	
	Name		Java Type								ANSI SQL
	string 		java.lang.String 						VARCHAR
	character 	char[], Character[], java.lang.String 	CHAR
	yes_no 		boolean, java.lang.Boolean CHAR(1), 	'Y' or 'N'
	true_false 	boolean, java.lang.Boolean CHAR(1), 	'T' or 'F'
	class 		java.lang.Class 						VARCHAR
	locale 		java.util.Locale 						VARCHAR
	timezone 	java.util.TimeZone 						VARCHAR
	currency 	java.util.Currency 						VARCHAR

	The Hibernate type system picks an SQL data type depending on the declared length of a string value: if your
	String property is annotated with @Column(length = ...) or @Length of Bean Validation, Hibernate selects the right
	SQL data type for the given string size. This selection also depends on the configured SQL dialect. For example,
	for MySQL, a length of up to 65,535 produces a regular VARCHAR(length) column when the schema is generated by
	Hibernate. For a length of up to 16,777,215, a MySQL-specific MEDIUMTEXT data type is produced, and even greater
	lengths use a LONGTEXT. The default length of Hibernate for all java.lang.String properties is 255, so without any
	further mapping, a String property maps to a VARCHAR(255) column

	 Also built in are some special converters for legacy databases or DBMSs with limited type systems, such as Oracle.
	 The Oracle DBMS doesn’t even have a truth-valued data type, the only data type required by the relational model.
	 Many existing Oracle schemas therefore represent Boolean values with Y/N or T/F characters. Or—and this is the
	 default in Hibernate’s Oracle dialect—a column of type NUMBER(1,0) is expected and generated.

	DATE AND TIME TYPES
	Table 5.3 lists types associated with dates, times, and timestamps. 
	
!!	Hibernate has to use the JDBC subclass when loading data from the database because the database types have higher
    accuracy than java.util.Date. A java.util.Date has millisecond accuracy, but a java.sql.Timestamp includes nanosecond
    information that may be present in the database. Hibernate won’t cut off this information to fit the value into
    java.util.Date. This Hibernate behavior may lead to problems if you try to compare java.util.Date values with the
    equals() method; it’s not symmetric with the java.sql.Timestamp subclass’s equals() method. The solution is simple,
    and not even specific to Hibernate: don’t call aDate.equals(bDate). You should always compare dates and times by
    comparing Unix time milliseconds (assuming you don’t care about the nanoseconds):

	Be careful: collections such as HashSet call the equals() method as well.
	Don’t mix java.util.Date and java.sql.Date|Time|Timestamp values in such a collection. You won’t have this kind of
	problem with a Calendar property. If you store a Calendar value, Hibernate will always return a Calendar value,
	created with Calendar.getInstance() (the actual type depends on locale and time zone).


BINARY AND LARGE VALUE TYPES
	
	Name					Java type					ANSI sql type
	binary 					byte[],java.lang.Byte[] 	VARBINARY
	text 					java.lang.String 			CLOB
	clob 					java.sql.Clob 				CLOB
	blob 					java.sql.Blob 				BLOB
	serializable 			java.io.Serializable 		VARBINARY

	If a property in your persistent Java class is of type byte[], Hibernate maps it to a VARBINARY column. The real
	SQL data type depends on the dialect; for example, in PostgreSQL, the data type is BYTEA, and in Oracle DBMS,
	it’s RAW.

	What does BLOB/CLOB mean?
	Jim Starkey, who came up with the idea of LOBs, says that the marketing department
	created the terms BLOB and CLOB and that they don’t mean anything. You can interpret them any way you like. We
	prefer locator objects (LOB), as a hint that they’re placeholders that help us locate and access the real thing.

		@Entity
		public class Item {
			@Lob
			protected java.sql.Blob imageBlob;
			
			@Lob
			protected java.sql.Clob description;
			
			// ...
		}

Creating custom JPA converters

	public class MonetaryAmount implements Serializable {
		 protected final BigDecimal value;
		 protected final Currency currency;
		 
		 public MonetaryAmount(BigDecimal value, Currency currency) {
			this.value = value;
			this.currency = currency;
		 }
		 
		 public BigDecimal getValue() {
			return value;
		 }
		 public Currency getCurrency() {
			return currency;
		 }
		 public boolean equals(Object o) {
			if (this == o) return true;
			if (!(o instanceof MonetaryAmount)) return false;
			final MonetaryAmount monetaryAmount = (MonetaryAmount) o;
			if (!value.equals(monetaryAmount.value)) return false;
			if (!currency.equals(monetaryAmount.currency)) return false;
				return true;
		}
		
		public int hashCode() {
			int result;
			result = value.hashCode();
			result = 29 * result + currency.hashCode();
			return result;
		}
		
		public String toString() {
			return getValue() + " " + getCurrency();
		}
		
		public static MonetaryAmount fromString(String s) {
			String[] split = s.split(" ");
			return new MonetaryAmount( new BigDecimal(split[0]),Currency.getInstance(split[1]) );
		}
	}

	1. This value-typed class should be java.io.Serializable: when Hibernate stores entity instance data in the shared
	second-level cache (see section 20.2), it disassembles the entity’s state. If an entity has a MonetaryAmount
	property, the serialized representation of the property value is stored in the second-level cache region. When entity
	data is retrieved from the cache region, the property value is deserialized and reassembled.
	2. The class doesn’t need a special constructor. You can make it immutable, even with final fields, because your
	code will be the only place an instance is created.
	3. You should implement the equals() and hashCode() methods and compare monetary amounts “by value.”
	4. You need a String representation of a monetary amount. Implement the toString() method and a static method to
	create an instance from a String.

	As is often the case, the database folks can’t implement multiple currencies right away and need more time. All they
	can provide quickly is a column data type change, in the database schema. They suggest that you store the
	BUYNOWPRICE in the ITEM table in a VARCHAR column and that you append the currency code of the monetary amount to its
	string value. You store, for example, the value "11.23 USD" or "99 EUR".  You have to convert an instance of
	MonetaryAmount to such a String representation when storing data. When loading data, you convert the String back
	into a MonetaryAmount.
	
	The simplest solution is a javax.persistence.AttributeConverter, as shown in the next listing, a standardized
	extension point in JPA.
		@Converter(autoApply = true)
		public class MonetaryAmountConverter implements AttributeConverter<MonetaryAmount, String> {
			 @Override
			 public String convertToDatabaseColumn(MonetaryAmount monetaryAmount) {
				return monetaryAmount.toString();
			 }
			 
			 @Override
			 public MonetaryAmount convertToEntityAttribute(String s) {
				return MonetaryAmount.fromString(s);
			 }
		}
	A converter has to implement the AttributeConverter interface; the two type arguments are the type of the Java
	property and the type in the database schema. The Java type is MonetaryAmount, and the database type is String,
	which maps, as usual, to an SQL VARCHAR. You must annotate the class with @Converter or declare it as such in the
	orm.xml metadata. With autoApply enabled, any MonetaryAmount property in your domain model, be it of an entity or
	an embeddable class, without further mapping will now be handled by this converter automatically.
	
	Example on how to use the convertor:
		@Entity
		public class Item {
			 @NotNull
			 @Convert(converter = MonetaryAmountConverter.class, disableConversion = false)
			 @Column(name = "PRICE", length = 63)
			 protected MonetaryAmount buyNowPrice;
			 // ...
		}
			// @Column renames the mapped database column PRICE; the default is BUYNOWPRICE. For automatic schema
			generation, define it as VARCHAR with a length of 63 characters.

	Later, when your DBA upgrades the database schema and offers you separate columns for the monetary amount value and
	currency, you only have to change your application in a few places. Drop the MonetaryAmountConverter from your
	 project and make MonetaryAmount an @Embeddable; it then maps automatically to two database columns.

	 Some limitations of the JPA converters are as follows:
		 You can’t apply them to identifier or version properties of an entity.
		 You shouldn’t apply a converter on a property mapped with @Enumerated or
		@Temporal, because these annotations already declare what kind of conversion
		has to occur. If you want to apply a custom converter for enums or date/time
		properties, don’t annotate them with @Enumerated or @Temporal.
		 You can apply a converter to a property mapping in an hbm.xml file, but you
		have to prefix the name: type="converter:qualified.ConverterName". 


Summary
	 We discussed the mapping of basic and embedded properties of an entity class.
	 You saw how to override basic mappings, how to change the name of a mapped column, and how to use derived,
	default, temporal, and enumeration properties.
	 We covered embeddable component classes and how you can create finegrained domain models.
	 You can map the properties of several Java classes in a composition, such as 	Address and City, to one
	entity table.
	 We looked at how Hibernate selects Java to SQL type converters, and what types are built into Hibernate.
	 You wrote a custom type converter for the MonetaryAmount class with the standard JPA extension interfaces,
	and then a low-level adapter with the native Hibernate UserType API.



Mapping inheritance
	We deliberately haven’t talked much about inheritance mapping so far. Mapping a hierarchy of classes to tables can
	be a complex issue, and we present various strategies in this chapter.
	A basic strategy for mapping classes to database tables might be “one table for every entity persistent class.” This
	approach sounds simple enough and indeed works well, until we encounter inheritance.

!!!	Inheritance is such a visible structural mismatch between the object-oriented and relational worlds because object-oriented
    systems model both "is a" and "has a" relationships. SQL-based models provide only "has a" relationships; SQL database
    management systems don’t support type inheritance—and even when it’s available, it’s usually proprietary or incomplete.

	There are four different strategies for representing an inheritance hierarchy:
		 Use one table per concrete class and default runtime polymorphic behavior.
		 Use one table per concrete class but discard polymorphism and inheritance relationships completely from the SQL
		schema. Use SQL UNION queries for runtime polymorphic behavior
		(The UNION operator is used to combine the result-set of two or more SELECT statements.)
				SELECT column_name(s) FROM table1
				UNION
				SELECT column_name(s) FROM table2;
		 Use one table per class hierarchy: enable polymorphism by denormalizing the SQL schema and relying on row-based
		 discrimination to determine super/ subtypes.
		 Use one table per subclass: represent "is a" (inheritance) relationships as "has a" (foreign key) relationships,
		and use SQL JOIN operations

	This chapter takes a top-down approach, assuming that you’re starting with a domain model and trying to derive a new
	SQL schema. The mapping strategies described are just as relevant if you’re working bottom-up, starting with existing
	database tables. We show some tricks along the way to help you deal with imperfect table layouts.


Table per concrete class with implicit polymorphism (see com.testehan.hibernate.basics.mappings.inheritance.v1.mappedSuperclass)

	Suppose we stick with the simplest approach suggested: exactly one table for each concrete class. You can map all
	properties of a class, including inherited properties, to columns of this table

!!	If you’re relying on this implicit polymorphism, you map concrete classes with @Entity, as usual. By default, properties
    of the superclass are ignored and not persistent! You have to annotate the superclass with @MappedSuperclass to
    enable embedding of its properties in the concrete subclass tables;
	
		Example:
		
			@MappedSuperclass
			public abstract class BillingDetails {
				@NotNull
				protected String owner;
				// ...
			}


			Now map the concrete subclasses: (The mapping for the BankAccount class looks the same, so we won’t show it here)
			@Entity
			@AttributeOverride(
			 name = "owner",
			 column = @Column(name = "CC_OWNER", nullable = false))
			public class CreditCard extends BillingDetails {
				@Id
				@GeneratedValue(generator = Constants.ID_GENERATOR)
				protected Long id;
			 
				@NotNull
				protected String cardNumber;
			 
				@NotNull
				protected String expMonth;
			 
				@NotNull
				protected String expYear;
				// ...
			}


		You can override column mappings from the superclass in a subclass with the @AttributeOverride annotation or
		several with @AttributeOverrides. The previous example renamed the OWNER column to CC_OWNER in the CREDITCARD table.

		You could declare the identifier property in the superclass, with a shared column name and generator strategy
		for all subclasses, so you wouldn’t have to repeat it. We haven’t done this in the examples to show you that it’s optional.

!!!	The main problem with implicit inheritance mapping is that it doesn’t support polymorphic associations very well. In
    the database, you usually represent associations as foreign key relationships. In the schema shown in figure 6.1, if
    the subclasses are all mapped to different tables, a polymorphic association to their superclass (abstract BillingDetails)
    can’t be represented as a simple foreign key relationship.
    You can’t have another entity mapped with a foreign key “referencing BILLINGDETAILS”—there is no such table. This
    would be problematic in the domain model, because BillingDetails is associated with User; both the CREDITCARD and
    BANKACCOUNT tables would need a foreign key reference to the USERS table. None of these issues can be easily resolved,
    so you should consider an alternative mapping strategy.

	Polymorphic queries that return instances of all classes that match the interface of the queried class are also
	problematic. Hibernate must execute a query against the superclass as several SQL SELECTs, one for each concrete
	subclass. The JPA query select bd from BillingDetails bd requires two SQL statements:
		select ID, OWNER, ACCOUNT, BANKNAME, SWIFT from BANKACCOUNT
		select ID, CC_OWNER, CARDNUMBER, EXPMONTH, EXPYEAR from CREDITCARD

	A further conceptual problem with this mapping strategy is that several different columns, of different tables,
	share exactly the same semantics. This makes schema evolution more complex. For example, renaming or changing the
	type of a superclass property results in changes to multiple columns in multiple tables. Many of the standard
	refactoring operations offered by your IDE would require manual adjustments, because the automatic procedures
	usually don’t account for things like @AttributeOverrides. It also makes it much more difficult to implement database
	integrity constraints that apply to all subclasses.

	We recommend this apprach (only) for the top level of your class hierarchy, where polymorphism isn’t usually required,
	and when modification of the superclass in the future is unlikely.
	With the help of the SQL UNION operation, you can eliminate most of the issues with polymorphic queries and associations. 


Table per concrete class with unions  (see com.testehan.hibernate.basics.mappings.inheritance.v2.union)

	First, let’s consider a union subclass mapping with BillingDetails as an abstract class (or interface), as in the
	previous section. In this situation, you again have two tables and duplicate superclass columns in both:
	CREDITCARD and BANKACCOUNT. What’s new is an inheritance strategy known as TABLE_PER_CLASS, declared on the
	superclass, as shown next.
	
		@Entity
		@Inheritance(strategy = InheritanceType.TABLE_PER_CLASS)
		public abstract class BillingDetails {
			@Id
			@GeneratedValue(generator = Constants.ID_GENERATOR)
			protected Long id;
		
			@NotNull
			 protected String owner;
			 // ...
		}

	The database identifier and its mapping have to be present in the superclass, to share it in all subclasses and
	their tables. This is no longer optional, as it was for the previous mapping strategy. The CREDITCARD and
	BANKACCOUNT tables both have an ID primary key column. All concrete class mappings inherit persistent properties
	from the superclass (or interface). An @Entity annotation on each subclass is all that is required.
		
		@Entity
		public class CreditCard extends BillingDetails {
			@NotNull
			protected String cardNumber;
		 
			@NotNull
			protected String expMonth;
		 
			@NotNull
			protected String expYear;
			// ...
		}

	Keep in mind that the SQL schema still isn’t aware of the inheritance; the tables look exactly alike, as in the
	previous example (and pic 6.1)
	Note that the JPA standard specifies that TABLE_PER_CLASS is optional, so not all JPA implementations may support
	it. The implementation is also vendor dependent—in Hibernate, it’s equivalent to a <union-subclass> mapping in the
	old native Hibernate XML metadata (don’t worry about this if you’ve never used native Hibernate XML files).

	If BillingDetails were concrete, you’d need an additional table to hold instances. We have to emphasize again that
	there is still no relationship between the database tables, except for the fact that they have some (many)
	similar columns.

	 The advantages of this mapping strategy are clearer if we examine polymorphic queries. For example, the query
	 select bd from BillingDetails bd generates the following SQL statement
			select
				 ID, OWNER, EXPMONTH, EXPYEAR, CARDNUMBER,
				 ACCOUNT, BANKNAME, SWIFT, CLAZZ_
			from
				 ( select
					 ID, OWNER, EXPMONTH, EXPYEAR, CARDNUMBER,
					 null as ACCOUNT,
					 null as BANKNAME,
					 null as SWIFT,
					 1 as CLAZZ_
				   from
					 CREDITCARD
				   union all
				   select
					 id, OWNER,
					 null as EXPMONTH,
					 null as EXPYEAR,
					 null as CARDNUMBER,
					 ACCOUNT, BANKNAME, SWIFT,
					 2 as CLAZZ_
				  from
					 BANKACCOUNT
				  ) as BILLINGDETAILS


	This SELECT uses a FROM-clause subquery to retrieve all instances of BillingDetails from all concrete class tables.
	 The tables are combined with a UNION operator, and a literal (in this case, 1 and 2) is inserted into the
	 intermediate result; Hibernate reads this to instantiate the correct class given the data from a particular row.
	 A union requires that the queries that are combined project over the same columns; hence, you have to pad and
	 fill nonexistent columns with NULL.

	 Another much more important advantage is the ability to handle polymorphic associations; for example, an
	 association mapping from User to BillingDetails would now be possible. Hibernate can use a UNION query to
	 simulate a single table as the target of the association mapping. We cover this topic in detail later in this chapter.


Table per class hierarchy   (see com.testehan.hibernate.basics.mappings.inheritance.v3.singleTable)

!!!	You can map an entire class hierarchy to a single table. This table includes columns for all properties of all
    classes in the hierarchy. The value of an extra type discriminator column or formula identifies the concrete
    subclass represented by a particular row.

		Example...abstract class has 2 fields: id and owner
		First concrete class has 3 fields: cardNumber, expMonth, expYear
		Second concrete class has 3 fields: account, bankName, swift
		
		Then, the table from the DB has the following fields:
			id,
			DB_TYPE <<Discriminator>>
			owner
			cardNumber
			expMonth
			expYear
			account
			bankName
			swift

	This mapping strategy is a winner in terms of both performance and simplicity. It’s the best-performing way to
	represent polymorphism—both polymorphic and nonpolymorphic queries perform well—and it’s even easy to write
	queries by hand. Ad hoc reporting is possible without complex joins or unions. Schema evolution is straightforward.

	There is one major problem: data integrity. You must declare columns for properties declared by subclasses to be
	nullable. If your subclasses each define several nonnullable properties, the loss of NOT NULL constraints may be
	a serious problem from the point of view of data correctness. Imagine that an expiration date for credit cards is
	required, but your database schema can’t enforce this rule because all columns of the table can be NULL. A simple
	application programming error can lead to invalid data.

	Another important issue is normalization. You’ve created functional dependencies between non-key columns,
	violating the third normal form. As always, denormalization for performance reasons can be misleading, because
	it sacrifices long-term stability, maintainability, and the integrity of data for immediate gains that may be
	also achieved by proper optimization of the SQL execution plans (in other words, ask your DBA).

	Use the SINGLE_TABLE inheritance strategy to create a table-per-class hierarchy mapping, as shown in the
	following listing

		@Entity
		@Inheritance(strategy = InheritanceType.SINGLE_TABLE)
		@DiscriminatorColumn(name = "BD_TYPE")
		public abstract class BillingDetails {
			 @Id
			 @GeneratedValue(generator = Constants.ID_GENERATOR)
			 protected Long id;
			 
			 @NotNull
			 @Column(nullable = false)
			 protected String owner;
			 // ...
		}

	The root class BillingDetails of the inheritance hierarchy is mapped to the table BILLINGDETAILS automatically.
	Shared properties of the superclass can be NOT NULL in the schema; every subclass instance must have a value. An
	implementation quirk of Hibernate requires that you declare nullability with @Column because Hibernate ignores Bean
	Validation’s @NotNull when it generates the database schema.
	
	You have to add a special discriminator column to distinguish what each row represents. This isn’t a property of the
	entity; it’s used internally by Hibernate. The column name is BD_TYPE, and the values are strings—in this case,
	"CC" or "BA". Hibernate automatically sets and retrieves the discriminator values.
	If you don’t specify a discriminator column in the superclass, its name defaults to DTYPE and the value are strings.
	All concrete classes in the inheritance hierarchy can have a discriminator value, such as CreditCard.

		@Entity
		@DiscriminatorValue("CC")
		public class CreditCard extends BillingDetails {
			 @NotNull														// Hibernate ignores the NotNulls here during DDL generation
			 protected String cardNumber;
			 @NotNull
			 protected String expMonth;
			 @NotNull
			 protected String expYear;
			 // ...
		}

	Without an explicit discriminator value, Hibernate defaults to the fully qualified class name if you use Hibernate
	XML files and the simple entity name if you use annotations or JPA XML files. Note that JPA doesn’t specify a
	default for non-string discriminator types; each persistence provider can have different defaults. Therefore, you
	should always specify discriminator values for your concrete classes.

	Annotate every subclass with @Entity, and then map properties of a subclass to columns in the BILLINGDETAILS table.
!!!!!!
	Remember that NOT NULL constraints aren’t allowed in the schema, because a BankAccount instance won’t have an
	expMonth property, and the EXPMONTH column must be NULL for that row. Hibernate ignores the @NotNull for schema
	DDL generation, but it observes it at runtime, before inserting a row. This helps you avoid programming errors;
	you don’t want to accidentally save credit card data without its expiration date. (Other, less well-behaved
	applications can of course still store incorrect data in this database.)

	Hibernate generates the following SQL for select bd from BillingDetails bd:
		select
		 ID, OWNER, EXPMONTH, EXPYEAR, CARDNUMBER,
		 ACCOUNT, BANKNAME, SWIFT, BD_TYPE
		from
		 BILLINGDETAILS

	To query the CreditCard subclass, Hibernate adds a restriction on the discriminator column
		select
		 ID, OWNER, EXPMONTH, EXPYEAR, CARDNUMBER
		from
		 BILLINGDETAILS
		where
		 BD_TYPE='CC'


	Bonus stuff :))
		Sometimes, especially in legacy schemas, you don’t have the freedom to include an
		extra discriminator column in your entity tables. In this case, you can apply an expression to calculate a
		discriminator value for each row. Formulas for discrimination aren’t part of the JPA specification, but
		Hibernate has an extension annotation, @DiscriminatorFormula.
			
			@Entity
			@Inheritance(strategy = InheritanceType.SINGLE_TABLE)
			@org.hibernate.annotations.DiscriminatorFormula("case when CARDNUMBER is not null then 'CC' else 'BA' end")
			public abstract class BillingDetails {
				// ...
			}
		
		There is no discriminator column in the schema, so this mapping relies on an SQL CASE/WHEN expression to
		determine whether a particular row represents a credit card or a bank account.
		
	The result of the expression is a literal, CC or BA, which you declare on the subclass mappings. The disadvantages
	of the table-per-class hierarchy strategy may be too serious for your design—considering denormalized schemas can
	become a major burden in the long term. Your DBA may not like it at all. The next inheritance-mapping strategy
	doesn’t expose you to this problem.


Table per subclass with joins (see com.testehan.hibernate.basics.mappings.inheritance.v4.joinedTables)

	The fourth option is to represent inheritance relationships as SQL foreign key associations. Every class/subclass
	that declares persistent properties—including abstract classes and even interfaces—has its own table.

	Unlike the table-per-concrete-class strategy we mapped first, the table of a concrete @Entity here contains columns
	only for each non-inherited property, declared by the subclass itself, along with a primary key that is also a
	foreign key of the superclass table. This is easier than it sounds; have a look at figure 6.3.
		
		
				BillingDetails (parent class)
				id : Long
				owner : String
				
		CreditCard (child 1) 			BankAccount (child 2)
		cardNumber : String					account : String
		expMonth : String					bankname : String
		expYear : String					swift : String


				<< Table >>
				BILLINGDETAILS
				ID << PK >>
				OWNER

		<< Table >>						<< Table >>
		CREDITCARD						BANKACCOUNT
		ID << PK >> << FK >>			ID << PK >> << FK >>
		CARDNUMBER						ACCOUNT
		EXPMONTH						BANKNAME
		EXPYEAR							SWIFT


	If you make an instance of the CreditCard subclass persistent, Hibernate inserts two rows. The values of properties
	declared by the BillingDetails superclass are stored in a new row of the BILLINGDETAILS table. Only the values
	of properties declared by the subclass are stored in a new row of the CREDITCARD table. The primary key shared
	by the two rows links them together. Later, the subclass instance may be retrieved from the database by joining
	the subclass table with the superclass table.

	The primary advantage of this strategy is that it normalizes the SQL schema.Schema evolution and integrity-constraint
	definition are straightforward. A foreign key referencing the table of a particular subclass may represent a
	polymorphic association to that particular subclass.
	
	Use the JOINED inheritance strategy to create a table per-subclass hierarchy mapping

		@Entity
		@Inheritance(strategy = InheritanceType.JOINED)
		public abstract class BillingDetails {
			 @Id
			 @GeneratedValue(generator = Constants.ID_GENERATOR)
			 protected Long id;
			 
			 @NotNull
			 protected String owner;
			 // ...
		}

	The root class BillingDetails is mapped to the table BILLINGDETAILS. Note that no discriminator is required with
	this strategy. (discriminator was used in previous approaches..see in the above chapters..)
!!!	In subclasses, you don’t need to specify the join column if the primary key column of the subclass table has
    (or is supposed to have) the same name as the primary key column of the superclass table.

		@Entity
		public class BankAccount extends BillingDetails {
			 @NotNull
			 protected String account;
			 
			 @NotNull
			 protected String bankname;
			 
			 @NotNull
			 protected String swift;
			 // ...
		}
	
	This entity has no identifier property; it automatically inherits the ID property and column from the superclass,
	and Hibernate knows how to join the tables if you want to retrieve instances of BankAccount. Of course, you can
	specify the column name explicitly (see below).

		@Entity
		@PrimaryKeyJoinColumn(name = "CREDITCARD_ID")
		public class CreditCard extends BillingDetails {
			 @NotNull
			 protected String cardNumber;
			
  			@NotNull
			 protected String expMonth;
			
			@NotNull
			 protected String expYear;
			 // ...
		}

	The primary key columns of the BANKACCOUNT and CREDITCARD tables each also have a foreign key constraint
	referencing the primary key of the BILLINGDETAILS table.
			
			(example of how a sql queries look for this approach)

					select
						 BD.ID, BD.OWNER,
						 CC.EXPMONTH, CC.EXPYEAR, CC.CARDNUMBER,
						 BA.ACCOUNT, BA.BANKNAME, BA.SWIFT,
						 case
							 when CC.CREDITCARD_ID is not null then 1
							 when BA.ID is not null then 2
							when BD.ID is not null then 0
						end
					from
						 BILLINGDETAILS BD
						 left outer join CREDITCARD CC on BD.ID=CC.CREDITCARD_ID
						 left outer join BANKACCOUNT BA on BD.ID=BA.ID	
	
	The SQL CASE ... WHEN clause detects the existence (or absence) of rows in the subclass tables CREDITCARD and
	BANKACCOUNT, so Hibernate can determine the concrete subclass for a particular row of the BILLINGDETAILS table.

    For a narrow subclass query like select cc from CreditCard cc, Hibernate uses an inner join:
		select
			CREDITCARD_ID, OWNER, EXPMONTH, EXPYEAR, CARDNUMBER
		from
			CREDITCARD
			inner join BILLINGDETAILS on CREDITCARD_ID=ID
		
	As you can see, this mapping strategy is more difficult to implement by hand—even ad hoc reporting is more complex.
	This is an important consideration if you plan to mix Hibernate code with handwritten SQL.
	Furthermore, even though this mapping strategy is deceptively simple, our experience is that performance can be
	unacceptable for complex class hierarchies. Queries always require a join across many tables, or many sequential reads.


Mixing inheritance strategies	

	You can map an entire inheritance hierarchy with the TABLE_PER_CLASS, SINGLE_TABLE, or JOINED strategy. You can’t
	mix them—for example, to switch from a table-per-class hierarchy with a discriminator to a normalized
	table-per-subclass strategy. Once you’ve made a decision for an inheritance strategy, you have to stick with it.
	This isn’t completely true, however. By using some tricks, you can switch the mapping strategy for a particular
	subclass. For example, you can map a class hierarchy to a single table, but, for a particular subclass, switch to
	a separate table with a foreign key– mapping strategy, just as with table-per-subclass.

		(at page 129 as written in the book, you can see the class diagram and how we expect the tables to look...the
		class diagram is the same  as the example from above, but we want for example a different table for the
		CreditCard class)


		@Entity
		@DiscriminatorValue("CC")
		@SecondaryTable(
		 name = "CREDITCARD",
		 pkJoinColumns = @PrimaryKeyJoinColumn(name = "CREDITCARD_ID")
		)
		public class CreditCard extends BillingDetails {
			@NotNull
			@Column(table = "CREDITCARD", nullable = false)
			protected String cardNumber;
			
			@Column(table = "CREDITCARD", nullable = false)
			protected String expMonth;
			
			@Column(table = "CREDITCARD", nullable = false)
			protected String expYear;
			// ...
		}

	The @SecondaryTable and @Column annotations group some properties and tell Hibernate to get them from a secondary
	table. You map all properties that you moved into the secondary table with the name of that secondary table. This
	is done with the table parameter of @Column, which we haven’t shown before. This mapping has many uses, and you’ll
	see it again later in this book. In this example, it separates the CreditCard properties from the single table
	strategy into the CREDITCARD table.
	The CREDITCARD_ID column of this table is at the same time the primary key, and it has a foreign key constraint
	referencing the ID of the single hierarchy table. If you don’t specify a primary key join column for the secondary
	table, the name of the primary key of the single inheritance table is used—in this case, ID.

!!!	Remember that InheritanceType.SINGLE_TABLE enforces all columns of subclasses to be nullable. One of the benefits of
    this mapping is that you can now declare columns of the CREDITCARD table as NOT NULL, guaranteeing data integrity.


Inheritance of embeddable classes

	An embeddable class is a component of its owning entity; hence, the normal entity inheritance rules presented in
	this chapter don’t apply. As a Hibernate extension, you can map an embeddable class that inherits some persistent
	properties from a superclass (or interface).

		@MappedSuperclass
		public abstract class Measurement {
			 @NotNull
			 protected String name;
			 @NotNull
			 protected String symbol;
			 // ...
		}
	
	Use the @MappedSuperclass annotation on the superclass of the embeddable class you’re mapping just like you would
	for an entity. Subclasses will inherit the properties of this class as persistent properties.

		@Embeddable
		@AttributeOverrides({
			@AttributeOverride(name = "name", column = @Column(name = "DIMENSIONS_NAME")),
			@AttributeOverride(name = "symbol", column = @Column(name = "DIMENSIONS_SYMBOL"))
		})
		public class Dimensions extends Measurement {
			 @NotNull
			 protected BigDecimal depth;
			 @NotNull
			 protected BigDecimal height;
			 @NotNull
			 protected BigDecimal width;
			 // ...
		}

	You define the Dimensions and Weight subclasses as @Embeddable. 
	Without this override, an Item embedding both Dimension and Weight would map to a table with conflicting column
	names. Following is the Weight class; its mapping also overrides the column names with a prefix (for uniformity,
	 we avoid the conflict with the previous override).

		@Embeddable
		@AttributeOverrides({
			@AttributeOverride(name = "name", column = @Column(name = "WEIGHT_NAME")),
			@AttributeOverride(name = "symbol", column = @Column(name = "WEIGHT_SYMBOL"))
		})
		public class Weight extends Measurement {
			@NotNull
			@Column(name = "WEIGHT")
			protected BigDecimal value;
			// ...
		}

	The owning entity Item defines two regular persistent embedded properties.
		@Entity
		public class Item {
			protected Dimensions dimensions;
			protected Weight weight;
			// ...
		}	

	The table for the examples from above will look like:
	
							<< Table >>
							ITEM
							
							ID << PK >>
							NAME
							...
							DIMENSIONS_NAME
							DIMENSIONS_SYMBOL
							DEPTH
							HEIGHT
							WIDTH
							WEIGHT_NAME
							WEIGHT_SYMBOL
							WEIGHT

!!!	A pitfall to watch out for is embedding a property of abstract superclass type (like Measurement) in an entity
(like Item). This can never work; the JPA provider doesn’t know how to store and load Measurement
instances polymorphically. It doesn’t have the information necessary to decide whether the values in the database
are Dimension or Weight instances, because there is no discriminator. This means although you can have
an @Embeddable class inherit some persistent properties from a @MappedSuperclass, the reference to an instance
isn’t polymorphic—it always names a concrete class.

	Compare this (see paragraph from above) with the alternative inheritance strategy for embeddable classes shown in
	the section “Converting properties of components,” in chapter 5, which supported polymorphism but required some
	 custom type-discrimination code.


Choosing a strategy

	Picking the right inheritance-mapping strategy depends on usage of the superclasses of your entity hierarchy. You
	have to consider how frequently you query for instances of the superclasses and whether you have associations
	targeting the superclasses. Another important aspect is the attributes of super- and subtypes: whether subtypes
	have many additional attributes or only different behavior than their supertypes. Here are some rules of thumb:
		
		 If you don’t require polymorphic associations or queries, lean toward table-per concrete class—in other words,
		if you never or rarely select bd from BillingDetails bd and you have no class that has an association to
		BillingDetails. An explicit UNION-based mapping with InheritanceType.TABLE_PER_CLASS should be preferred,
		because (optimized) polymorphic queries and associations will then be possible later

		 If you do require polymorphic associations (an association to a superclass, hence to all classes in the
		hierarchy with dynamic resolution of the concrete class at runtime) or queries, and subclasses declare
		relatively few properties (particularly if the main difference between subclasses is in their behavior), lean
		toward InheritanceType.SINGLE_TABLE. Your goal is to minimize the number of nullable columns and to convince
		yourself (and your DBA) that a denormalized schema won’t create problems in the long run.

		 If you do require polymorphic associations or queries, and subclasses declare many (non-optional) properties
		(subclasses differ mainly by the data they hold), lean toward InheritanceType.JOINED. Alternatively, depending
		on the width and depth of your inheritance hierarchy and the possible cost of joins versus unions, use
		InheritanceType.TABLE_PER_CLASS. This decision might require evaluation of SQL execution plans with real data.

	By default, choose InheritanceType.SINGLE_TABLE only for simple problems. Otherwise, for complex cases, or when a
	data modeler insisting on the importance of NOT NULL constraints and normalization overrules you, you should
	consider the InheritanceType.JOINED strategy

	Complex inheritance is often best avoided for all sorts of reasons unrelated to persistence or ORM. Hibernate acts
	as a buffer between the domain and relational models, but that doesn’t mean you can ignore persistence concerns
	completely when designing your classes

	When you start thinking about mixing inheritance strategies, remember that implicit polymorphism in Hibernate is
	smart enough to handle exotic cases. Also, consider that you can’t put inheritance annotations on interfaces;
	this isn’t standardized in JPA.

	For example, consider an additional interface in the example application: ElectronicPaymentOption. This is a
	business interface that doesn’t have a persistence aspect—except that in the application, a persistent class
	such as CreditCard will likely implement this interface. No matter how you map the BillingDetails hierarchy,
	Hibernate can answer the query select o from ElectronicPaymentOption o correctly.
	This even works if other classes, which aren’t part of the BillingDetails hierarchy, are mapped as persistent and
	implement this interface. Hibernate always knows what tables to query, which instances to construct, and how to
	return a polymorphic result.  You can apply all mapping strategies to abstract classes. Hibernate won’t try to
	instantiate an abstract class, even if you query or load it.


Polymorphic associations

    Polymorphism is a defining feature of object-oriented languages like Java. Support for polymorphic associations
    and polymorphic queries is a fundamental feature of an ORM solution like Hibernate

Polymorphic many-to-one associations
	First, consider the defaultBilling property of User. It references one particular BillingDetails instance, which
	at runtime can be any concrete instance of that class. You map this unidirectional association to the abstract
	class BillingDetails as
	follows:
		
		@Entity
		@Table(name = "USERS")
		public class User {
			 
			 @ManyToOne(fetch = FetchType.LAZY)
			 protected BillingDetails defaultBilling;
			 // ...
		}
	
	The USERS table now has the join/foreign key column DEFAULTBILLING_ID representing this relationship. It’s a
	nullable column because a User might not have a default billing method assigned. Because BillingDetails is abstract,
	the association must refer to an instance of one of its subclasses—CreditCard or BankAccount—at runtime. You don’t
	have to do anything special to enable polymorphic associations in Hibernate; if the target class of an association
	is mapped with @Entity and @Inheritance, the association is naturally polymorphic.
	
		CreditCard cc = new CreditCard("John Doe", "1234123412341234", "06", "2015");
		User johndoe = new User("johndoe");
		johndoe.setDefaultBilling(cc);
		
		em.persist(cc);
		em.persist(johndoe);
	
	Now, when you navigate the association in a second unit of work, Hibernate automatically retrieves the CreditCard
	instance:
		User user = em.find(User.class, USER_ID);
		user.getDefaultBilling().pay(123);

!!!	There’s just one thing to watch out for: because the defaultBilling property is mapped with FetchType.LAZY,
	Hibernate will proxy the association target. In this case, you wouldn’t be able to perform a typecast to the
	concrete class CreditCard at runtime, and even the instanceof operator would behave strangely:

		BillingDetails bd = user.getDefaultBilling();
		assertFalse(bd instanceof CreditCard);
		// CreditCard creditCard = (CreditCard) bd;						this will throw a ClassCastException

	The bd reference isn’t a CreditCard instance in this case; it’s a runtime-generated special subclass of
	BillingDetails, a Hibernate proxy. When you invoke a method on the proxy, Hibernate delegates the call to an
	instance of CreditCard that it fetches lazily. Until this initialization occurs, Hibernate doesn’t know what
	the subtype of the given instance is—this would require a database hit, which you try to avoid with lazy loading
	in the first place.

		User user = em.find(User.class, USER_ID);
		BillingDetails bd = user.getDefaultBilling();
		CreditCard creditCard = em.getReference(CreditCard.class, bd.getId());
		assertTrue(bd != creditCard);
	
	After the getReference() call, bd and creditCard refer to two different proxy instances, both of which delegate to
	the same underlying CreditCard instance. The second proxy has a different interface, though, and you can call methods
	like creditCard .getExpMonth() that apply only to this interface. (Note that bd.getId() will trigger a SELECT if
	you map the id property with field access.)

    You can avoid these issues by avoiding lazy fetching, as in the following code, using an eager fetch query:
		User user = (User) em.createQuery(
		 "select u from User u " +
		 "left join fetch u.defaultBilling " +
		 "where u.id = :id")
		 .setParameter("id", USER_ID)
		 .getSingleResult();
		CreditCard creditCard = (CreditCard) user.getDefaultBilling();

	Truly object-oriented code shouldn’t use instanceof or numerous typecasts. If you find yourself running into
	problems with proxies, you should question your design, asking whether there is a more polymorphic approach.


Polymorphic collections
	A User may have references to many BillingDetails, not only a single default. You can map this with a
	bidirectional one-to-many association:
		@Entity
		@Table(name = "USERS")
		public class User {
			 @OneToMany(mappedBy = "user")
			 protected Set<BillingDetails> billingDetails = new HashSet<>();
			 // ...
		}

		@Entity
		@Inheritance(strategy = InheritanceType.TABLE_PER_CLASS)
		public abstract class BillingDetails {
			 
			 @ManyToOne(fetch = FetchType.LAZY)
			 protected User user;
			 // ...
		}

	So far, there is nothing special about this association mapping. The BillingDetails class hierarchy can be
	mapped with TABLE_PER_CLASS, SINGLE_TABLE, or a JOINED inheritance type. Hibernate is smart enough to use the
	right SQL queries, with either JOIN or UNION operators, when loading the collection elements.
!!!	There is one limitation, however: the BillingDetails class can’t be a @MappedSuperclass, as shown in section 6.1.
	It has to be mapped with @Entity and @Inheritance.


Summary

	 Table per concrete class with implicit polymorphism is the simplest strategy to map inheritance hierarchies of
	entities, but it doesn’t support polymorphic associations very well. In addition, different columns from different tables share
	exactly the same semantics, making schema evolution more complex. We recommend this approach for the top level of your
	class hierarchy only, where polymorphism isn’t usually required and when modification of the superclass in the future is unlikely.
	 The table-per-concrete-class-with-unions strategy is optional, and JPA implementations may not support it, but
	it does handles polymorphic associations.
	 The table-per-class-hierarchy strategy is a winner in terms of both performance and simplicity; ad hoc reporting
	is possible without complex joins or unions, and schema evolution is straightforward. The one major problem is data
	integrity, because you must declare some columns as nullable. Another issue is normalization: this strategy creates
	functional dependencies between non-key columns, violating the third normal form.
	 The table-per-subclass-with-joins strategy’s primary advantage is that it normalizes the SQL schema, making schema
	evolution and integrity constraint definition straightforward. The disadvantages are that it’s more difficult to
	 implement by hand, and performance can be unacceptable for complex class hierarchies.





Mapping collections and entity associations

	Managing the associations between classes and the relationships between tables is at the heart of ORM. Most of the
	difficult problems involved in implementing an ORM solution relate to collections and entity association management.
	Feel free to come back to this chapter to grok this topic fully. We start this chapter with basic collection-mapping
	concepts and simple examples.


Sets, bags, lists, and maps of value types

	Java has a rich collection API, from which you can choose the interface and implementation that best fits your domain
	model design. Let’s walk through the most common collection mappings, repeating the same Image and Item example with
	minor variations. We’ll start by looking at the database schema and creating and mapping a collection property in
	general. Then we’ll proceed to how to select a specific collection interface and map various collection types: a set,
	identifier bag, list, map, and finally sorted and ordered collections.

		Database schema Example that will be used in the next chapters:
			 You need an IMAGE table in the database to hold the images, or maybe just the filenames of images. This table
			 also has a foreign key column, say ITEM_ID, referencing the ITEM table. Look at the schema shown in
			 figure 7.1. You could have an ON DELETE CASCADE option on the ITEM_ID foreign key column. When the application
			 deletes an ITEM row, the database automatically deletes IMAGE rows referencing this ITEM in the database.
			 Let’s assume for now that this isn’t the case.


Creating and mapping a collection property

	 There are no mapped collections; they aren’t necessary. When you need an item’s images, you write and execute a query
	 in the JPA query language: select img from Image img where img.item = :itemParameter. Persistent collections are
	 always optional—a feature. The collection you could create is Item#images, referencing all images for a particular
	 item. You create and map this collection property to do the following:

!!!		 Execute the SQL query SELECT * from IMAGE where ITEM_ID = ? automatically when you call someItem.getImages(). As
        long as your domain model instances are in a managed state (more later), you can read from the database on-demand
        while navigating the associations between your classes. You don’t have to manually write and execute a query with
        the EntityManager to load data. On the other hand, the collection query when you start iterating the collection is
        always “all images for this item,” never “only images that match criteria XYZ.”
!!!		 Avoid saving each Image with entityManager.persist(). If you have a mapped collection, adding the Image to the
        collection with someItem.getImages().add() will make it persistent automatically when the Item is saved. This
        cascading persistence is convenient because you can save instances without calling EntityManager.
		 Have a dependent life cycle of Images. When you delete an Item, Hibernate deletes all attached Images with an
		extra SQL DELETE. You don’t have to worry about the life cycle of images and cleaning up orphans (assuming your
		database foreign key constraint doesn’t ON DELETE CASCADE). The JPA provider handles the composition life cycle.

	It’s important to realize that although these benefits sound great, the price you pay is additional mapping complexity.
	We’ve seen many JPA beginners struggle with collection mappings, and frequently the answer to “Why are you doing this?”
	has been “I thought this collection was required.”

	Analyzing the scenario with images for auction items, you’d benefit from a collection mapping. The images have a
	dependent life cycle; when you delete an item, all the attached images should be deleted. When an item is stored, all
	attached images should be stored. And when you display an item, you often also display all images, so someItem.getImages()
	is convenient in UI code. You don’t have to call the persistence service again to get the images; they’re just there


Selecting a collection interface

!!!
	The idiom for a collection property in the Java domain model is:
		<<Interface>> images = new <<Implementation>>();
		// Getter and setter methods
		// ...
	
	Use an interface to declare the type of the property, not an implementation. Pick a matching implementation, and initialize
	the collection right away; doing so avoids uninitialized collections. We don’t recommend initializing collections late,
	in constructors or setter methods. Using generics, here’s a typical Set:
		Set<String> images = new HashSet<String>()

	Raw collections without generics
	If you don’t specify the type of collection elements with generics, or the key/value types of a map, you need to tell
	Hibernate the type(s). For example, instead of a Set<String>, you map a raw Set with @ElementCollection(targetClass=String.class).
	This also applies to type parameters of a Map. Specify the key type of a Map with @MapKeyClass. All the examples in this
	book use generic collections and maps, and so should you.
	
	Out of the box, Hibernate supports the most important JDK collection interfaces and preserves the semantics of JDK
	collections, maps, and arrays in a persistent fashion. Each JDK interface has a matching implementation supported by
	Hibernate, and it’s important that you use the right combination. Hibernate wraps the collection you’ve already
	initialized on declaration of the field, or sometimes replaces it, if it’s not the
	right one. It does that to enable, among other things, lazy loading and dirty checking of collection elements.

!!!!!!!!!!!!!!
	 Without extending Hibernate, you can choose from the following collections:
		 A java.util.Set property, initialized with a java.util.HashSet. The order of elements isn’t preserved, and duplicate
		elements aren’t allowed. All JPA providers support this type.
		 A java.util.SortedSet property, initialized with a java.util.TreeSet. This collection supports stable order of elements:
		sorting occurs in-memory, after Hibernate loads the data. This is a Hibernate-only extension; other JPA providers may ignore
		the “sorted” aspect of the set.
		 A java.util.List property, initialized with a java.util.ArrayList. Hibernate preserves the position of each element with an
		additional index column in the database table. All JPA providers support this type.
		 A java.util.Collection property, initialized with a java.util.ArrayList. This collection has bag semantics; duplicates are
		possible, but the order of elements isn’t preserved. All JPA providers support this type.
		 A java.util.Map property, initialized with a java.util.HashMap. The key and value pairs of a map can be preserved in the
		database. All JPA providers support this type.
		 A java.util.SortedMap property, initialized with a java.util.TreeMap. It supports stable order of elements: sorting occurs
		in-memory, after Hibernate loads the data. This is a Hibernate-only extension; other JPA providers may ignore the “sorted”
		aspect of the map.
		 Hibernate supports persistent arrays, but JPA doesn’t. They’re rarely used, and we won’t show them in this book: Hibernate
		can’t wrap array properties, so many benefits of collections, such as on-demand lazy loading, won’t work. Only use
		persistent arrays in your domain model if you’re sure you won’t need lazy loading. (You can load arrays on-demand,
		but this requires interception with bytecode enhancement, as explained in section 12.1.3.)
!!!!!!!!!!

	If you want to map collection interfaces and implementations not directly supported by Hibernate, you need to tell Hibernate
	about the semantics of your custom collections. The extension point in Hibernate is the PersistentCollection interface in
	the org.hibernate.collection.spi package, where you usually extend one of the existing PersistentSet, PersistentBag,
	and PersistentList classes. Custom persistent collections aren’t easy to write, and we don’t recommend doing this if
	you aren’t an experienced Hibernate user. You can find an example in the source code for the Hibernate test suite.


Mapping a set

	The simplest implementation is a Set of String image filenames. Add a collection property to the Item class, as shown in the following listing
		
		@Entity
		public class Item {
			 @ElementCollection
			 @CollectionTable(name = "IMAGE",								// if this wouldn't have been specified, it would have Defaulted to "ITEM_IMAGES"
				joinColumns = @JoinColumn(name = "ITEM_ID"))			
			 @Column(name = "FILENAME")										// if this wouldn't have been specified, it would have Defaulted  to "IMAGES"
			 protected Set<String> images = new HashSet<String>();			// Initialize the field here.
			 // ...
		}
	
	The @ElementCollection JPA annotation is required for a collection of value-typed elements. Without the
	@CollectionTable and @Column annotations, Hibernate would use default schema names.

		DB tables:
		
		ITEM								Image
		ID 		Name						ITEM_ID			Filename			
		1		Foo							1				foo.jpg
		2		b							1				bar.jpg
		3		c							1				baz.jpg
											2				b.jpg

	Primary key is composed in Image table out of both columns...each item can have a picture attached only once.
	This fits the domain model and Set collection.


Mapping an identifier bag
	
	A bag is an unordered collection that allows duplicate elements, like the java.util.Collection interface.
	Curiously, the Java Collections framework doesn’t include a bag implementation. You initialize the property
	with an ArrayList, and Hibernate ignores the index of elements when storing and loading elements.

		@Entity
		public class Item {
			 @ElementCollection
			 @CollectionTable(name = "IMAGE")
			 @Column(name = "FILENAME")
			 @org.hibernate.annotations.CollectionId(												//Surrogate primary key allows duplicates
			 columns = @Column(name = "IMAGE_ID"),													// Surrogate primary key column
			 type = @org.hibernate.annotations.Type(type = "long"),									// Hibernate only annotation
			 generator = Constants.ID_GENERATOR)											// Configures primary key
			 protected Collection<String> images = new ArrayList<String>();
			 // ...
		}

	This looks much more complex: you can’t continue with the same schema as before. The IMAGE collection table
	needs a different primary key to allow duplicate FILENAME values for each ITEM_ID. You introduce a surrogate
	primary key column named IMAGE_ID B, and you use a Hibernate-only annotation C to configure how the primary
	key is generated.

		the difference in how the DB tables look from the ones right above, is that we have an extra column in
		Image, IMAGE_ID, which will be the PK in that table...


	Obviusly, we could have mapped each table with an @Entity class. But, instead, we decided to use a JPA feature
	and map a collection to IMAGE, however, even with a composition life cycle. This is, effectively, a decision
	that some predefined query and manipulation rules are all you need for this table, instead of the more generic
	 @Entity mapping. When you make such a decision, be sure you know the reasons and consequences.


Mapping a list
	
	When you haven’t used ORM software before, a persistent list seems to be a very powerful concept; imagine how
	much work storing and loading a java.util.List<String> is with plain JDBC and SQL. If you add an element to
	the middle of the list, depending on the list implementation, the list shifts all subsequent elements to the
	right or rearranges pointers. If you remove an element from the middle of the list, something else happens,
	and so on. If the ORM software can do all of this automatically for database records, this makes a persistent
	list look more appealing than it actually is.  As we noted in section 3.2.4, the first reaction is often
	to preserve the order of data elements as users enter them. You often have to show them later in the same
	order. But if another criterion can be used for sorting the data, like an entry timestamp, you should sort
	the data when querying and not store the display order. What if the display order changes? The order the
	data is displayed in is most likely not an integral part of the data, but an orthogonal concern. Think twice
	before you map a persistent List;
	Hibernate isn’t as smart as you might think, as you’ll see in the next example. 

		@Entity
		public class Item {
			 @ElementCollection
			 @CollectionTable(name = "IMAGE")
			 @OrderColumn												// Enables persistent order; defaults to IMAGES_ORDER
			 @Column(name = "FILENAME")
			 protected List<String> images = new ArrayList<String>();
			 // ...
		}

!!!
	There is a new annotation in this example: @OrderColumn. This column stores an index in the persistent list,
	starting at zero. Note that Hibernate stores and expects the index to be contiguous in the database. If there
	are gaps, Hibernate will add null elements when loading and constructing the List.

	The primary key of the IMAGE table is a composite of ITEM_ID and IMAGES_ORDER. This allows duplicate FILENAME
	values, which is consistent with the semantics of a List.

		How the DB tables look ?
		
				ITEM								Image
		ID 		Name						ITEM_ID		Images_order		Filename			
		1		Foo							1				0				foo.jpg
		2		b							1				1				bar.jpg
		3		c							1				2				baz.jpg
											2				0				b.jpg
											
!!!!!											
	We said earlier that Hibernate isn’t as smart as you might think. Consider modifications to the list: say the
	list has the three elements A, B, and C, in that order. What happens if you remove A from the list? Hibernate
	executes one SQL DELETE for that row. Then it executes two UPDATEs, for B and C, shifting their position to
	the left to close the gap in the index. For each element to the right of the deleted element,
	Hibernate executes an UPDATE. If you write SQL for this by hand, you can do it with one UPDATE. The same is
	true for insertions in the middle of the list. Hibernate shifts all existing elements to the right one by one.
	At least Hibernate is smart enough to execute a single DELETE when you clear() the list.

Mapping a map
		
	Now, suppose the images for an item have user-supplied names in addition to the filename. One way to model this in
	Java is with a map, using key/value pairs.

	@Entity
	public class Item {
		 @ElementCollection
		 @CollectionTable(name = "IMAGE")
		 @MapKeyColumn(name = "FILENAME")				// map keys
		 @Column(name = "IMAGENAME")					// map values
		 protected Map<String, String> images = new HashMap<String, String>();
		 // ...
	}

	Each map entry is a key/value pair. Here you map the key with @MapKeyColumn to FILENAME and the value to the
	IMAGENAME column. This means the user can only use a file once, because a Map doesn’t allow duplicate keys.

	As you can see from the schema in figure 7.5, the primary key of the collection table is a composite of ITEM_ID and
	FILENAME. The example uses a String as the key for the map; but Hibernate supports any basic type, such as
	BigDecimal and Integer. If the key is a Java enum, you must use @MapKeyEnumerated. With any temporal types such
	as java.util.Date, use @MapKeyTemporal etc..


Sorted and ordered collections

	The map in the previous example was unordered. What should you do to always sort map entries by filename? 
!!!	In a startling abuse of the English language, the words sorted and ordered mean different things when it comes to
    persistent collections in Hibernate. You sort a collection in memory using a Java comparator. You order a collection
    when it’s loaded from the database, using an SQL query with an ORDER BY clause.

		@Entity
		public class Item {
			 @ElementCollection
			 @CollectionTable(name = "IMAGE")
			 @MapKeyColumn(name = "FILENAME")
			 @Column(name = "IMAGENAME")
			 @org.hibernate.annotations.SortComparator(ReverseStringComparator.class)
			 protected SortedMap<String, String> images = new TreeMap<String, String>();
			 // ...
		}

	Sorted collections are a Hibernate feature; hence the org.hibernate.annotations.SortComparator annotation with an
	implementation of java.util.Comparator. We won’t show you this trivial class here; it sorts strings in reverse order.

		@Entity
		public class Item {
			 @ElementCollection
			 @CollectionTable(name = "IMAGE")
			 @Column(name = "FILENAME")
			 @org.hibernate.annotations.SortNatural
			 protected SortedSet<String> images = new TreeSet<String>();
			 // ...
		}
	
	Here natural sorting is used, falling back on the String#compareTo() method.
	Unfortunately, you can’t sort a bag; there is no TreeBag. The indexes of list elements predefine their order.
	Alternatively, instead of switching to Sorted* interfaces, you may want to retrieve
	the elements of a collection in the right order from the database, and not sort in memory. Instead of a
	java.util.SortedSet, a java.util.LinkedHashSet is used in the following example.

		@Entity
		public class Item {
			 @ElementCollection
			 @CollectionTable(name = "IMAGE")
			 @Column(name = "FILENAME")
			 // @javax.persistence.OrderBy													// Only one possible order: "FILENAME asc"
			 @org.hibernate.annotations.OrderBy(clause = "FILENAME desc")
			 protected Set<String> images = new LinkedHashSet<String>();
			 // ...
		}

	The LinkedHashSet class has a stable iteration order over its elements, and Hibernate will fill it in the right order
	when loading a collection. To do this, Hibernate applies an ORDER BY clause to the SQL statement that loads the
	collection. You must declare this SQL clause with the proprietary @org.hibernate.annotations.OrderBy annotation.
	You could even call an SQL function, like @OrderBy("substring(FILENAME, 0, 3) desc"), which would sort by the first
	three letters of the filename.

	You can apply the annotation @org.hibernate.annotations.OrderBy to any collection; its parameter is a plain SQL
	fragment that Hibernate attaches to the SQL statement loading the collection. Java Persistence has a similar
	annotation, @javax.persistence.OrderBy. Its (only) parameter is not SQL but "someProperty DESC|ASC".

!!!!
	Keep in mind that the elements of ordered collections are only in the desired order when they’re loaded. As soon as
	you add and remove elements, the iteration order of the collections might be different than “by filename”; they behave
	like regular linked sets, maps, or lists.  In a real system, it’s likely that you’ll need to keep more than just the
	image name and filename. You’ll probably need to create an Image class for this extra information. This is the
	perfect use case for a collection of components.


Collections of components

	You mapped an embeddable component earlier: the address of a User in section 5.2. The current situation is different
	because an Item has many references to an Image, as shown in figure 7.6. The association in the UML diagram
	is a composition (the black diamond); hence, the referenced Images are bound to the life cycle of the owning Item.
	The code in the next listing shows the new Image embeddable class, capturing all the properties of an image that
	interest you.

		@Embeddable
		public class Image {
			 @Column(nullable = false)
			 protected String title;
			 
			 @Column(nullable = false)
			 protected String filename;
			 
			 protected int width;
			 
			 protected int height;
			 
			 // ...
		}

!!!!
	First, note that all properties are non-optional, NOT NULL. The size properties are nonnullable because their
	values are primitives. Second, you have to consider equality and how the database and Java tier compare two images.
!!!

Equality of component instances

	Let’s say you keep several Image instances in a HashSet. You know that sets don’t allow duplicate elements. How do
	sets detect duplicates? The HashSet calls the equals() method on each Image you put in the Set. (It also calls the
	hashCode() method to get a hash, obviously.)
	You’re right: the regular Java equality check relies on identity. The java.lang.Object#equals() method compares
	instances with a==b. Using this procedure, you’d have four instances of Image in the collection.
	Clearly, three is the “correct” answer for this use case.  For the Image class, you don’t rely on Java identity—you
	override the equals() and hashCode() methods


			@Embeddable
			public class Image {
			
				 @Override
				 public boolean equals(Object o) {								// equality check
					 if (this == o) return true;
					 if (o == null || getClass() != o.getClass()) return false;
					 Image other = (Image) o;
					 if (!title.equals(other.title)) return false;
					 if (!filename.equals(other.filename)) return false;
					 if (width != other.width) return false;
					 if (height != other.height) return false;
					 return true;
				 }
				 
				 @Override
				 public int hashCode() {										// must be symmetric
					 int result = title.hashCode();
					 result = 31 * result + filename.hashCode();
					 result = 31 * result + width;
					 result = 31 * result + height;
					 return result;
				 }
				 // ...
			}
	
	This custom equality check in equals() B compares all values of one Image to the values of another Image. If all
	values are the same, then the images must be the same. The hashCode() C method has to be symmetric; if two
	instances are equal, they must have the same hash code.

!!!
	you don’t have any problems with the regular identity equality unless you put embeddable components into a Set or
	use them as keys in a Map. Then you should redefine equality based on values, not identity. It’s best if you
	override these methods on every @Embeddable class; all value types should be compared “by value.”


Set of components

		@Entity
		public class Item {
			 @ElementCollection																// Required
			 @CollectionTable(name = "IMAGE")												// Overrides collection table name
			 @AttributeOverride(
				name = "filename",
				column = @Column(name = "FNAME", nullable = false)
			 )
			 protected Set<Image> images = new HashSet<Image>();
			 // ...
		}

	As before, the @ElementCollection annotation is required. Hibernate automatically knows that the target of the
	collection is an @Embeddable type, from your declaration of a generic collection. The @CollectionTable annotation
	overrides the default name for the collection table, which would have been ITEM_IMAGES.

	The Image mapping defines the columns of the collection table. Just as for a single embedded value, you can use
	@AttributeOverride to customize the mapping without modifying the target embeddable class.

	You’re mapping a set, so the primary key of the collection table is a composite of the foreign key column
	ITEM_ID and all “embedded” non-nullable columns: TITLE, FNAME, WIDTH, and HEIGHT.

	The ITEM_ID value wasn’t included in the overridden equals() and hashCode() methods of Image, as discussed in the
	previous section. Therefore, if you mix images of different items in one set, you’ll run into equality problems in
	the Java tier. In the database table, you obviously can distinguish images of different items, because the item’s
	identifier is included in primary key equality checks

	If you want to include the Item in the equality routine of the Image, to be symmetric with the database primary key,
	you need an Image#item property. This is a simple back-pointer, provided by Hibernate when Image instances are loaded:
			@Embeddable
			public class Image {
				 @org.hibernate.annotations.Parent
				 protected Item item;
				 // ...
			}

	You can now get the parent Item value in the equals() and hashCode() implementations and write, for example, a
	comparison with this.getItem().getId().equals(other.getItem().getId()). Be careful if the Item isn’t persistent
	and has no identifier value;


Components as map keys

	Our final example is a mapping a Map, with both keys and values of embeddable type, as you can see in figure

			@Entity
			public class Item {
				@ElementCollection
				@CollectionTable(name = "IMAGE")
				protected Map<Filename, Image> images = new HashMap<Filename, Image>();
				// ...
			}

	You can’t apply @MapKeyColumn and @AttributeOverrides; they have no effect when the map’s key is an @Embeddable
	class (Filename in this case).


Mapping entity associations

	At the beginning of this chapter, we promised to talk about parent/children relationships. So far, you’ve mapped an
	entity, Item. Let’s say this is the parent. It has a collection of children: the collection of Image instances.
	The term parent/child implies some kind of life cycle dependency, so a collection of strings or embeddable components
	is appropriate. The children are fully dependent on the parent; they will be saved,
	updated, and removed always with the parent, never alone.

!!	You already mapped a parent/child relationship! The parent was an entity, and the many children were of value type.

	Now you want to map a relationship of a different kind: an association between two entity classes. Their instances
	don’t have a dependent life cycle. An instance can be saved, updated, and removed without affecting any other.
	Naturally, sometimes you have dependencies even between entity instances. You need more fine-grained control of how
	the relationship between two classes affects instance state, not completely dependent (embedded) types. Are we still
	talking about a parent/child relationship? It turns out that parent/child is vague, and everyone has their own
	definition. We’ll try not to use that term from now on and will instead rely on more-precise or at least well
	defined vocabulary.

!!!
	The relationship we’ll explore in the following sections is always the same, between the Item and Bid entity
	classes. The association from Bid to Item is a many-to-one association. Later you’ll make this association
	bidirectional, so the inverse association from Item to Bid will be one-to-many.  The many-to-one association
	is the simplest, so we’ll talk about it first. The other associations, many-to-many and one-to-one, are more
	complex and we’ll discuss them in the next chapter. Let’s start with the many-to-one association


The simplest possible association	(@ManyToOne)
	
	We call the mapping of the Bid#item property a unidirectional many-to-one association. Before we discuss
	this mapping, look at the database schema in figure 7.14.

		<<Table Item>>								<<Table BID>>
		ID <<PK>>									ID <<PK>>	
		NAME										ITEM_ID	<<FK>>
													AMOUNT

		@Entity
		public class Bid {
			 @ManyToOne(fetch = FetchType.LAZY)								// Defaults to EAGER
			 @JoinColumn(name = "ITEM_ID", nullable = false)
			 protected Item item;
			 // ...
		}

!!!
	The @ManyToOne annotation marks a property as an entity association, and it’s required. Unfortunately, its fetch
	parameter defaults to EAGER: this means the associated Item is loaded whenever the Bid is loaded. We usually
	prefer lazy loading as a default strategy, and we’ll talk more about it later in section 12.1.1.
	A many-to-one entity association maps naturally to a foreign key column: ITEM_ID in the BID table. In JPA, this
	is called the join column. You don’t need anything but the @ManyToOne annotation on the property. The default
	name for the join column is ITEM_ID: Hibernate automatically uses a combination of the target entity name and
	its identifier property, separated with an underscore.

	You can override the foreign key column with the @JoinColumn annotation. We used it here for a different reason:
	to make the foreign key column NOT NULL when Hibernate generates the SQL schema. A bid always has to have a reference
	to an item; it can’t survive on its own. (Note that this already indicates some kind of life cycle dependency you
	have to keep in mind.) Alternatively, you could mark this association as non-optional with either
	 @ManyToOne(optional = false) or, as usual, Bean Validation’s @NotNull.

	This was easy. It’s critically important to realize that you can write a complete and complex application without
	using anything else.

	You don’t need to map the other side of this relationship; you can ignore the one-tomany association from Item to
	Bid. There is only a foreign key column in the database schema, and you’ve already mapped it. We are serious
	about this: when you see a foreign key column and two entity classes involved, you should probably map it
	with @ManyToOne and nothing else. You can now get the Item of each Bid by calling someBid.getItem(). The JPA
	provider will dereference the foreign key and load the Item for you; it also takes care of managing the foreign key
	values. How do you get all of an item’s bids? Well, you write a query and execute it with EntityManager, in whatever
	query language Hibernate supports. For example, in JPQL, you’d use select b from Bid b where b.item = :itemParameter.
	One of the reasons you use a full ORM tool like Hibernate is, of course, that you don’t want to write and execute
	that query yourself...SO check out next part


Making it bidirectional

	At the beginning of this chapter, we had a list of reasons a mapping of the collection Item#images was a good idea.
	Let’s do the same for the collection Item#bids. This collection would implement the one-to-many association between
	Item and Bid entity classes. If you create and map this collection property, you get the following:
		 Hibernate executes the SQL query SELECT * from BID where ITEM_ID = ? automatically when you call
		someItem.getBids() and start iterating through the collection elements.
		 You can cascade state changes from an Item to all referenced Bids in the collection. You can select what life
		cycle events should be transitive: for example, you could declare that all referenced Bid instances should be
		saved when the Item is saved, so you don’t have to call EntityManager#persist() repeatedly for all bids.

	Well, that isn’t a very long list. The primary benefit of a one-to-many mapping is navigational access to data.
	It’s one of the core promises of ORM, enabling you to access data by calling only methods of your Java domain model.
	The ORM engine is supposed to take care of loading the required data in a smart way while you work with a
	highlevel interface of your own design: someItem.getBids().iterator().next().getAmount(), and so on.

!!!!
	So, should you map the Item#bids collection at all? You get navigational data access, but the price you pay is
	additional Java code and significantly more complexity.
	This is frequently a difficult decision. How often will you call someItem.getBids() in your application and then
	access/display all bids in a predefined order? If you only want to display a subset of bids, or if you need to
	retrieve them in a different order every time, then you need to write and execute queries manually anyway.
	The one-to many mapping and its collection would only be maintenance baggage.
	In our experience, this is a frequent source of problems and bugs, especially for ORM beginners.
	
		@Entity
		public class Item {
			@OneToMany(mappedBy = "item", fetch = FetchType.LAZY)
			protected Set<Bid> bids = new HashSet<>();
			// ...
		}
	The @OneToMany annotation is required. In this case, you also have to set the mappedBy parameter.
	The argument is the name of the property on the “other side.”

	The default for the fetch parameter of a collection mapping is always FetchType.LAZY.
	You won’t need this option in the future. It’s a good default setting; the opposite would be the rarely needed
	EAGER. You don’t want all the bids eagerly loaded every time you load an Item. They should be loaded when accessed,
	on demand.


Cascading state

	If an entity state change can be cascaded across an association to another entity, you need fewer lines of code to
	manage relationships. The following code creates a new Item and a new Bid and then links them:

		Item someItem = new Item("Some Item");
		Bid someBid = new Bid(new BigDecimal("123.00"), someItem);
		someItem.getBids().add(someBid);

	With the current mapping of @ManyToOne and @OneToMany, you need the following code to save a new Item and several
	Bid instances.

		Item someItem = new Item("Some Item");
		em.persist(someItem);														// EntityManager of course..
		
		Bid someBid = new Bid(new BigDecimal("123.00"), someItem);
		someItem.getBids().add(someBid);
		em.persist(someBid);
		
		Bid secondBid = new Bid(new BigDecimal("456.00"), someItem);
		someItem.getBids().add(secondBid);
		em.persist(secondBid);
		
		tx.commit();

	When you create several bids, calling persist() on each seems redundant. New instances are transient and have to be
	made persistent. The relationship between Bid and Item doesn’t influence their life cycle. If Bid were to be a value
	type, the state of a Bid would be automatically the same as the owning Item. In this case, however, Bid has its
	own completely independent state.

!!!
	We said earlier that fine-grained control is sometimes necessary to express the dependencies between associated
	entity classes; this is such a case. The mechanism for this in JPA is the cascade option. For example, to save
	all bids when the item is saved, map the collection as shown next.

		@Entity
		public class Item {
			 @OneToMany(mappedBy = "item", cascade = CascadeType.PERSIST)
			 protected Set<Bid> bids = new HashSet<>();
			 // ...
		}
	Cascading options are per operation you’d like to be transitive, so you use CascadeType.PERSIST for the
	EntityManager#persist() operation. You can now simplify the code that links items and bids and then saves them.

		Item someItem = new Item("Some Item");
		em.persist(someItem);												// EntityManager of course..
		
		Bid someBid = new Bid(new BigDecimal("123.00"), someItem);
		someItem.getBids().add(someBid);
		
		Bid secondBid = new Bid(new BigDecimal("456.00"), someItem);
		someItem.getBids().add(secondBid);
		
		tx.commit();


	It seems reasonable that deletion of an item implies deletion of all the bids for the item, because they’re no
	longer relevant alone. This is what the composition (the filled-out diamond) in the UML diagram means.
	With the current cascading options, you have to write the following code to delete an item:

		Item item = em.find(Item.class, ITEM_ID);
		for (Bid bid : item.getBids()) {
		 em.remove(bid);						// remove each bid
		}
		em.remove(item);						// finally, remove the item

	The deletion order is important. If you remove the Item first, you’ll get a foreign key–constraint violation,
	because SQL operations are queued in the order of your remove() calls. First the row(s) in the BID table have to be
	deleted, and then the row in the ITEM table. JPA offers a cascading option to help with this. The persistence
	engine can remove an associated entity instance automatically.

		@Entity
		public class Item {
			@OneToMany(mappedBy = "item", cascade = {CascadeType.PERSIST, CascadeType.REMOVE})
			protected Set<Bid> bids = new HashSet<>();
			// ...
		}
	
	Just as before with PERSIST, Hibernate now cascades the remove() operation on this association. If you call
	EntityManager#remove() on an Item, Hibernate loads the bids collection elements and internally calls remove()
	on each instance:

		Item item = em.find(Item.class, ITEM_ID);
		em.remove(item);

!!!!!
	This deletion process is inefficient: Hibernate must always load the collection and delete each Bid individually.
	A single SQL statement would have the same effect on the database: delete from BID where ITEM_ID = ?.
	You know this because nobody in the database has a foreign key reference on the
	BID table. Hibernate doesn’t know this and can’t search the whole database for any row that might have a BID_ID
	
	If Item#bids was instead a collection of embeddable components, someItem .getBids().clear() would execute a single
	SQL DELETE. With a collection of value types, Hibernate assumes that nobody can possibly hold a reference to
	the bids, and removing only the reference from the collection makes it orphan removable data.

	
	JPA offers a (questionable) flag that enables the same behavior for @OneToMany (and only @OneToMany) entity
	associations.

			@Entity
			public class Item {
				@OneToMany(mappedBy = "item", cascade = CascadeType.PERSIST,  orphanRemoval = true)
				protected Set<Bid> bids = new HashSet<>();
				// ...
			}
	
	The orphanRemoval=true argument tells Hibernate that you want to permanently remove a Bid when it’s removed from
	the collection. Here is an example of deleting a single Bid:

		Item item = em.find(Item.class, ITEM_ID);
		Bid firstBid = item.getBids().iterator().next();
		item.getBids().remove(firstBid);							// one bid removed

	Hibernate monitors the collection and on transaction commit will notice that you removed an element from the
	collection. Hibernate now considers the Bid to be orphaned. You guarantee that nobody else had a reference to
	it; the only reference was the one you just removed from the collection. Hibernate automatically executes
	an SQL DELETE to remove the Bid instance in the database.
	You still won’t get the clear() one-shot DELETE as with a collection of components. Hibernate respects the
	regular entity-state transitions, and the bids are all loaded and removed individually.

	All the removal operations we’ve shown are inefficient; bids have to be loaded into memory, and many SQL DELETEs
	are necessary. SQL databases support a more efficient foreign key feature: the ON DELETE option.
	In DDL, it looks like this:
		foreign key (ITEM_ID) references ITEM on delete cascade for the BID table.
	This option tells the database to maintain referential integrity of composites transparently for all applications
	accessing the database. Whenever you delete a row in the ITEM table, the database will automatically delete any
	row in the BID table with the same ITEM_ID key value. You only need one DELETE statement to remove all
	dependent data recursively, and nothing has to be loaded into application (server) memory.
	You should check whether your schema already has this option enabled on foreign keys. If you want this option
	added to the Hibernate-generated schema, use the Hibernate @OnDelete annotation.

		@Entity
		public class Item {
			@OneToMany(mappedBy = "item", cascade = CascadeType.PERSIST)
			@org.hibernate.annotations.OnDelete(action = org.hibernate.annotations.OnDeleteAction.CASCADE )
			protected Set<Bid> bids = new HashSet<>();
			// ...
		}
	
	If you work on a new schema, the easiest approach is to not enable database-level cascading and map a composition
	relationship in your domain model as embedded/embeddable, not as an entity association. Hibernate can then
	execute efficient SQL DELETE operations to remove the entire composite.


Summary
	 Using simple collection mappings, such as a Set<String>, you worked through a rich set of interfaces and implementations.
	 You know how sorted collections work as well as Hibernate’s options for letting the database return the collection
	elements in the desired order.
	 We discussed complex collections of user-defined embeddable types and sets, bags, and maps of components.
	 You saw how to use components as both keys and values in maps, and a collection in an embeddable component.
	 Mapping the first foreign key column to an entity many-to-one association makes it bidirectional as a one-to-many.
	 You also learned about several cascading options.
	 We covered key concepts of object/relational mapping. Once you’ve mapped your first @ManyToOne and maybe a simple
	collection of strings, the worst will be behind you.
	 Be sure you try the code (and watch the SQL log)!

Advanced entity association mappings

	We’ve shown the particular benefits you gain from collection mappings in the previous chapter; the rules for when
	a collection mapping is appropriate also apply to all examples in this chapter. Always make sure you actually
	need a collection before you attempt a complex collection mapping. Let’s start with mappings that don’t involve
	collections: one-to-one entity associations.


One-to-one associations

	We argued in section 5.2 that the relationships between User and Address (the user has a billingAddress,
	homeAddress, and shippingAddress) are best represented with an @Embeddable component mapping. This is usually
	the simplest way to represent one-to-one relationships, because the life cycle is typically dependent in such a
	case. It’s either an aggregation or a composition in UML.  What about using a dedicated ADDRESS table and mapping
	both User and Address as entities? One benefit of this model is the possibility for shared references—another
	entity class (let’s say Shipment) can also have a reference to a particular Address instance. If a User also has
	a reference to this instance, as their shippingAddress

	There are several possible mappings for one-to-one associations. The first strategy we consider is a shared primary
	key value


Sharing a primary key

	Rows in two tables related by a primary key association share the same primary key values. The User has the same
	primary key value as their (shipping-) Address. The main difficulty with this approach is ensuring that associated
	instances are assigned the same primary key value when the instances are saved. Before we look at this issue, let’s
	create the basic mapping. The Address class is now a standalone entity; it’s no longer a component.

		@Entity
		public class Address {
			 @Id
			 @GeneratedValue(generator = Constants.ID_GENERATOR)
			 protected Long id;
			 @NotNull
			 protected String street;
			 @NotNull
			 protected String zipcode;
			 @NotNull
			 protected String city;
			 // ...
		}

		@Entity
		@Table(name = "USERS")
		public class User {
			 @Id
			 protected Long id;
			 
			 @OneToOne(fetch = FetchType.LAZY, optional = false )		// defaults to Eager, optional is Required for lazy loading with proxies
			 @PrimaryKeyJoinColumn									   // selects shared primary key strategy
			 protected Address shippingAddress;
			 
			 protected User() {
			 }
			 
			 public User(Long id, String username) {
				this.id = id;
				this.username = username;
			 }
			 // ...
		}

	For the User, you don’t declare an identifier generator B. As mentioned in section 4.2.4, this is one of the
	rare cases when you use an application-assigned identifier value.
	You can see that the constructor design (weakly) enforces this :the public API of the class requires an
	identifier value to create an instance.
	
	Two new annotations are present in the example. @OneToOne does what you’d expect: it’s required to mark an
	entity-valued property as a one-to-one association. As usual, you should prefer the lazy-loading strategy,
	so you override the default FetchType.EAGER with LAZY. The second new annotation is @PrimaryKeyJoinColumn,
	selecting the shared primary key strategy you’d like to map. This is now a unidirectional shared primary
	key one-to-one association mapping, from User to Address.
	The optional=false switch E defines that a User must have a shippingAddress.

!!	The JPA specification doesn’t include a standardized method to deal with the problem of shared primary key
	generation. This means you’re responsible for setting the identifier value of a User instance correctly
	before you save it, to the identifier value of the linked Address instance:

		Address someAddress = new Address("Some Street 123", "12345", "Some City");
		em.persist(someAddress);														// Generates identifier value
		User someUser = new User(someAddress.getId(), "johndoe" );
		em.persist(someUser);
		someUser.setShippingAddress(someAddress);										// optional

	After persisting the Address, you take its generated identifier value and set it on the User before saving
	it, too. The last line of this example is optional: your code now expects a value when calling
	someUser.getShippingAddress(), so you should set it. Hibernate won’t give you an error if you forget
	this last step.

	There are three problems with the mapping and code:
		 You have to remember that the Address must be saved first and then get its identifier value after
		the call to persist(). This is only possible if the Address entity has an identifier generator that
		produces values on persist() before the INSERT, as we discussed in section 4.2.5. Otherwise,
		someAddress.getId() returns null, and you can’t manually set the identifier value of the User.
		 Lazy loading with proxies only works if the association is non-optional. This is often a surprise
		for developers new to JPA. The default for @OneToOne is FetchType.EAGER: when Hibernate loads a User,
		it loads the shippingAddress right away. Conceptually, lazy loading with proxies only makes sense if
		Hibernate knows that there is a linked shippingAddress. If the property were nullable, Hibernate would
		have to check in the database whether the property value is NULL, by querying the ADDRESS table. If you
		have to check the database, you might as well load the value right away, because there would be no
		benefit in using a proxy.
		 The one-to-one association is unidirectional; sometimes you need bidirectional navigation.


The foreign primary key generator
	(didn't put the details here, but the idea is that it is using a foreigh key, and so the java code is
	simplified:)
		
		User someUser = new User("johndoe");
		Address someAddress = new Address(someUser, "Some Street 123", "12345", "Some City" );
		someUser.setShippingAddress(someAddress);
		em.persist(someUser);


Using a foreign key join column

	Instead of sharing a primary key, two rows can have a relationship based on a simple additional foreign
	key column. One table has a foreign key column that references
	the primary key of the associated table. (The source and target of this foreign key constraint can even
	be the same table: we call this a self-referencing relationship.)

	Let’s change the mapping for User#shippingAddress. Instead of the shared primary key, you now add
	a SHIPPINGADDRESS_ID column in the USERS table. Additionally, the column has a UNIQUE constraint, so no
	two users can reference the same shipping address

		@Entity
		@Table(name = "USERS")
		public class User {
			 @Id
			 @GeneratedValue(generator = Constants.ID_GENERATOR)
			 protected Long id;

			 @OneToOne(fetch = FetchType.LAZY, optional = false, cascade = CascadeType.PERSIST )
			 @JoinColumn(unique = true)
			 protected Address shippingAddress;
			 // ...
		}
	You don’t need any special identifier generators or primary key assignment; instead of
	@PrimaryKeyJoinColumn, you apply the regular @JoinColumn. If you’re more familiar
	with SQL than JPA, it helps to think “foreign key column” every time you see @JoinColumn in a mapping.


Using a join table
		
	You’ve probably noticed that nullable columns can be problematic. Sometimes a better solution for
	optional values is an intermediate table, which contains a row if a link is present, or doesn’t if not.

	In the database schema, you add an intermediate link table called ITEM_SHIPMENT. A row in this table
	represents a Shipment made in the context of an auction. Figure 8.6 shows the tables. Note how the schema
	enforces uniqueness and the one-to-one relationship: the primary key of ITEM_SHIPMENT is
	the SHIPMENT_ID column, and the ITEM_ID column is unique. An item can therefore be in only one shipment.
	Of course, that also means a shipment can contain only one item.
		
	@Entity
	public class Shipment {
		@OneToOne(fetch = FetchType.LAZY)
		@JoinTable( 
			name = "ITEM_SHIPMENT", 
			joinColumns = @JoinColumn(name = "SHIPMENT_ID"), 
			inverseJoinColumns = @JoinColumn(name = "ITEM_ID", nullable = false, unique = true)
		)
		protected Item auction;
		
		public Shipment() {
		}
		
		public Shipment(Item auction) {
			this.auction = auction;
		}
		// ...
	}

	Lazy loading has been enabled, with a twist: when Hibernate loads a Shipment, it queries both the
	SHIPMENT and the ITEM_SHIPMENT join table. Hibernate has to know if
	there is a link to an Item present before it can use a proxy. It does that in one outer join SQL query,
	so you won’t see any extra SQL statements. If there is a row in ITEM_SHIPMENT, Hibernate uses an Item
	placeholder

	The @JoinTable annotation is new; you always have to specify the name of the intermediate table. This
	mapping effectively hides the join table; there is no corresponding Java class. The annotation defines
	the column names of the ITEM_SHIPMENT table, and Hibernate generates in the schema the UNIQUE constraint
	on the ITEM_ID column. Hibernate also generates the appropriate foreign key constraints on the columns
	of the join table.

	Here you store a Shipment without Items and another linked to a single Item:
		Shipment someShipment = new Shipment();
		em.persist(someShipment);
		
		Item someItem = new Item("Some Item");
		em.persist(someItem);
		
		Shipment auctionShipment = new Shipment(someItem);
		em.persist(auctionShipment);

!!!!!
	To summarize One-to-One relationships, use a shared primary key association if one of the two entities
	is always stored before the other and can act as the primary key source. Use a foreign key association
	in all other cases, and a hidden intermediate join table when your one-to-one association is optional.
!!!!!	


One-to-many associations

	A plural entity association is by definition a collection of entity references. You mapped one of these, a one-to-many
	association, in the previous chapters. One-to many associations are the most important kind of entity association that
	involves a collection. We go so far as to discourage the use of more complex association styles when
	a simple bidirectional many-to-one/one-to-many will do the job.
!!
!!!	Also, remember that you don’t have to map any collection of entities if you don’t want to; you can always write an
	explicit query instead of direct access through iteration. If you decide to map collections of entity references,
	you have a few options, and we discussed some more complex situations now.


Considering one-to-many bags

	So far, you have only seen a @OneToMany on a Set, but it’s possible to use a bag mapping instead for a
	bidirectional one-to-many association. Why would you do this?
	Bags have the most efficient performance characteristics of all the collections you can use for a
	bidirectional one-to-many entity association. By default, collections in
	Hibernate are loaded when they’re accessed for the first time in the application. Because a bag doesn’t
	have to maintain the index of its elements (like a list) or check
	for duplicate elements (like a set), you can add new elements to the bag without triggering the loading.
	This is an important feature if you’re going to map a possibly large collection of entity references.

	To map a bidirectional one-to-many association as a bag, you have to replace the type of the bids
	collection in the Item entity with a Collection and an ArrayList
	implementation. The mapping for the association between Item and Bid remains essentially unchanged:
		@Entity
		public class Item {
		 @OneToMany(mappedBy = "item")
		 public Collection<Bid> bids = new ArrayList<>();

		 // ...
		}

	As mentioned, the advantage of bags is that the collection doesn’t have to be initialized when you add a
	new element:
		
		Item item = em.find(Item.class, ITEM_ID);
		Bid bid = new Bid(new BigDecimal("456.00"), item);
		item.getBids().add(bid);						// no select is performed to get all existing bids here
		em.persist(bid);

	If the collection is a Set or a List, Hibernate loads all the elements when you add another element. 

!!	In a real application, you wouldn’t map the association with a List. Preserving the order of elements in
	the database seems like a common use case but on second
	thought isn’t very useful: sometimes you want to show a list with the highest or newest bid first, or only
	bids made by a certain user, or bids made within a certain time range.
	None of these operations requires a persistent list index. As mentioned in section 3.2.4, avoid storing a
	display order in the database; keep it flexible with queries
	instead of hardcoded mappings. Furthermore, maintaining the index when the application removes, adds, or
	shifts elements in the list can be expensive and may trigger many SQL statements.


Optional one-to-many with a join table
	An optional entity association, be it one-to-one or one-to-many, is best represented in an SQL database
	with a join table
	You added a join table earlier in this chapter, for a one-to-one association. To guarantee the multiplicity
	of one-to-one, you applied a unique constraint on a foreign key column
	of the join table. In the current case, you have a one-to-many multiplicity, so only the ITEM_ID primary
	key column has to be unique: only one User can buy any given Item,
	once. The BUYER_ID column isn’t unique because a User can buy many Items.

		@Entity
		@Table(name = "USERS")
		public class User {
			 @OneToMany(mappedBy = "buyer")
			 protected Set<Item> boughtItems = new HashSet<Item>();
			 // ...
		}

		@Entity
		public class Item {
			 @ManyToOne(fetch = FetchType.LAZY)
			 @JoinTable(
				 name = "ITEM_BUYER",
				 joinColumns = @JoinColumn(name = "ITEM_ID"),
				 inverseJoinColumns = @JoinColumn(nullable = false)
			 )
			 protected User buyer;
			 // ...
		}
	This is now a clean, optional one-to-many/many-to-one relationship. If an Item hasn’t been bought, there is
	no corresponding row in the join table ITEM_BUYER. You don’t have any problematic nullable columns in your
	schema. Still, you should write a procedural constraint and a trigger that runs on INSERT, for the
	ITEM_BUYER table: “Only allow insertion of a buyer if the auction end time for the given item has been
	reached and the user made the winning bid.”



Optional one-to-many with a join table

!!! We always try to avoid nullable columns in a relational database schema. Unknown information degrades the
    quality of the data you store.

    An optional entity association, be it one-to-one or one-to-many, is best represented in an SQL database
    with a join table


One-to-many association in an embeddable class



Unidirectional and bidirectional many-to-many associations

	A join table in the database represents a regular many-to-many association, which some developers also
	call the link table, or association table. Figure 8.14 shows a many-to-many relationship with a link table.
	The link table CATEGORY_ITEM has two columns, both with a foreign key constraint referencing the
	CATEGORY and ITEM tables, respectively. Its primary key is a composite key
	of both columns. You can only link a particular Category and Item once, but you can link the same item
	to several categories.  In JPA, you map many-to-many associations with @ManyToMany on a collection:

		@Entity
		public class Category {
			 @ManyToMany(cascade = CascadeType.PERSIST)
			 @JoinTable(
				 name = "CATEGORY_ITEM",
				 joinColumns = @JoinColumn(name = "CATEGORY_ID"),
				 inverseJoinColumns = @JoinColumn(name = "ITEM_ID")
			 )
			 protected Set<Item> items = new HashSet<Item>();
			 // ...
		}

	Let’s make this association bidirectional (you don’t have to, if you don’t need it):

		@Entity
		public class Item {
			 @ManyToMany(mappedBy = "items")
			 protected Set<Category> categories = new HashSet<Category>();
			 // ...
		}


	A regular @ManyToMany mapping hides the link table; there is no corresponding Java class, only some
	collection properties. So whenever someone says, “My link table has more columns with information about
	the link”—and, in our experience, someone always says this sooner rather than later—you need to map this
	information to a Java class.


Many-to-many with an intermediate entity

	You may always represent a many-to-many association as two many-to-one associations to an intervening
	class. You don’t hide the link table but represent it with a Java class.
	This model is usually more easily extensible, so we tend not to use regular many-to-many associations in
	applications. It’s a lot of work to change code later, when inevitably
	more columns are added to a link table; so before you map an @ManyToMany as shown in the previous section,
	consider the alternative shown in figure 8.15.


Entity associations with Maps

	Map keys and values can be references to other entities, providing another strategy for mapping
	many-to-many and ternary relationships. First, let’s assume that only the value
	of each map entry is a reference to another entity.

		@Entity
		public class Item {
			 @MapKey(name = "id")
			 @OneToMany(mappedBy = "item")
			 protected Map<Long, Bid> bids = new HashMap<>();
			 // ...
		}
	
	New here is the @MapKey annotation. It maps a property of the target entity, in this case the Bid entity,
	as the key of the map. The default if you omit the name attribute is
	the identifier property of the target entity, so the name option here is redundant. Because the keys of a
	map form a set, you should expect values to be unique for a particular map. This is the case for Bid
	primary keys but likely not for any other property of Bid. It’s up to you to ensure that the selected
	property has unique values—Hibernate won’t check.


Key/Value ternary relationship

	You may be a little bored by now, but we promise this is the last time we show another way to map the
	association between Category and Item. Previously, in section 8.3.3,
	you used an embeddable CategorizedItem component to represent the link. Here we show a representation of
	the relationship with a Map, instead of an additional Java
	class. The key of each map entry is an Item, and the related value is the User who added the Item to the
	Category, as shown in figure 8.18.

		@Entity
		public class Category {
			 @ManyToMany(cascade = CascadeType.PERSIST)
			 @MapKeyJoinColumn(name = "ITEM_ID")
			 @JoinTable(
				name = "CATEGORY_ITEM",
				joinColumns = @JoinColumn(name = "CATEGORY_ID"),
				inverseJoinColumns = @JoinColumn(name = "USER_ID")
			 )
			 
			 protected Map<Item, User> itemAddedBy = new HashMap<>();
			 // ...
		}

	someCategory.getItemAddedBy().put(someItem, someUser);
	someCategory.getItemAddedBy().put(otherItem, someUser);
	otherCategory.getItemAddedBy().put(someItem, someUser);

!!!
	To remove the link, remove the entry from the map. This is a convenient Java API for managing a complex relationship,
	hiding a database link table with three columns.
	But remember that in practice, link tables often grow additional columns, and changing all the Java application \
	code later is expensive if you depend on a Map API.


Summary
     You learned how to map complex entity associations using one-to-one associations, one-to-many associations,
    many-to-many associations, ternary associations, and entity associations with maps.
!!!  Simplify the relationships between your classes, and you’ll rarely need many of the techniques we’ve shown. In
    particular, you can often best represent many-to many entity associations as two many-to-one associations from an
    intermediate entity class, or with a collection of components.
     Before you attempt a complex collection mapping, always make sure you actually need a collection. Ask yourself
     whether you frequently iterate through its elements.
     The Java structures shown in this chapter may make data access easier sometimes, but typically they complicate
    data storage, updates, and deletion.


Complex and legacy schemas

	In this chapter, we focus on the most important part of your system: the database schema, where your collection of
	integrity rules resides—the model of the real world that you’ve created. If your application can auction an item
	only once in the real world, your database schema should guarantee that. If an auction always has a
	starting price, your database model should include an appropriate constraint. If data satisfies all integrity rules,
	the data is consistent

	Sometimes you can start a project top-down. There is no existing database schema and maybe not even any data—your
	application is completely new. Many developers like to
	let Hibernate automatically generate the scripts for a database schema. You’ll probably also let Hibernate deploy
	the schema on the test database on your development
	machine or your continuous build systems for integration testing. Later, a DBA will take the generated scripts and
	write an improved and final schema for production
	deployment. The first part of this chapter shows you how to improve the schema from within JPA and Hibernate,
	to make your DBA happy

	At the other end of the spectrum are systems with existing, possibly complex schemas, with years’ worth of data.
	Your new application is just a small gear in a big machine, and your DBA won’t allow any (sometimes even
	non-disruptive) changes to the database. You need a flexible object/relational mapping so you don’t have to twist
	and bend the Java classes too much when things don’t fit right away. This will be the subject of the second half of
	this chapter, including a discussion of composite primary and foreign keys.

	Hibernate creates the basic schema for your tables and constraints automatically; it even creates sequences,
	depending on the identifier generator you select. But there are some schema artifacts Hibernate can’t and won’t
	create automatically. These include all kinds of highly vendor-specific performance options and any other artifacts
	that are relevant only for the physical storage of data (tablespaces, for example). Besides these physical concerns,
	your DBA will often supply custom additional schema statements to improve the generated schema. DBAs should get
	involved early and verify the automatically generated schema from Hibernate. Never go into production with an
	unchecked automatically generated schema.

!!!!!
	Exporting the schema script to a file
	Hibernate bundles the class org.hibernate.tool.hbm2ddl.SchemaExport with a 	main() method you can run from the
	command line. This utility can either talk to your DBMS directly and create the schema or write a text file with
	the DDL script for further customization by your DBA.
!!!!!

Adding auxiliary database objects

	You can hook the following three types of custom SQL scripts into Hibernate’s schema-generation process:
		 The create script executes when the schema is generated. A custom create script can run before, after, or instead of
		Hibernate’s automatically generated scripts.
		In other words, you can write an SQL script that runs before or after Hibernate generates tables, constraints, and so on from
		your mapping metadata.
		 The drop script executes when Hibernate removes schema artifacts. Just like the create script, a drop script can run
		before, after, or instead of Hibernate’s automatically generated statements.
		 The load script always executes after Hibernate generates the schema, as the last step after creation. Its main purpose is
		importing test or master data, before your application or unit test runs. It can contain any kind of SQL statement,
		including DDL statements such as ALTER, if you want to further customize the schema.

	We’ve mentioned that DDL is usually highly vendor-specific. If your application has to support several database dialects, you may
	need several sets of create/drop/load scripts to customize the schema for each database dialect. You can solve this with several
	persistence unit configurations in the persistence.xml file

	Alternatively, Hibernate has its own proprietary configuration for schema customization in an hbm.xml mapping file.
	
		<hibernate-mapping xmlns="http://www.hibernate.org/xsd/orm/hbm">
		 <database-object>
			<create>
				CREATE ...
			</create>
			<drop>
				DROP ...
			</drop>
			<dialect-scope name="org.hibernate.dialect.H2Dialect"/>
			<dialect-scope name="org.hibernate.dialect.PostgreSQL82Dialect"/>
		 </database-object>
		</hibernate-mapping>

	Place your custom SQL fragments into the <create> and <drop> elements. Hibernate executes these statements after creating the
	schema for your domain model classes, which is after creating your tables and before dropping the automatically generated part
	of the schema. This behavior can’t be changed, so the standard JPA schemageneration script settings offer more flexibility.
	The <dialect-scope> elements restrict your SQL statements to a particular set of configured database dialects. Without any
	<dialect-scope> elements, the SQL statements are always applied. Hibernate also supports a load script: if Hibernate finds a
	file called import.sql in the root of your classpath, it executes that file after the schema has been created. Alternatively,
	if you have several import files, you can name them as a commaseparated list with the hibernate.hbm2ddl.import_files property
	in your persistence unit configuration


SQL constraints

	Systems that ensure data integrity only in application code are prone to data corruption and often degrade the quality of the
	database over time. If the data store doesn’t enforce rules, a trivial undetected application bug can cause unrecoverable problems
	such as incorrect or lost data. In contrast to ensuring data consistency in procedural (or object-oriented) application code,
	database management systems allow you to implement integrity rules with declarations, as a database schema. The advantages of
	declarative rules are fewer possible errors in code and a chance for the DBMS to optimize data access.

	In SQL databases, we identify four kinds of rules:
		 Domain constraints—A domain is (loosely speaking, and in the database world) a data type in a database. Hence, a domain
		constraint defines the range of possible values a particular data type can handle. For example, an INTEGER data type is usable
		for integer values. A CHAR data type can hold character strings: for example, all characters defined in ASCII or some other
		encoding. Because we mostly use data types built-in the DBMS, we rely on the domain constraints as defined by the vendor.
		If supported by your SQL database, you can use the (often limited) support for custom domains to add additional constraints
		for particular existing data types, or create user-defined data types (UDT).
		 Column constraints—Restricting a column to hold values of a particular domain and type creates a column constraint. For
		example, you declare in the schema that the EMAIL column holds values of VARCHAR type. Alternatively, you could create a new
		domain called EMAIL_ADDRESS with further constraints and apply it to a column instead of VARCHAR. A special column
		constraint in an SQL database is NOT NULL.
		 Table constraints—An integrity rule that applies to several columns or several rows is a table constraint. A typical
		declarative table constraint is UNIQUE: all rows are checked for duplicate values (for example, each user must have a distinct
		item address). A rule affecting only a single row but multiple columns is “the auction end time has to be after the auction
		start time.”
		 Database constraints—If a rule applies to more than one table, it has database scope. You should already be familiar
		with the most common database constraint, the foreign key. This rule guarantees the integrity of references between rows,
		usually in separate tables, but not always (self-referencing foreign key constraints aren’t uncommon). Other database
		constraints involving several tables aren’t uncommon: for example, a bid can only  be stored if the auction end time of the
		referenced item hasn’t been reached

	Most (if not all) SQL database management systems support these kinds of constraints and the most important options of each.
	In addition to simple keywords such as NOT NULL and UNIQUE, you can usually also declare more complex rules with the CHECK
	constraint that applies an arbitrary SQL expression. Still, integrity constraints are one of
	the weak areas in the SQL standard, and solutions from vendors can differ significantly.

!!!!!
	Concerning constraints... an application shouldn’t pass invalid data to the database to see what sticks and what doesn’t. The
	DBMS is the last line of defense, not the first validator. Use Bean Validation in the Java application tier instead, and
	 show your users nice validation error emails in their own language.


ADDING DOMAIN AND COLUMN CONSTRAINTS

	create domain if not exists
		EMAIL_ADDRESS as varchar
		check (position('@', value) > 1);

	The additional constraint is a check of the presence of an @ symbol in the string

		@Entity
		public class User {
			 @Column(
				 nullable = false,
				 unique = true,
				 columnDefinition = "EMAIL_ADDRESS(255)"
			 )
			  protected String item;
			// ...
		}


TABLE-LEVEL CONSTRAINTS

	An auction can’t end before it starts. So far you have no rule in your SQL schema, or even in your Java domain model,
	that implements this restriction. You need a singlerow table constraint:

	@Entity
	@org.hibernate.annotations.Check(constraints = "AUCTIONSTART < AUCTIONEND")
	public class Item {
		@NotNull
		protected Date auctionStart;
		@NotNull
		protected Date auctionEnd;
		// ...
	}


DATABASE CONSTRAINTS

	A user can only make bids until an auction ends. Your database should guarantee that invalid bids can’t be stored so
	that whenever a row is inserted into the BID table, the CREATEDON timestamp of the bid is checked against the auction
	ending time. This kind of constraint involves two tables: BID and ITEM


Creating indexes

	Indexes are a key feature when optimizing the performance of a database application. The query optimizer in a DBMS can
	use indexes to avoid excessive scans of the data tables. Because they’re relevant only in the physical implementation
	of a database, indexes aren’t part of the SQL standard, and the DDL and available indexing options
	are product specific. You can, however, embed the most common schema artifacts for typical indexes in mapping metadata.

	Many queries in CaveatEmptor will probably involve the username of a User entity. You can speed up these queries by creating an index for the column of this property.
	Another candidate for an index is the combination of USERNAME and EMAIL columns, which you also use frequently in queries.

		@Entity
		@Table(
		 name = "USERS",
		 indexes = {
			@Index(name = "IDX_USERNAME", columnList = "USERNAME"),
			@Index(name = "IDX_USERNAME_EMAIL", columnList = "USERNAME, EMAIL")
		 }
		)
		public class User {
		 // ...
		}

	Get the excellent book SQL Tuning by Dan Tow (Tow, 2003) if you want to learn efficient database-optimization
	techniques and especially how indexes can get you closer to the best-performing execution plan for your queries.


Summary

	 Focus on the database schema.
	 You can add additional integrity rules to a Hibernate-generated database schema. You now know how to execute
	custom create, drop, and load SQL scripts.
	 We discussed using SQL constraints: domain, columns, table, and database constraints.
	 We also covered using custom SQL data types, as well as check, unique, and foreign key constraints.
	 You saw some common issues you have to resolve when dealing with legacy schemas, and specifically keys.
	 You learned about several types of mapping: natural primary keys, composite primary keys, foreign keys in
	composite primary keys, foreign keys to composite primary keys, and foreign keys referencing non-primary keys.
	 You saw how to move properties of an entity to a secondary table. 




============================================================================================
Part 3 - Transactional data processing
============================================================================================	

In part 3, you’ll load and store data with Hibernate and Java Persistence. You’ll introduce the programming interfaces, how to write
transactional applications, and how Hibernate can load data from the database most efficiently.

You’ll see the life cycle of entity instances: how they become persistent, detached, and removed. This chapter is where you’ll get
to know the most important interface in JPA: the EntityManager. Next, chapter 11 defines database and system transaction essentials
and how to control concurrent access with Hibernate and JPA.

You’ll also see nontransactional data access. In chapter 12, we’ll go through lazy and eager loading, fetch plans, strategies, and
profiles, and wrap up with optimizing SQL execution.
Finally, chapter 13 covers cascading state transitions, listening to and intercepting events, auditing and versioning with Hibernate
Envers, and filtering data dynamically.



Managing data

	An efficient application solution requires something more (than just good mappings between Java classes and DB tables): you must
	investigate strategies for runtime data management. These strategies are crucial to the performance and correct behavior of your
	applications.


The persistence life cycle

	Because JPA is a transparent persistence mechanism—classes are unaware of their own persistence capability—it’s possible to
	write application logic that’s unaware whether the data it operates on represents persistent state or temporary state that
	exists only in memory. The application shouldn’t necessarily need to care that an instance is persistent when invoking its
	methods. You can, for example, invoke the Item#calculateTotalPrice() business method without having to consider persistence at
	all (for example, in a unit test).
	Any application with persistent state must interact with the persistence service whenever it needs to propagate state held in
	memory to the database (or vice versa).
	In other words, you have to call the Java Persistence interfaces to store and load data.

	When interacting with the persistence mechanism that way, the application must concern itself with the state and life cycle of an
	entity instance with respect to persistence.
	We refer to this as the persistence life cycle: the states an entity instance goes through during its life.
	We also use the term unit of work: a set of (possibly) state changing operations considered one (usually atomic) group.

!!!!
	Another piece of the puzzle is the persistence context provided by the persistence service. Think of the persistence context
	as a service that remembers all the modifications and state changes you made to data in a particular unit of work
	(this is somewhat simplified, but it’s a good starting point).
!!!!!

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
Entity instance states

	Different ORM solutions use different terminology and define different states and state transitions for the persistence
	life cycle. Moreover, the states used internally may be different from those exposed to the client application.
	JPA defines four states, hiding the complexity of Hibernate’s internal implementation from the client code. Figure 10.1
	shows these states and their transitions.

	TRANSIENT STATE
		Instances created with the new Java operator are transient, which means their state is lost and garbage-collected as
		soon as they’re no longer referenced. For example, new 	Item() creates a transient instance of the Item class,
		just like new Long() and new BigDecimal(). Hibernate doesn’t provide any rollback functionality for transient
		instances; if you modify the price of a transient Item, you can’t automatically undo the change. For an entity
		instance to transition from transient to persistent state, to become managed, requires either a call to the
		EntityManager#persist() method or the creation of a reference from an already-persistent instance and enabled
		cascading of state for that mapped association.

	PERSISTENT STATE
		A persistent entity instance has a representation in the database. It’s stored in the database—or it will be stored
		when the unit of work completes. It’s an instance with a database identity, as defined in section 4.2; its database
		identifier is set to the primary key value of the database representation. The application may have created instances
		and then made them persistent by calling EntityManager#persist(). There may be instances that became persistent when
		the application created a reference to the object from another persistent instance that the JPA provider already manages.
		A persistent entity instance may be an instance retrieved from the database by execution of a query, by an identifier
		lookup, or by navigating the object graph starting from another persistent instance. Persistent instances are always
		associated with a persistence context. You see more about this in a moment.

	REMOVED STATE
		You can delete a persistent entity instance from the database in several ways: For example, you can remove it with
		EntityManager#remove(). It may also become available for deletion if you remove a reference to it from a mapped
		collection with orphan removal enabled. An entity instance is then in the removed state: the provider will delete
		it at the end of a unit of work. You should discard any references you may hold to it in the application after you
		finish working with it—for example, after you’ve rendered the removal confirmation screen your users see.

	DETACHED STATE
		To understand detached entity instances, consider loading an instance. You call EntityManager#find() to retrieve an
		entity instance by its (known) identifier. Then you end your unit of work and close the persistence context. The
		application still has a handle—a reference to the instance you loaded. It’s now in the detached state, and
		the data is becoming stale. You could discard the reference and let the garbage collector reclaim the memory. Or,
		you could continue working with the data in the detached state and later call the merge() method to save your
		modifications in a new unit of work. We’ll discuss detachment and merging again later in this chapter, in a
		dedicated section.


!!!!!!!!!!!!!
The persistence context

	In a Java Persistence application, an EntityManager has a persistence context. You create a persistence context when you call
	EntityManagerFactory#createEntityManager(). The context is closed when you call EntityManager#close(). In JPA terminology,
	this is an application-managed persistence context; your application defines the scope of the persistence context, demarcating
	the unit of work.

	The persistence context monitors and manages all entities in persistent state. The persistence context is the centerpiece
	of much of the functionality of a JPA provider

!!!!
	The persistence context allows the persistence engine to perform automatic dirty checking, detecting which entity instances
	the application modified. The provider then synchronizes with the database the state of instances monitored by a persistence
	context, either automatically or on demand. Typically, when a unit of work completes, the provider propagates state held in
	memory to the database through the execution of SQL INSERT, UPDATE, and DELETE statements (all part of the Data Modification
	Language [DML]). This flushing procedure may also occur at other times.
	For example, Hibernate may synchronize with the database before execution of a query. This ensures that queries are
	aware of changes made earlier during the unit of work.

!!!
	The persistence context acts as a first-level cache; it remembers all entity instances you’ve handled in a particular
	unit of work. For example, if you ask Hibernate to load an entity instance using a primary key value (a lookup by identifier),
	Hibernate can first check the current unit of work in the persistence context. If Hibernate finds the
	entity instance in the persistence context, no database hit occurs—this is a repeatable read for an application.
	Consecutive em.find(Item.class, ITEM_ID) calls with the same persistence context will yield the same result.

	This cache also affects results of arbitrary queries, executed for example with the javax.persistence.Query API.
	Hibernate reads the SQL result set of a query and transforms it into entity instances. This process first tries to
	resolve every entity instance in the persistence context by identifier lookup. Only if an instance with the
	same identifier value can’t be found in the current persistence context does Hibernate read the rest of the data from
	the result-set row. Hibernate ignores any potentially newer data in the result set, due to read-committed transaction
	isolation at the database level, if the entity instance is already present in the persistence context.

!!!
	The persistence context cache is always on—it can’t be turned off. It ensures the following:
		 The persistence layer isn’t vulnerable to stack overflows in the case of circular references in an object graph.
		 There can never be conflicting representations of the same database row at the end of a unit of work. The provider
		can safely write all changes made to an entity instance to the database.
		 Likewise, changes made in a particular persistence context are always immediately visible to all other code
		executed inside that unit of work and its persistence context. JPA guarantees repeatable entity-instance reads.

!!!
	The persistence context provides a guaranteed scope of object identity; in the scope of a single persistence context,
	only one instance represents a particular database row. Consider the comparison of references entityA == entityB.
	This is true only if both are references to the same Java instance on the heap. Now, consider the comparison
	entityA.getId().equals(entityB.getId()). This is true if both have the same database identifier value. Within one
	persistence context, Hibernate guarantees that both comparisons yield the same result.

Would process-scoped identity be better?
    For a typical web or enterprise application, persistence context-scoped identity is preferred. Process-scoped identity,
    where only one in-memory instance represents the row in the entire process (JVM), would offer some potential advantages in terms of
    cache utilization. In a pervasively multithreaded application, though, the cost of always synchronizing shared access
    to persistent instances in a global identity map is too high a price to pay. It’s simpler and more scalable to have
    each thread work with a distinct copy of the data in each persistence context.

The EntityManager interface

	Any transparent persistence tool includes a persistence manager API. This persistence manager usually provides services
	for basic CRUD (create, read, update, delete) operations, query execution, and controlling the persistence context.
	In Java Persistence applications, the main interface you interact with is the EntityManager, to create units
	of work.


The canonical unit of work
	
!!!!	
	In Java SE and some EE architectures (if you only have plain servlets, for example), you get an EntityManager by calling
	EntityManagerFactory#createEntityManager(). Your application code shares the EntityManagerFactory, representing one
	persistence unit, or one logical database. Most applications have only one shared EntityManagerFactory.
!!!

	You use the EntityManager for a single unit of work in a single thread, and it’s inexpensive to create. The following
	listing shows the canonical, typical form of a unit of work.

		EntityManager em = null;
		UserTransaction tx = TM.getUserTransaction();
		try {
			 tx.begin();
			 em = JPA.createEntityManager();
			 // ...
			 tx.commit();
		} catch (Exception ex) {
			 // Transaction rollback, exception handling
			 // ...
		} finally {
			 if (em != null && em.isOpen())
			 em.close();
		}


	(The TM class is a convenience class bundled with the example code of this book. Here it simplifies the lookup of
	the standard UserTransaction API in JNDI. The JPA class provides convenient access to the shared EntityManagerFactory.)
	Everything between tx.begin() and tx.commit() occurs in one transaction. For now, keep in mind that all database
	operations in transaction scope, such as the SQL statements executed by Hibernate, completely either succeed or fail.

!!!!!
	Creating an EntityManager starts its persistence context. Hibernate won’t access the database until necessary; the
	EntityManager doesn’t obtain a JDBC Connection from the pool until SQL statements have to be executed. You can create
	and close an  EntityManager without hitting the database. Hibernate executes SQL statements when you look up or
	query data and when it flushes changes detected by the persistence context to the database. Hibernate joins the
	in-progress system transaction when an EntityManager is created and waits for the transaction to commit. When
	Hibernate is notified (by the JTA engine) of the commit, it performs dirty checking of the persistence context and
	synchronizes with the database. You can also force dirty checking synchronization manually by calling
	EntityManager#flush() at any time during a transaction.

!!!
	You decide the scope of the persistence context by choosing when to close() the EntityManager. You have to close the
	persistence context at some point, so always place the close() call in a finally block.

	How long should the persistence context be open? Let’s assume for the following examples that you’re writing a
	server, and each client request will be processed with one persistence context and system transaction in a
	multithreaded environment. If you’re familiar with servlets, imagine the code in listing 10.1 (from directly above)
	embedded in a servlet’s service() method. Within this unit of work, you access the EntityManager to load and
	store data.


Making data persistent

	Item item = new Item();
	item.setName("Some Item");
	em.persist(item);
	Long ITEM_ID = item.getId();

!!!
	A new transient Item is instantiated as usual. Of course, you may also instantiate it before creating the
	EntityManager. A call to persist() makes the transient instance of Item persistent. It’s now managed by and
	associated with the current persistence context. To store the Item instance in the database, Hibernate has to
	execute an SQL INSERT statement.
!!!	When the transaction of this unit of work commits, Hibernate flushes the
	persistence context, and the INSERT occurs at that time. Hibernate may even batch the INSERT at the JDBC level
	with other statements.
!!!	When you call persist(), only the identifier value of the Item is assigned. Alternatively,
	if your identifier generator isn’t pre-insert, the INSERT statement will be executed immediately when persist()
	is called.

	An entity instance is in persistent state if EntityManager#contains(e) returns true. It’s in transient state if
	PersistenceUnitUtil#getIdentifier(e) returns null. It’s in detached state if it’s not persistent, and
	PersistenceUnitUtil#getIdentifier(e) returns the value of the entity’s identifier property.
	You can get to the PersistenceUnitUtil from the EntityManagerFactory. There are two issues to look out for. First,
	be aware that the identifier value may not be assigned and available until the persistence context is flushed. Second,
	Hibernate (unlike some other JPA providers) never returns null from PersistenceUnitUtil#getIdentifier() if your
	identifier property is a primitive (a long and not a Long)

	It’s better (but not required) to fully initialize the Item instance before managing it with a persistence context.
	The SQL INSERT statement contains the values that were held by the instance at the point when persist() was called.
	If you don’t set the name of the Item before making it persistent, a NOT NULL constraint may be violated. You
	can modify the Item after calling persist(), and your changes will be propagated to the database with an additional
	SQL UPDATE statement.

!!!!
	If one of the INSERT or UPDATE statements made when flushing fails, Hibernate causes a rollback of changes made to
	persistent instances in this transaction at the database level. But Hibernate doesn’t roll back in-memory changes
	to persistent instances. If you change the Item#name after persist(), a commit failure won’t roll
	back to the old name. This is reasonable because a failure of a transaction is normally non-recoverable, and you
	have to discard the failed persistence context and EntityManager immediately.
!!!

Retrieving and modifying persistent data
 
	You can retrieve persistent instances from the database with the EntityManager. For the next example, we assume
	you’ve kept the identifier value of the Item stored in the
	previous section somewhere and are now looking up the same instance in a new unit of work by identifier:

		Item item = em.find(Item.class, ITEM_ID);
		if (item != null)
		 item.setName("New Name");

	The retrieved entity instance is in persistent state, and you can now modify it inside the unit of work. If no
	persistent instance with the given identifier value can be found, find() returns null. The find() operation always
	hits the database if there was no hit for the given entity type and identifier in the persistence context cache.
	The entity instance is always initialized during loading.

!!!!
	You can modify the Item instance, and the persistence context will detect these changes and record them in the database
	automatically. When Hibernate flushes the persistence context during commit, it executes the necessary SQL DML statements
	to synchronize the changes with the database. Hibernate propagates state changes to the database as late as possible,
	toward the end of the transaction. DML statements usually create locks in the database that are held until the transaction
	completes, so Hibernate keeps the lock duration in the database as short as possible.

	Hibernate writes the new Item#name to the database with an SQL UPDATE. By default, Hibernate includes all columns of
	the mapped ITEM table in the SQL UPDATE statement, updating unchanged columns to their old values.

!!!
	Hibernate detects the changed name by comparing the Item with a snapshot copy it took before, when the Item was loaded
	from the database. If your Item is different from the snapshot, an UPDATE is necessary. This snapshot in the persistence
	context consumes memory. Dirty checking with snapshots can also be time consuming, because Hibernate has to compare all
	instances in the persistence context with their snapshot during flushing.
!!!


Making data transient

	To make an entity instance transient and delete its database representation, call the remove() method on the EntityManager:

		Item item = em.find(Item.class, ITEM_ID);
		em.remove(item);
		assertFalse(em.contains(item));
		// em.persist(item);											// canceling the deletion
		assertNull(item.getId());
		tx.commit();
		em.close();


Refreshing data

	Let’s say you load an entity instance from the database and work with the data. For some reason, you know that another
	application or maybe another thread of your application has updated the underlying row in the database. Next, we’ll
	see how to refresh the data held in memory.

		Item item = em.find(Item.class, ITEM_ID);
		item.setName("Some Name");
		
		// Someone updates this row in the database
		String oldName = item.getName();
		em.refresh(item);
		assertNotEquals(item.getName(), oldName);

	Calling refresh() causes Hibernate to execute a SELECT to read and marshal a whole result set, overwriting changes you
	already made to the persistent instance in application memory. If the database row no longer exists (someone deleted it),
	Hibernate throws an EntityNotFoundException on refresh().

	Most applications don’t have to manually refresh in-memory state; concurrent modifications are typically resolved at
	transaction commit time. The best use case for refreshing is with an extended persistence context, which might span
	several request/response cycles and/or system transactions. While you wait for user input with an open persistence
	context, data gets stale, and selective refreshing may be required depending on the duration of the conversation and
	the dialogue between the user and the system. Refreshing can be useful to undo changes made in memory during a conversation,
	if the user cancels the dialogue.



Replicating data

	Replication is useful, for example, when you need to retrieve data from one database and store it in another. Replication
	takes detached instances loaded in one persistence context and makes them persistent in another persistence context.
	You usually open these contexts from two different EntityManagerFactory configurations, enabling two logical databases.
	You have to map the entity in both configurations.

			tx.begin();
			EntityManager emA = getDatabaseA().createEntityManager();
			Item item = emA.find(Item.class, ITEM_ID);
			EntityManager emB = getDatabaseB().createEntityManager();
			emB.unwrap(Session.class).replicate(item, org.hibernate.ReplicationMode.LATEST_VERSION);
			tx.commit();
			emA.close();
			emB.close();


Caching in the persistence context

	The persistence context is a cache of persistent instances. Every entity instance in persistent state is associated with the
	persistence context.
	
	Many Hibernate users who ignore this simple fact run into an OutOfMemoryException. This is typically the case when you load
	thousands of entity instances in a unit of work but never intend to modify them. Hibernate still has to create a snapshot of
	each instance in the persistence context cache, which can lead to memory exhaustion. (Obviously, you should execute a
	bulk data operation if you modify thousands of rows—we’ll get back to this kind of unit of work in section 20.1.)

!!!!
	The persistence context cache never shrinks automatically. Keep the size of your persistence context to the necessary
	minimum. Often, many persistent instances in your context are there by accident—for example, because you needed only a
	few items but queried for many. Extremely large graphs can have a serious performance impact and require significant
	memory for state snapshots. Check that your queries return only data you need, and consider the following ways to control
    Hibernate’s caching behavior :  You can call EntityManager#detach(i) to evict a persistent instance manually from the
    persistence context. You can call EntityManager#clear() to detach all persistent entity instances, leaving you with
    an empty persistence context.


Flushing the persistence context

	By default, Hibernate flushes the persistence context of an EntityManager and synchronizes changes with the database
	whenever the joined transaction is committed.
	All the previous code examples, except some in the last section, have used that strategy. JPA allows implementations
	to synchronize the persistence context at other times, if they wish. Hibernate, as a JPA implementation, synchronizes
	at the following times:
		 When a joined JTA system transaction is committed
		 Before a query is executed—we don’t mean lookup with find() but a query with javax.persistence.Query or the
		    similar Hibernate API
		 When the application calls flush() explicitly

	You can control this behavior with the FlushModeType setting of an EntityManager:

		tx.begin();
		EntityManager em = JPA.createEntityManager();
		Item item = em.find(Item.class, ITEM_ID);
		item.setName("New Name");
		em.setFlushMode(FlushModeType.COMMIT);							// Disables flushing before queries
		
		assertEquals(
		 em.createQuery("select i.name from Item i where i.id = :id")
		 .setParameter("id", ITEM_ID).getSingleResult(),
		 "Original Name"
		);
		
		tx.commit();																		// flush location
		em.close();

	With FlushModeType.COMMIT, you’re disabling flushing before queries, so you may see different data returned by the
	query than what you have in memory. The synchronization then occurs only when the transaction commits. You can
	at any time, while a transaction is in progress, force dirty checking and synchronization with the database by
	calling EntityManager#flush().

	This concludes our discussion of the transient, persistent, and removed entity states, and the basic usage of the
	EntityManager API. Mastering these state transitions and API methods is essential; every JPA application is built
	with these operations.


Implementing equality methods

	You can implement equals() and hashCode() methods several ways. Keep in mind that when you override equals(), you
	always need to also override hashCode() so the two methods are consistent. If two instances are equal, they must
	have the same hash value.
	
	A seemingly clever approach is to implement equals() to compare just the database identifier property, which is often
	a surrogate primary key value. Basically, if two Item instances have the same identifier returned by getId(), they must
	be the same.If getId() returns null, it must be a transient Item that hasn’t been saved.
	Unfortunately, this solution has one huge problem: identifier values aren’t assigned by Hibernate until an instance
	becomes persistent. If a transient instance were added to a Set before being saved, then when you save it, its hash
	value would change while it’s contained by the Set. This is contrary to the contract of java.util.Set, breaking the
	collection. In particular, this problem makes cascading persistent state useless for mapped associations based on
	sets. We strongly discourage database identifier equality

!!!!
	To get to the solution that we recommend, you need to understand the notion of a business key. A business key is a
	property, or some combination of properties, that is unique for each instance with the same database identity.
	Essentially, it’s the natural key that you would use if you weren’t using a surrogate primary key instead. Unlike a
	natural primary key, it isn’t an absolute requirement that the business key never changes—as long as it changes
	rarely, that’s enough.

	The business key is what the user thinks of as uniquely identifying a particular record, whereas the surrogate key
	is what the application and database systems rely on. The business key property or properties are most likely
	constrained UNIQUE in your database schema.

	Let’s write custom equality methods for the User entity class; this is easier than comparing Item instances.
	For the User class, username is a great candidate business key.
    It’s always required, it’s unique with a database constraint, and it changes rarely, if ever.

    @Override
     public boolean equals(Object other) {
        if (this == other) return true;
        if (other == null) return false;
        if (!(other instanceof User)) return false;
        User that = (User) other;
        return this.getUsername().equals(that.getUsername());
     }
     @Override
     public int hashCode() {
        return getUsername().hashCode();
     }

!!!!!
     You may have noticed that the equals() method code always accesses the properties of the “other” reference via
     getter methods. This is extremely important, because the reference passed as other may be a Hibernate proxy,
     not the actual instance that holds the persistent state. You can’t access the username field of a User proxy directly.
     To initialize the proxy to get the property value, you need to access it with a getter method. This is one point
     where Hibernate isn’t completely transparent, but it’s good practice anyway to use getter methods instead of
     direct instance variable access
!!!!!
     Check the type of the other reference with instanceof, not by comparing the values of getClass(). Again, the other
     reference may be a proxy, which is a runtimegenerated subclass of User, so this and other may not be exactly the
     same type but a valid super/subtype.


Detaching entity instances

	User user = em.find(User.class, USER_ID);
	em.detach(user);
	assertFalse(em.contains(user));

	This example also demonstrates the EntityManager#contains() operation, which returns true if the given instance is
	in managed persistent state in this persistence context.
	You can now work with the user reference in detached state. Many applications only read and render the data after
	the persistence context is closed. Modifying the loaded user after the persistence context is closed has no effect
	on its persistent representation in the database. JPA allows you to merge any changes back into the database in a
	new persistence context, though.


Merging entity instances

	Let’s assume you’ve retrieved a User instance in a previous persistence context, and now you want to modify it and save
	these modifications:
		detachedUser.setUsername("johndoe");
		tx.begin();
		em = JPA.createEntityManager();
		User mergedUser = em.merge(detachedUser);
		mergedUser.setUsername("doejohn");
		tx.commit();
		em.close();

	The goal is record the new username of the detached User. First, when you call merge(), Hibernate checks whether a
	persistent instance in the persistence context has the same database identifier as the detached instance you’re merging.
	In this example, the persistence context is empty; nothing has been loaded from the database. Hibernate therefore loads
	an instance with this identifier from the database. Then, merge() copies the detached entity instance onto this loaded
	persistent instance. In other words, the new username you have set on the detached User is also set on the persistent
	merged User, which merge() returns to you. Now discard the old reference to the stale and outdated detached state;
	the detachedUser no longer represents the current state. You can continue modifying the returned mergedUser; Hibernate
	will execute a single UPDATE when it flushes the persistence context during commit.

	If there is no persistent instance with the same identifier in the persistence context, and a lookup by identifier in
	the database is negative, Hibernate instantiates a fresh User. Hibernate then copies your detached instance onto this
	fresh instance, which it inserts into the database when you synchronize the persistence context with
	the database.

	If the instance you’re giving to merge() is not detached but rather is transient (it doesn’t have an identifier value),
	Hibernate instantiates a fresh User, copies the values of the transient User onto it, and then makes it persistent and
	returns it to you. In simpler terms, the merge() operation can handle detached and transient entity instances.
	Hibernate always returns the result to you as a persistent instance.

	If you want to delete a detached instance, you have to merge it first. Then call remove() on the persistent instance
	returned by merge().


Summary

	 We discussed the most important strategies and some optional ones for interacting with entity instances in a JPA
	 application.
	 You learned about the life cycle of entity instances and how they become persistent, detached, and removed.
	 The most important interface in JPA is the EntityManager.
	 In most applications, data isn’t stored and loaded in isolation. Hibernate is typically integrated in a multiuser
	application, and the database is accessed concurrently in many threads.



Transactions and concurrency

	In this chapter, we finally talk about transactions: how you create and control concurrent units of work in an application. 

	A unit of work is an atomic group of operations. Transactions allow you to set unit of work boundaries and help you
	isolate one unit of work from another. In a multiuser application, you may also be processing these units of work
	concurrently.

	To handle concurrency, we first focus on units of work at the lowest level: database and system transactions. You’ll
	learn the APIs for transaction demarcation and how to define units of work in Java code. We’ll talk about how to
	preserve isolation and control concurrent access with pessimistic and optimistic strategies.
	Finally, we look at some special cases and JPA features, based on accessing the database without explicit transactions.


Transaction essentials

	Application functionality requires that several things be done in one go. For example, when an auction finishes,
	the CaveatEmptor application must perform three different
	tasks:
		1 Find the winning bid (highest amount) for the auction item.
		2 Charge the seller of the item the cost of the auction.
		3 Notify the seller and successful bidder.
	What happens if you can’t bill the auction costs because of a failure in the external credit-card system? The business
	requirements may state that either all listed actions must succeed or none must succeed. If so, you call these
	steps collectively a transaction or unit of work. If only a single step fails, the entire unit of work must fail.


ACID attributes

	ACID stands for atomicity, consistency, isolation, durability. Atomicity is the notion that all operations in a
	transaction execute as an atomic unit. Furthermore, transactions allow multiple users to work concurrently with the
	same data without compromising the consistency of the data (consistent with database integrity rules). A particular
	transaction should not be visible to other concurrently running transactions; they should run in isolation.
	Changes made in a transaction should be durable, even if the system fails after the transaction has completed
	successfully.


Database and system transactions

	We’ve also mentioned system and database transactions. Consider the last example again: during the unit of work ending
	an auction, we might mark the winning bid in a database system. Then, in the same unit of work, we talk to an external
	system to bill the seller’s credit card. This is a transaction spanning several (sub)systems, with coordinated
	subordinate transactions on possibly several resources such as a database connection and an external billing processor.
	
	Database transactions have to be short, because open transactions consume database resources and potentially prevent
	concurrent access due to exclusive locks on data. A single database transaction usually involves only a single batch
	of database operations.

	To execute all of your database operations inside a system transaction, you have to set the boundaries of that unit
	of work. You must start the transaction and, at some point, commit the changes. If an error occurs (either while
	executing database operations or when committing the transaction), you have to roll back the changes to leave
	the data in a consistent state. This process defines a transaction demarcation and, depending on the technique you
	use, involves a certain level of manual intervention.

TODO Continue from here
Programmatic transactions with JTA

!!!!!
	In a Java SE environment, you call the JDBC API to mark transaction boundaries. You begin a transaction with
	setAutoCommit(false) on a JDBC Connection and end it by calling commit(). You may, at any time while the
	transaction is in progress, force an immediate rollback with rollback(). In an application that manipulates data
	in several systems, a particular unit of work involves access to more than one transactional resource. In this case,
	you can’t achieve atomicity with JDBC alone. You need a transaction manager that can handle several resources
	in one system transaction. JTA standardizes system transaction management and distributed transactions so you won’t
	have to worry much about the lower-level details. The main API in JTA is the UserTransaction interface with methods
	to begin() and commit() a system transaction.


	Other transaction demarcation APIs
		JTA provides a nice abstraction of the underlying resource’s transaction system, with the added bonus of distributed
		system transactions. Many developers still believe 	you can only get JTA with components that run in a Java EE
		application server. Today, high-quality standalone JTA providers such as Bitronix (used for the example code of
		this book) and Atomikos are available and easy to install in any Java environment. Think of these solutions as
		JTA-enabled database connection pools. You should use JTA whenever you can and avoid proprietary transaction APIs
		such as org.hibernate.Transaction or the very limited javax.persistence.EntityTransaction. These APIs were
		created at a time when JTA wasn’t readily available outside of EJB runtime containers


	Typical unit of work with transaction boundaries

		EntityManager em = null;
		UserTransaction tx = TM.getUserTransaction();
		try {
			 tx.begin();
			 em = JPA.createEntityManager();
			 // ...
			 tx.commit();
		} catch (Exception ex) {
		 try {
			 if (tx.getStatus() == Status.STATUS_ACTIVE
			 || tx.getStatus() == Status.STATUS_MARKED_ROLLBACK)
			 tx.rollback();
		 } catch (Exception rbEx) {
			 System.err.println("Rollback of transaction failed, trace follows!");
			 rbEx.printStackTrace(System.err);
		 }
		 throw new RuntimeException(ex);
		} finally {
			 if (em != null && em.isOpen())
			 em.close();
		}
	
	The EntityManager is lazy; we mentioned in the previous chapter that it doesn’t consume any database connections until
	SQL statements have to be executed. The same is true for JTA: starting and committing an empty transaction is cheap
	when you haven’t accessed any transactional resources. For example, you could execute this
	empty unit of work on a server, for each client request, without consuming any resources or holding any database locks.

	When you create an EntityManager, it looks for an ongoing JTA system transaction within the current thread of execution.
	If the EntityManager finds an ongoing transaction, it joins the transaction by listening to transaction events.
	This means you should always call UserTransaction#begin() and EntityManagerFactory#createEntityManager() on the same
	thread if you want them to be joined. By default, and as explained in chapter 10, Hibernate automatically flushes
	the persistence context when the transaction commits.

	If the EntityManager can’t find a started transaction in the same thread when it’s created, it’s in a special
	unsynchronized mode. In this mode, JPA won’t automatically flush the persistence context. We talk more about this
	behavior later in this chapter;

	The transaction manager will stop a transaction when it has been running for too long. Remember that you want to keep
	database transactions as short as possible in a busy OLTP system. The default timeout depends on the JTA
	provider—Bitronix, for example, defaults to 60 seconds


Handling exceptions

	If any EntityManager call or flushing the persistence context during a commit throws an exception, you must check the
	current state of the system transaction. When an exception occurs, Hibernate marks the transaction for rollback.
	This means the only possible outcome for this transaction is undoing all of its changes. Because you started
	the transaction, it’s your job to check for STATUS_MARKED_ROLLBACK. The transaction might also still be STATUS_ACTIVE,
	if Hibernate wasn’t able to mark it for rollback. In both cases, call UserTransaction#rollback() to abort any SQL
	statements that have been sent to the database within this unit of work.

!!!
	The exception for rollback requires special treatment: you want to catch this exception and log it; otherwise, the
	original exception that led to the rollback is lost. Continue throwing the original exception after rollback. Typically,
	you have another layer of interceptors in your system that will finally deal with the exception, for example by rendering
	an error screen or contacting the operations team. An error during rollback is more difficult to handle properly; we
	suggest logging and escalation, because a failed rollback indicates a serious system problem.

	 Hibernate throws typed exceptions, all subtypes of RuntimeException that help you identify errors:
		 The most common, HibernateException, is a generic error. You have to either check the exception item or find
		out more about the cause by calling getCause() on the exception.
		 A JDBCException is any exception thrown by Hibernate’s internal JDBC layer. This kind of exception is always
		caused by a particular SQL statement, and you can get the offending statement with getSQL(). The internal exception
		thrown by the JDBC connection (the JDBC driver) is available with getSQLException()
		or getCause(), and the database- and vendor-specific error code is available with getErrorCode().
!!!		 Hibernate includes subtypes of JDBCException and an internal converter that tries to translate the vendor-specific
        error code thrown by the database driver into something more meaningful. The built-in converter can produce
        JDBCConnectionException, SQLGrammarException, LockAcquisitionException, DataException, and ConstraintViolationException
        for the most important database dialects supported by Hibernate. You can either manipulate or enhance the dialect
        for your database or plug in a SQLExceptionConverterFactory to customize this conversion.

	Our recommendation is that you use the fine-grained exception types to display better-looking (fatal) error emails,
	not for validation. For example, you could catch ConstraintViolationException separately and render a screen that says,
	“Application bug: someone forgot to validate data before sending it to the database. Please report it to the programmers.”
	For other exceptions, you’d render a generic error screen.

	You now know what exceptions you should catch and when to expect them. One question is probably on your mind: what should
	you do after you’ve caught an exception and rolled back the system transaction? Exceptions thrown by Hibernate are fatal.
	This means you have to close the current persistence context. You aren’t allowed to continue working with the EntityManager
	that threw an exception. Render an error screen and/or log the error, and then let the user restart the conversation
	with the system using a fresh transaction and persistence context.

	As usual, this isn’t the whole picture. Some standardized exceptions aren’t fatal:
		 javax.persistence.NoResultException—Thrown when a Query or TypedQuery is executed with getSingleResult() and no result
		 was returned from the database. You can wrap the query call with exception-handling code and continue working with
		 the persistence context. The current transaction won’t be marked for rollback.
		 javax.persistence.NonUniqueResultException—Thrown when a Query or TypedQuery is executed with getSingleResult() and
		several results were returned from the database. You can wrap the query call with exception handling code and
		continue working with the persistence context. Hibernate won’t mark the current transaction for rollback.
		 javax.persistence.QueryTimeoutException—Thrown when a Query or TypedQuery takes too long to execute. Doesn’t mark
		 the transaction for rollback. You may want to repeat the query, if appropriate.
		 javax.persistence.LockTimeoutException—Thrown when a pessimistic lock couldn’t be acquired. May occur during flushing
		or explicit locking (more on this topic later in this chapter). The transaction isn’t marked for rollback, and you
		may want to repeat the operation. Keep in mind that endlessly hammering on a database system that is already struggling
		to keep up won’t improve the situation.


Controlling concurrent access
 
	Databases (and other transactional systems) attempt to ensure transaction isolation, meaning that, from the point of view of each
	concurrent transaction, it appears that no other transactions are in progress. Traditionally, database systems have implemented
	isolation with locking. A transaction may place a lock on a particular item of data in the database, temporarily preventing read
	and/or write access to that item by other transactions. Some modern database engines implement transaction isolation
	with multiversion concurrency control (MVCC), which vendors generally consider more scalable. We’ll discuss isolation assuming a
	locking model, but most of our observations are also applicable to MVCC.

!!!!
	Applications inherit the isolation guarantees provided by the database management system. For example, Hibernate never locks anything in
	memory. If you consider the many years of experience that database vendors have with implementing concurrency control, you’ll
	see the advantage of this approach. Additionally, some features in Java Persistence, either because you explicitly use them
	or by design, can improve the isolation guarantee beyond what the database provides.
	
	We discuss concurrency control in several steps. First, we explore the lowest layer: the transaction isolation guarantees
	provided by the database. After that, you’ll see the Java Persistence features for pessimistic and optimistic
	concurrency control at the application level, and what other isolation guarantees Hibernate can provide.


Understanding database-level concurrency
(multiple transactions issues)

	If we’re talking about isolation, you may assume that two things are either isolated or not; there is no grey area
	in the real world. When we talk about database transactions, complete isolation comes at a high price. You can’t
	stop the world to access data exclusively in a multiuser OLTP system. Therefore, several isolation levels are available,
	which, naturally, weaken full isolation but increase performance and scalability of the system.

	A lost update - occurs if two transactions both update a data item and then the second transaction aborts, causing
	both changes to be lost. This occurs in systems that don’t implement concurrency control, where concurrent
	transactions aren’t isolated. This is shown in figure 11.1.

	A dirty read - occurs if a transaction reads changes made by another transaction that hasn’t yet been committed.
	This is dangerous because the changes made by the other transaction may later be rolled back, and invalid data may be
	written by the first transaction; see figure 11.2.

	An unrepeatable read - occurs if a transaction reads a data item twice and reads different state each time. For example,
	another transaction may have written to the data item and committed between the two reads, as shown in figure 11.3.

	A phantom read - is said to occur when a transaction executes a query twice, and the second result includes data
	that wasn’t visible in the first result or less data because something was deleted. It need not necessarily be
	exactly the same  query. Another transaction inserting or deleting data between the executions of
	the two queries causes this situation, as shown in figure 11.5.


ANSI ISOLATION LEVELS
	The standard isolation levels are defined by the ANSI SQL standard, but they aren’t specific to SQL databases. JTA
	defines exactly the same isolation levels, and you’ll use these levels to declare your desired transaction isolation.
	With increased levels of isolation come higher cost and serious degradation of performance and scalability:

		 Read uncommitted isolation—A system that permits dirty reads but not lost updates operates in read uncommitted
		isolation. One transaction may not write to a row if another uncommitted transaction has already written to it.
		Any transaction may read any row, however. A DBMS may implement this isolation level with exclusive write locks.
		 Read committed isolation—A system that permits unrepeatable reads but not dirty reads implements read committed
		isolation. A DBMS may achieve this by using shared read locks and exclusive write locks. Reading transactions
		don’t block other transactions from accessing a row, but an uncommitted writing transaction blocks all other
		transactions from accessing the row.
		 Repeatable read isolation—A system operating in repeatable read isolation mode permits neither unrepeatable
		reads nor dirty reads. Phantom reads may occur. Reading transactions block writing transactions but not other
		reading transactions, and writing transactions block all other transactions.
		 Serializable isolation—The strictest isolation, serializable, emulates serial execution, as if transactions
		were executed one after another, rather than concurrently. A DBMS may not implement serializable using only
		row-level locks. A DBMS must instead provide some other mechanism that prevents a newly inserted row from
		becoming visible to a transaction that has already executed a query that would return the row. A crude mechanism
		is exclusively locking the entire database table after a write, so no phantom reads can occur.

	How exactly a DBMS implements its locking system varies significantly; each vendor has a different strategy. You
	should study the documentation of your DBMS to find out more about its locking system, how locks are escalated
	(from row-level, to pages, to entire tables, for example), and what impact each isolation level has on the performance
	and scalability of your system


CHOOSING AN ISOLATION LEVEL

	Developers (ourselves included) are often unsure what transaction isolation level to use in a production application.
	Too high an isolation level harms the scalability of a highly concurrent application. Insufficient isolation may cause
	subtle, difficult-to reproduce bugs in an application that you won’t discover until the system is working
	under heavy load. Note that we refer to optimistic locking (with versioning) in the following explanation, a concept
	explained later in this chapter. You may want to skip this section for now and come back to it later when it’s time to
	pick an isolation level for your application. Choosing the correct isolation level is, after all, highly dependent on
	your particular scenario. Read the following discussion as recommendations, not dictums carved in stone.

	First, for almost all scenarios, eliminate the read uncommitted isolation level. It’s extremely dangerous to use one
	transaction’s uncommitted changes in a different transaction. The rollback or failure of one transaction will affect
	other concurrent transactions. Rollback of the first transaction could bring other transactions down
	with it, or perhaps even cause them to leave the database in an incorrect state (the seller of an auction item might
	be charged twice—consistent with database integrity rules but incorrect). It’s possible that changes made by a
	transaction that ends up being rolled back could be committed anyway, because they could be read and then
	propagated by another transaction that is successful!

	Second, most applications don’t need serializable isolation. Phantom reads aren’t usually problematic, and this
	isolation level tends to scale poorly. Few existing applications use serializable isolation in production, but
	rather rely on selectively applied pessimistic locks that effectively force a serialized execution of operations in
	certain situations.

!!!!!!!!
	The JPA specification assumes that read committed is the default isolation level. This
    means you have to deal with unrepeatable reads, phantom reads, and the last commit
    wins problem

    We recommend a different approach if you’re using a JTA transaction manager or even a simple JDBC connection pool.
    JTA transaction management systems, such as Bitronix used for the examples of this book, allow you to set a default
    transaction isolation level for every connection obtained from the pool. I

    From time to time, a particular unit of work in your application may require a different, usually stricter
    isolation level. Instead of changing the isolation level of the entire transaction, you should use the Java Persistence
    API to obtain additional locks on the relevant data. This fine-grained locking is more scalable in a
    highly concurrent application. JPA offers optimistic version checking and databaselevel pessimistic locking.

	(more info on these can be found on this chapter...)


ENABLING VERSIONING

	@Entity
	public class Item implements Serializable {
	 @Version
	 protected long version;
	 // ...
	}

	In this example, each entity instance carries a numeric version. It’s mapped to an additional column of the
	ITEM database table; as usual, the column name defaults to
	the property name, here VERSION. The actual name of the property and column doesn’t matter—you could rename it if
	VERSION is a reserved keyword in your DBMS.  You could add a getVersion() method to the class, but you shouldn’t
	have a setter method and the application shouldn’t modify the value. Hibernate automatically changes the version
	value: it increments the version number whenever an Item instance has been found dirty during flushing of the
	persistence context. The version is a simple counter without any useful semantic value beyond concurrency
	control. You can use an int, an Integer, a short, a Short, or a Long instead of a long; Hibernate wraps and
	starts from zero again if the version number reaches the limit of the data type.

	What modifications trigger the increment of an entity’s version? Hibernate increments the version whenever an
	entity instance is dirty. This includes all dirty valuetyped properties of the entity, no matter if they’re
	single-valued (like a String or int property), embedded (like an Address), or collections. The exceptions are
	@OneToMany and @ManyToMany association collections that have been made read-only with mappedBy.

	In theory, versioning with a timestamp is slightly less safe, because two concurrent transactions may both
	load and update the same Item in the same millisecond; this is
	exacerbated by the fact that a JVM usually doesn’t have millisecond accuracy (you should check your JVM and
	operating system documentation for the guaranteed precision). Furthermore, retrieving the current time from the
	JVM isn’ t necessarily safe in a clustered environment, where the system time of nodes may not be synchronized,
	or time synchronization isn’t as accurate as you’d need for your transactional load.

    What modifications trigger the increment of an entity’s version? Hibernate increments the version whenever an
    entity instance is dirty. This includes all dirty valuetyped properties of the entity, no matter if they’re
    single-valued (like a String or int property), embedded (like an Address), or collections. The exceptions are
    @OneToMany and @ManyToMany association collections that have been made read-only with mappedBy.

    If you don’t want to increment the version of the entity instance when a particular property’s value has changed,
    annotate the property with @org.hibernate.annotations.OptimisticLock(excluded = true). You may not like the
    additional VERSION column in your database schema

VERSIONING WITHOUT VERSION NUMBERS OR TIMESTAMPS

    In theory, versioning with a timestamp is slightly less safe, because two concurrent
    transactions may both load and update the same Item in the same millisecond; this is
    exacerbated by the fact that a JVM usually doesn’t have millisecond accuracy (you
    should check your JVM and operating system documentation for the guaranteed precision). Furthermore, retrieving
    the current time from the JVM isn’ t necessarily safe in a clustered environment, where the system time of
    nodes may not be synchronized, or time synchronization isn’t as accurate as you’d need for your transactional load.

	If you don’t have version or timestamp columns, Hibernate can still perform automatic versioning. This alternative
	implementation of versioning checks the current database state against the unmodified values of persistent
	properties at the time Hibernate retrieved the entity instance (or the last time the persistence context was flushed).

		@Entity
		@org.hibernate.annotations.OptimisticLocking(type = org.hibernate.annotations.OptimisticLockType.ALL)
		@org.hibernate.annotations.DynamicUpdate
		public class Item {
			// ...
		}

	For this strategy, you also have to enable dynamic SQL generation of UPDATE statements,
	using @org.hibernate.annotations.DynamicUpdate as explained in section 4.3.2.

	Hibernate now executes the following SQL to flush a modification of an Item instance:
			update ITEM set NAME = 'New Name'
			 where ID = 123
			 and NAME = 'Old Name'
			 and PRICE = '9.99'
			 and DESCRIPTION = 'Some item for auction'
			 and ...
			 and SELLER_ID = 45
			 
	Hibernate lists all columns and their last known values in the WHERE clause. If any concurrent transaction
	has modified any of these values or even deleted the row, this
	statement returns with zero updated rows. Hibernate then throws an exception at flush time.



	
FORCING A VERSION INCREMENT

	What happens if two users place a bid for the same auction item at the same time? When a user makes a new bid, the
	application must do several things:
		1 Retrieve the currently highest Bid for the Item from the database.
		2 Compare the new Bid with the highest Bid; if the new Bid is higher, it must be stored in the database.
	There is the potential for a race condition in between these two steps. If, in between reading the highest Bid and
	placing the new Bid, another Bid is made, you won’t see it. This conflict isn’t visible; even enabling versioning
	of the Item doesn’t help. The Item is never modified during the procedure. Forcing a version increment of the Item
	makes the conflict detectable.
	
		tx.begin();
		EntityManager em = JPA.createEntityManager();
		Item item = em.find(
			 Item.class,
			 ITEM_ID,
			 LockModeType.OPTIMISTIC_FORCE_INCREMENT					// Tells Hibernate to increment Item version
		);
		Bid highestBid = queryHighestBid(em, item);
	
		try {
			Bid newBid = new Bid( new BigDecimal("44.44"), item, highestBid );
			em.persist(newBid);
		} catch (InvalidBidException ex) {
		}
		tx.commit();
		em.close();
	
!!!	
	find() accepts a LockModeType. The OPTIMISTIC_FORCE_INCREMENT mode tells Hibernate that the version of the
	retrieved Item should be incremented after loading, even if it’s never modified in the unit of work


Explicit pessimistic locking
 
	Example:
	
		tx.begin();
		EntityManager em = JPA.createEntityManager();
		BigDecimal totalPrice = new BigDecimal(0);
		for (Long categoryId : CATEGORIES) {
			List<Item> items = em.createQuery("select i from Item i where i.category.id = :catId")
			 .setLockMode(LockModeType.PESSIMISTIC_READ)
			 .setHint("javax.persistence.lock.timeout", 5000)
			 .setParameter("catId", categoryId)
			 .getResultList();
			for (Item item : items)
				totalPrice = totalPrice.add(item.getBuyNowPrice());
		}
		tx.commit();
		em.close();
		assertEquals(totalPrice.compareTo(new BigDecimal("108")), 0);
 
	For each Category, query all Item instances in PESSIMISTIC_READ lock mode. Hibernate locks the rows in the database
	with the SQL query. If possible, wait 5 seconds if another transaction holds a conflicting lock. If the lock
	can’t be obtained, the query throws an exception. B If the query returns successfully, you know that you hold an
	exclusive lock on the data and no other transaction can access it with an exclusive lock or modify it until this
	transaction commits.
	Your locks are released after commit, when the transaction completes

	The JPA specification defines that the lock mode PESSIMISTIC_READ guarantees
    repeatable reads. JPA also standardizes the PESSIMISTIC_WRITE mode, with additional
    guarantees: in addition to repeatable reads, the JPA provider must serialize data
    access, and no phantom reads can occur.

!!! The duration of a pessimistic lock in JPA is a single database transaction. This means you can’t use an exclusive
    lock to block concurrent access for longer than a single database transaction. When the database lock can’t be
    obtained, an exception is thrown.
    Compare this with an optimistic approach, where Hibernate throws an exception at commit time, not when you query.
    With a pessimistic strategy, you know that you can read and write the data safely as soon as your locking query
    succeeds. With an optimistic approach, you hope for the best and may be surprised later, when you commit.

    Locking pessimistically for longer than a single database transaction is usually a performance bottleneck; every data
    access involves additional lock checks to a globally synchronized lock manager. Optimistic locking, however, is the
    perfect concurrency control strategy for long-running conversations (as you’ll see in the next chapter) and
    performs well. Depending on your conflict-resolution strategy—what happens after a conflict is detected—your application
    users may be just as happy as with blocked concurrent access. They may also appreciate the application not locking
    them out of particular screens while others look at the same data.

 (ALSO found a nice article here : https://vladmihalcea.com/optimistic-vs-pessimistic-locking/    which ties nicely in
    what was said in the previous paragraph.
    "Pessimistic locking would not help us in this case since Alice’s read and the write happen in different HTTP
    requests and database transactions. (so when using a DB in a webapp...which is the most used scenario in todays world)
    "So, optimistic locking (the one where we use a Version column for ex) can help you prevent Lost Updates even when
    using application-level transactions that incorporate the user-think time as wel"  )
	
	
(!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! From this point on, the notes are way less, since I am running out of time !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!)
(!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! From this point on, the notes are way less, since I am running out of time !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!)
(!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! From this point on, the notes are way less, since I am running out of time !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!)
(!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! From this point on, the notes are way less, since I am running out of time !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!)
(!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! From this point on, the notes are way less, since I am running out of time !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!)
(!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! From this point on, the notes are way less, since I am running out of time !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!)
(!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! From this point on, the notes are way less, since I am running out of time !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!)


Avoiding deadlocks

	Deadlocks can suddenly appear when the application has to handle a high transaction load in production. Usually the
	DBMS terminates one of the deadlocked transactions after a timeout period and fails; the other transaction can then
	proceed. Alternatively, depending on the DBMS, the DBMS may detect a deadlock situation automatically and immediately
	abort one of the transactions.

!!!	An alternative pragmatic optimization that significantly reduces the probability of deadlocks is to order the UPDATE
	statements by primary key value: Hibernate should always update the row with primary key 1 before updating row 2,
	no matter in what order the data was loaded and modified by the application. You can enable this optimization for
	the entire persistence unit with the configuration property hibernate.order_updates. Hibernate then orders all
	UPDATE statements it executes in ascending order by primary key value of the modified entity instances and collection
	elements detected during flushing.


Queueing modifications

	An unsynchronized persistence context therefore allows you to decouple persistence operations from transactions.
	This special behavior of the EntityManager will be essential later in the book, when we discuss application design.

!!!	The ability to queue data modifications, independent from transactions (and client/server requests), is a
	major feature of the persistence context. 
		
		Example:
		
			EntityManager em = JPA.createEntityManager();
			Item item = em.find(Item.class, ITEM_ID);
			em.remove(item);
			
			tx.begin();
			em.joinTransaction();
			tx.commit();							// flush
			em.close();


 Summary

	 You learned to use transactions, concurrency, isolation, and locking.
	 Hibernate relies on a database’s concurrency-control mechanism but provides better isolation guarantees in a
	transaction, thanks to automatic versioning and the persistence context cache.
	 We discussed how to set transaction boundaries programmatically and handle exceptions.
	 You explored optimistic concurrency control and explicit pessimistic locking.
	 You saw how to work with auto-commit mode and an unsynchronized persistence context outside of a transaction, and
	how to queue modification.



Fetch plans, strategies, and profiles

	In this chapter, we explore Hibernate’s solution for the fundamental ORM problem of navigation, as mentioned in section
	1.2.5. We show you how to retrieve data from the database and how you can optimize this loading.  Hibernate provides
	the following ways to get data out of the database and into memory:
		 Retrieving an entity instance by identifier is the most convenient method when the unique identifier value of an
		entity instance is known: for example, 	entityManager.find(Item.class, 123).
		 You can navigate the entity graph, starting from an already-loaded entity instance, by accessing the associated
		instances through property accessor methods such as someItem.getSeller().getAddress().getCity(), and so on. Elements
		of mapped collections are also loaded on demand when you start iterating through a collection. Hibernate automatically
		loads nodes of the graph if the persistence context is still open. What and how data is loaded when you call accessors
		and iterate through collections is the focus of this chapter.
		 You can use the Java Persistence Query Language (JPQL), a full object-oriented query language based on strings such
		as select i from Item i where i.id = ?.
		 The CriteriaQuery interface provides a type-safe and object-oriented way to perform queries without string
		manipulation.
		 You can write native SQL queries, call stored procedures, and let Hibernate take care of mapping the JDBC result
		sets to instances of your domain model classes.

	What Hibernate loads depends on the fetch plan: you define the (sub)graph of the network of objects that should be
	loaded. Then you pick the right fetch strategy, defining how the data should be loaded. You can store your selection
	of plan and strategy as a fetch profile and reuse it. Defining fetch plans and what data should be loaded by Hibernate
	relies on two fundamental techniques: lazy and eager loading of nodes in the network of objects.


Lazy and eager loading
	
	In your domain-model mapping, you define the global default fetch plan, with the FetchType.LAZY and FetchType.EAGER
	options on associations and collections. This plan is the default setting for all operations involving your persistent
	domain model classes. It’s always active when you load an entity instance by identifier and when you navigate the
	entity graph by following associations and iterating through persistent collections.

!!!
	Our recommended strategy is a lazy default fetch plan for all entities and collections. If you map all of your
	associations and collections with FetchType.LAZY, Hibernate will only load the data you’re accessing at this time.
	While you navigate the graph of your domain model instances, Hibernate will load data on demand, bit by bit. You then
	override this behavior on a per-case basis when necessary.
	To implement lazy loading, Hibernate relies on runtime-generated entity placeholders called proxies and on smart
	wrappers for collections.


Understanding entity proxies
	
	Consider the getReference() method of the EntityManager API. In section 10.2.3, you had a first look at this operation
	and how it may return a proxy. Let’s further explore this important feature and find out how proxies work:
		
		Item item = em.getReference(Item.class, ITEM_ID);				// NO Select is performed
		assertEquals(item.getId(), ITEM_ID);							
	
!!!	
	This code doesn’t execute any SQL against the database. All Hibernate does is create an Item proxy: it looks
	(and smells) like the real thing, but it’s only a placeholder. In the persistence context, in memory,you now have
	this proxy available in persistent state, as shown in figure 12.1.

!!!
	The proxy is an instance of a runtime-generated subclass of Item, carrying the identifier value of the entity
	instance it represents. This is why Hibernate (in line with JPA) requires that entity classes have at least a public
	or protected no-argument constructor (the class may have other constructors, too). The entity class and its methods
	must not be final; otherwise, Hibernate can’t produce a proxy. Note that the JPA specification doesn’t mention
	proxies; it’s up to the JPA provider how lazy loading is implemented.

	If you call any method on the proxy that isn’t the “identifier getter,” you trigger initialization of the proxy and
	hit the database. If you call item.getName(), the SQL SELECT to load the Item will be executed. The previous example
	called item.getId() without triggering initialization because getId() is the identifier getter method in the given
	mapping; the getId() method was annotated with @Id. If @Id was on a field, then calling getId(), just like calling
	any other method, would initialize the proxy!
	(Dan : this is not my experience...it seems like getId does not trigger the initialization with the select of the proxy)
	(Remember that we usually prefer mappings and access on fields, because this allows you more freedom when designing
	accessor methods; see section 3.2.3. It’s up to you whether calling getId() without initializing a proxy is more
	important.)  With proxies, be careful how you compare classes.

		assertNotEquals(item.getClass(), Item.class);			// Class is runtime generated, named something like Item_$$_javassist_1
		assertEquals(HibernateProxyHelper.getClassWithoutInitializingProxy(item), Item.class );

	If you really must get the actual type represented by a proxy, use the HibernateProxyHelper, as seen above.
	
	JPA provides PersistenceUtil, which you can use to check the initialization state of an entity or any of its attributes:

		PersistenceUtil persistenceUtil = Persistence.getPersistenceUtil();
		assertFalse(persistenceUtil.isLoaded(item));
		assertFalse(persistenceUtil.isLoaded(item, "seller"));
		assertFalse(Hibernate.isInitialized(item));
		// assertFalse(Hibernate.isInitialized(item.getSeller()));			// Would trigger initialization of item!

!!!
	Hibernate also offers a utility method for quick-and-dirty initialization of proxies:

		Hibernate.initialize(item);
		// select * from ITEM where ID = ?

		assertFalse(Hibernate.isInitialized(item.getSeller()));		// Make sure the default EAGER of @ManyToOne has  been overridden with LAZY.

		Hibernate.initialize(item.getSeller());
		// select * from USERS where ID = ?
	The first call hits the database and loads the Item data, populating the proxy with the item’s name, price, and so on. The seller
	of the Item is an @ManyToOne association mapped with FetchType .LAZY, so Hibernate creates a User proxy when the Item is loaded.
	You can check the seller proxy state and load it manually, just like the Item. Remember that the JPA default for @ManyToOne is
	FetchType.EAGER! You usually want to override this to get a lazy default fetch plan.

	Data can only be loaded on demand while the persistence context manages the proxy, not in detached state. (if you try to do this operation
	while in detached state you will get a  LazyInitializationException)

	How does lazy loading of one-to-one associations work?
    Lazy loading for one-to-one entity associations is sometimes confusing for new Hibernate users. If you consider one-to-one
    associations based on shared primary keys
    (see section 8.1.1), an association can be proxied only if it’s optional=false. For
    example, an Address always has a reference to a User. If this association is nullable
    and optional, Hibernate must first hit the database to find out whether it should apply
    a proxy or a null—and the purpose of lazy loading is to not hit the database at all.
    You can enable lazy loading of optional one-to-one associations through bytecode
    instrumentation and interception, which we discuss later in this chapter.

!!!
	 Runtime proxy generation as provided by Hibernate is an excellent choice for transparent lazy loading. Your domain model
	 classes don’t have to implement any special (super)type, as some older ORM solutions would require. No code
	 generation or post-processing of bytecode is needed either, simplifying your build procedure. But you should be aware
	 of some potentially negative aspects:
		 Cases where runtime proxies aren’t completely transparent are polymorphic associations that are tested with
		instanceof, a problem shown in section 6.8.1.
		 With entity proxies, you have to be careful not to access fields directly when writing custom equals() and hashCode()
		methods, as discussed in section 10.3.2.
		 Proxies can only be used to lazy-load entity associations. They can’t be used to lazy load individual basic properties
		or embedded components, such as Item#description or User#homeAddress. If you set the @Basic(fetch = FetchType.LAZY) hint
		on such a property, Hibernate ignores it; the value is eagerly loaded when the owning entity instance is loaded. Although
		possible with bytecode instrumentation and interception, we consider this kind of optimization to be rarely useful.
		Optimizing at the level of individual columns selected in SQL is unnecessary if you aren’t working with (a) a
		significant number of optional/nullable columns or (b) columns containing large values
		that have to be retrieved on demand because of the physical limitations of your system. Large values are best
		represented with locator objects (LOBs) instead; they provide lazy loading by definition (see the section “Binary
		and large value types” in chapter 5).


Lazy persistent collections
	
	When you load an Item, Hibernate doesn’t load its lazy collection of images right away. The lazy bids one-to-many
	collection is also only loaded on demand, when accessed and needed:

	The find() operation loads the Item entity instance into the persistence context, as you can see in figure 12.2.
	The Item instance has a reference to an uninitialized User proxy: the seller. It also has a reference to an
	uninitialized Set of bids and an uninitialized List of images. Hibernate implements lazy loading (and dirty checking)
	of collections with its own special implementations called collection wrappers. Although the bids certainly look like
	a Set, Hibernate replaced the implementation with an org.hibernate.collection.internal.PersistentSet while you weren’t
	looking. It’s not a HashSet, but it has the same behavior. That’s why it’s so important to program with interfaces
	in your domain model and only rely on Set and not HashSet. Lists and maps work the same way.
	These special collections can detect when you access them and load their data at that time. 

	With LazyCollectionOption.EXTRA, the collection supports operations that don’t trigger initialization. For example,
	you could ask for the collection’s size:

		Item item = em.find(Item.class, ITEM_ID);
		// select * from ITEM where ID = ?
		
		assertEquals(item.getBids().size(), 3);
		// select count(b) from BID b where b.ITEM_ID = ?
	
	The size() operation triggers a SELECT COUNT() SQL query but doesn’t load the bids into memory.


Lazy loading with interception

	The fundamental problem with lazy loading is that the JPA provider must know when to load the seller of an Item or the
	collection of bids. Instead of runtime-generated proxies and smart collections, many other JPA providers rely exclusively
	on interception of method calls. For example, when you call someItem.getSeller(), the JPA provider would intercept this
	call and load the User instance representing the seller.
	This approach requires special code in your Item class to implement the interception: the getSeller() method or the
	seller field must be wrapped. Because you don’t want to write this code by hand, typically you run a bytecode enhancer
	(bundled with your JPA provider) after compiling your domain model classes. This enhancer injects the necessary
	interception code into your compiled classes, manipulating the fields and methods at the bytecode level.

	Let’s discuss lazy loading based on interception with a few examples. First, you probably want to disable Hibernate’s
	proxy generation:
			@Entity
			@org.hibernate.annotations.Proxy(lazy = false)
			public class User {
			 // ...
			}
	
	Hibernate will now no longer generate a proxy for the User entity. If you call entityManager.getReference(User.class, USER_ID),
	a SELECT is executed, just as for find():

		User user = em.getReference(User.class, USER_ID); // Proxies are disabled. getReference() will return an initialized instance.
		// select * from USERS where ID = ?
		assertTrue(Hibernate.isInitialized(user));

			@Entity
			public class Item {
				@ManyToOne(fetch = FetchType.LAZY)			// Has no effect—no User proxy.
				@org.hibernate.annotations.LazyToOne(org.hibernate.annotations.LazyToOneOption.NO_PROXY)
				protected User seller;
				// ...
			}

	"org.hibernate.annotations.LazyToOne(org.hibernate.annotations.LazyToOneOption.NO_PROXY)" tells Hibernate that the
	bytecode enhancer must add interception code for the seller property. Without this option, or if you don’t run the
	bytecode enhancer, this association would be eagerly loaded and the field would be populated right away when the
	Item is loaded, because proxies for the User entity have been disabled.

!!!
	For lazy entity associations, proxies are usually a better choice than interception. A more common use case for
	interception is properties of basic type, such a String or byte[], with potentially large values. We might argue
	that LOBs (see “Binary and large value types” in chapter 5) should be preferred for large strings or binary data,
	but you might not want to have the java.sql.Blob or java.sql.Clob type in your domain
	model. With interception and bytecode enhancement, you can load a simple String or byte[] field on demand:

		@Entity
		public class Item {
		 @Basic(fetch = FetchType.LAZY)
		 protected String description;
		 // ...
		}
	The Item#description will be lazy loaded if you run the bytecode enhancer on the compiled class. If you don’t
	run the bytecode enhancer—for example, during development—the String will be loaded together with the Item
	instance.

	If you rely on interception, be aware that Hibernate will load all lazy fields of an entity or embeddable class,
	even if only one has to be loaded:


Eager loading of associations and collections
 
	Sometimes, although not often, you want the opposite: to specify that a particular entity association or
	collection should always be loaded. You want the guarantee that this data is available in memory without an
	additional database hit.

!!!!!
	More important, you want a guarantee that, for example, you can access the seller of an Item once the Item
	instance is in detached state. When the persistence context is closed, lazy loading is no longer available.
	If seller were an uninitialized proxy, you’d get a LazyInitializationException when you accessed it in detached
	state. For data to be available in detached state, you need to either load it manually while the persistence
	context is still open or, if you always want it loaded, change your fetch plan to be eager instead of lazy.

!!!!!
	Unlike FetchType.LAZY, which is a hint the JPA provider can ignore, a FetchType.EAGER is a hard requirement.
	The provider has to guarantee that the data is loaded and available in detached state; it can’t ignore the setting

	Consider the collection mapping: is it really a good idea to say, “Whenever an item is loaded into memory, load
	the bids of the item right away, too”? Even if you only want to display the item’s name or find out when the
	auction ends, all bids will be loaded into memory. Always eager-loading collections, with FetchType.EAGER as
	the default fetch plan in the mapping, usually isn’t a great strategy. You’ll also see the Cartesian product
	problem appear if you eagerly load several collections, which we discuss later in this chapter. It’s best if
	you leave collections as the default FetchType.LAZY.


		Item item = em.find(Item.class, ITEM_ID);				// with EAGER loading
		// select i.*, u.*, b.*
		// from ITEM i
		// left outer join USERS u on u.ID = i.SELLER_ID
		// left outer join BID b on b.ITEM_ID = i.ID
		// where i.ID = ?

		em.detach(item);									// Done fetching: no more lazy loading
		assertEquals(item.getBids().size(), 3);
		assertNotNull(item.getBids().iterator().next().getAmount());
		assertEquals(item.getSeller().getUsername(), "johndoe");

	For the find(), Hibernate executes a single SQL SELECT and JOINs three tables to retrieve the data. You can see the
	contents of the persistence context in figure 12.3. Note how the boundaries of the loaded graph are represented:
	the collection of images hasn’t been loaded, and each Bid has a reference to an uninitialized User proxy, the
	bidder.

!!!	If you now detach the Item, you can access the loaded seller and bids without causing a LazyInitializationException.
    If you try to access the images or one of the bidder proxies, you’ll get an exception!
	In the following examples, we assume that your domain model has a lazy default fetch plan. Hibernate will only
	load the data you explicitly request and the associations and collections you access.


Selecting a fetch strategy

	Hibernate executes SQL SELECT statements to load data into memory. If you load an entity instance, one or more
	SELECT(s) are executed, depending on the number of tables involved and the fetching strategy you’ve applied.
	Your goal is to minimize the number of SQL statements and to simplify the SQL statements so that querying can be
	as efficient as possible. Consider our recommended fetch plan from earlier in this chapter: every association
	and collection should be loaded on demand, lazily. This default fetch plan will most likely result in too many
	SQL statements, each loading only one small piece of data. This will lead to n+1 selects problems, and we discuss
	this issue first.
	The alternative fetch plan, using eager loading, will result in fewer SQL statements, because larger chunks of
	data are loaded into memory with each SQL query. You might then see the Cartesian product problem, as SQL result sets
	become too large.
    You need to find the middle ground between these two extremes: the ideal fetching strategy for each procedure and
    use case in your application. Like fetch plans, you can set a global fetching strategy in your mappings: the
    default setting that is always active. Then, for a particular procedure, you might override the default fetching
    strategy with a custom JPQL, CriteriaQuery, or even SQL query

!!!
The n+1 selects problem

	The following example code checks whether the seller of each Item has a username:
		List<Item> items = em.createQuery("select i from Item i").getResultList();
		// select * from ITEM
		for (Item item : items) {
			assertNotNull(item.getSeller().getUsername());
			// select * from USERS where ID = ?
		}

	You see one SQL SELECT to load the Item entity instances. Then, while you iterate through all the items,
	retrieving each User requires an additional SELECT. This amounts to one query for the Item plus n queries
	depending on how many items you have and whether a particular User is selling more than one Item. Obviously,
	this is a very inefficient strategy if you know you’ll access the seller of each Item.

	The following example checks whether each Item has some bids:
		List<Item> items = em.createQuery("select i from Item i").getResultList();
		// select * from ITEM
		for (Item item : items) {
		 assertTrue(item.getBids().size() > 0);
		 // select * from BID where ITEM_ID = ?
		}

!!!
	Again, if you know you’ll access each bids collection, loading only one at a time is inefficient. If you have
	100 items, you’ll execute 101 SQL queries! With what you know so far, you might be tempted to change the
	default fetch plan in your mappings and put a FetchType.EAGER on your seller or bids associations. But doing
	so can lead to our next topic: the Cartesian product problem.


The Cartesian product problem

	If you look at your domain and data model and say, “Every time I need an Item, I also need the seller of that
	Item,” you can map the association with FetchType.EAGER instead of a lazy fetch plan. You want a guarantee
	that whenever an Item is loaded, the seller will be loaded right away—you want that data to be available
	 when the Item is detached and the persistence context is closed:

		@Entity
		public class Item {
			 @ManyToOne(fetch = FetchType.EAGER)
			 protected User seller;
			 // ...
		}

	Eagerly loading collections with JOINs, on the other hand, can lead to serious performance issues. If you also
	switched to FetchType.EAGER for the bids and images collections, you’d run into the Cartesian product problem.

	This result set contains many redundant data items, and only the shaded cells are relevant for Hibernate. The
	Item has three bids and three images. The size of the product depends on the size of the collections you’re
	retrieving: three times three is nine rows total. Now imagine that you have an Item with 50 bids and 5
	images—you’ll see a result set with possibly 250 rows! You can create even larger SQL products when
	you write your own queries with JPQL or CriteriaQuery: imagine what happens if you load 500 items and eager-fetch
	dozens of bids and images with JOINs.

	Considerable processing time and memory are required on the database server to create such results, which then
	must be transferred across the network. If you’re hoping that the JDBC driver will compress the data on the wire
	somehow, you’re probably expecting too much from database vendors


Prefetching data in batches

		@Entity
		@org.hibernate.annotations.BatchSize(size = 10)
		@Table(name = "USERS")
		public class User {
			// ...
		}
	
	This setting tells Hibernate that it may load up to 10 User proxies if one has to be loaded, all with the same
	SELECT. Batch fetching is often called a blind-guess optimization, because you don’t know how many uninitialized
	User proxies may be in a particular persistence context. You can’t say for sure that 10 is an ideal value—it’s a
	guess. You know that instead of n+1 SQL queries, you’ll now see n+1/10 queries, a significant
	reduction. Reasonable values are usually small, because you don’t want to load too much data into memory either,
	especially if you aren’t sure you’ll need it

	Once you access the eleventh seller, another 10 are loaded in one batch, and so on, until the persistence
	context contains no uninitialized User proxies.

	Batch fetching is also available for collections:

		@Entity
		public class Item {
			 @OneToMany(mappedBy = "item")
			 @org.hibernate.annotations.BatchSize(size = 5)
			 protected Set<Bid> bids = new HashSet<>();
			 // ...
		}

!!!
	Although you may prefetch data you won’t need in the end and consume more memory, the reduction in database
	round trips can make a huge difference. Memory is cheap, but scaling database servers isn’t.


Prefetching collections with subselects
	
	A potentially better strategy for loading all bids of several Item instances is prefetching with a subselect. To
	enable this optimization, add a Hibernate annotation to your collection mapping:
		@Entity
		public class Item {
			 @OneToMany(mappedBy = "item")
			 @org.hibernate.annotations.Fetch(org.hibernate.annotations.FetchMode.SUBSELECT)
			 protected Set<Bid> bids = new HashSet<>();
			 // ...
		}
	
	Hibernate now initializes all bids collections for all loaded Item instances as soon as you force the initialization
	of one bids collection.

	List<Item> items = em.createQuery("select i from Item i").getResultList();
	// select * from ITEM
	for (Item item : items) {
		assertTrue(item.getBids().size() > 0);
		// select * from BID where ITEM_ID in ( select ID from ITEM )
	}
	
	Hibernate remembers the original query used to load the items. It then embeds this 	initial query (slightly modified)
	in a subselect, retrieving the collection of bids for each Item.


Eager fetching with multiple SELECTs
 
	When you’re trying to fetch several collections with one SQL query and JOINs, you run into the Cartesian product
	problem, as explained earlier. Instead of a JOIN operation, you can tell Hibernate to eagerly load data with additional
	SELECT queries and hence avoid large results and SQL products with duplicates

		@Entity
		public class Item {
			 @ManyToOne(fetch = FetchType.EAGER)
			 @org.hibernate.annotations.Fetch(org.hibernate.annotations.FetchMode.SELECT)
			 protected User seller;								// defaults to join, but by using this FetchMode, it uses this other approach
			 
			 @OneToMany(mappedBy = "item", fetch = FetchType.EAGER)
			 @org.hibernate.annotations.Fetch(org.hibernate.annotations.FetchMode.SELECT)
			 protected Set<Bid> bids = new HashSet<>();			// defaults to join, but by using this FetchMode, it uses this other approach
			 // ...
			 
		}

	Now, when an Item is loaded, the seller and bids have to be loaded as well:

		Item item = em.find(Item.class, ITEM_ID);
		// select * from ITEM where ID = ?
		// select * from USERS where ID = ?
		// select * from BID where ITEM_ID = ?
		em.detach(item);

	Hibernate uses one SELECT to load a row from the ITEM table. It then immediately executes two more SELECTs: one
	loading a row from the USERS table (the seller) and the other loading several rows from the BID table (the bids).
	The additional SELECT queries aren’t executed lazily; the find() method produces several SQL queries.


Using fetch profiles

	Fetch profiles complement the fetching options in the query languages and APIs. They allow you to maintain your
	profile definitions in either XML or annotation metadata. Early Hibernate versions didn’t have support for special
	fetch profiles, but today Hibernate supports the following:
		 Fetch profiles — A proprietary API based on declaration of the profile with @org.hibernate.annotations.FetchProfile
		and execution with Session #enableFetchProfile(). This simple mechanism currently supports overriding
		lazy-mapped entity associations and collections selectively, enabling a JOIN eager fetching strategy for a particular
		unit of work.
		 Entity graphs — Specified in JPA 2.1, you can declare a graph of entity attributes and associations with the
		 @EntityGraph annotation. This fetch plan, or a combination of plans, can be enabled as a hint when executing
		 EntityManager #find() or queries (JPQL, criteria). The provided graph controls what should be loaded;
		 unfortunately it doesn’t control how it should be loaded
	
	It’s fair to say that there is room for improvement here, and we expect future versions of Hibernate and JPA to offer
	a unified and more powerful API.


Declaring Hibernate fetch profiles

	Hibernate fetch profiles are global metadata: they’re declared for the entire persistence unit. Although you could place the @FetchProfile annotation on a class, we prefer it as package-level metadata in a package-info.java:

		@org.hibernate.annotations.FetchProfiles({
			@FetchProfile(name = Item.PROFILE_JOIN_SELLER,						// profile name
				fetchOverrides = @FetchProfile.FetchOverride(			// override
					entity = Item.class,
					association = "seller",
					mode = FetchMode.JOIN									// join mode
				)),
			@FetchProfile(name = Item.PROFILE_JOIN_BIDS,
				fetchOverrides = @FetchProfile.FetchOverride(
					 entity = Item.class,
					 association = "bids",
					 mode = FetchMode.JOIN
				))
		})

	1. Each profile has a name. This is a simple string isolated in a constant.
	2. Each override in a profile names one entity association or collection.
	3. The only supported mode at the time of writing is JOIN

	The profiles can now be enabled for a unit of work:

		Item item = em.find(Item.class, ITEM_ID);									//1 Retrieves instance
		em.clear();
		em.unwrap(Session.class).enableFetchProfile(Item.PROFILE_JOIN_SELLER);			//2 Enables profile
		item = em.find(Item.class, ITEM_ID);
		em.clear();
		em.unwrap(Session.class).enableFetchProfile(Item.PROFILE_JOIN_BIDS);		//3 Overlays second profile
		item = em.find(Item.class, ITEM_ID);	

		1. The Item#seller is mapped lazy, so the default fetch plan only retrieves the Item instance.
		2. You need the Hibernate API to enable a profile. It’s then active for any operation in that unit of work. The
		 Item#seller is fetched with a join in the same SQL statement whenever an Item is loaded with this EntityManager.
		3. You can overlay another profile on the same unit of work. Now the Item#seller and the Item#bids collection are
		fetched with a join in the same SQL statement whenever an Item is loaded


Working with entity graphs

	An entity graph is a declaration of entity nodes and attributes, overriding or augmenting the default fetch plan
	when you execute an EntityManager#find() or with a hint on query operations. This is an example of a retrieval
	operation using an entity graph:

		Map<String, Object> properties = new HashMap<>();
		properties.put(
			"javax.persistence.loadgraph",
			em.getEntityGraph(Item.class.getSimpleName())
		);
		Item item = em.find(Item.class, ITEM_ID, properties);
		// select * from ITEM where ID = ?

	The name of the entity graph you’re using is Item, and the hint for the find() operation indicates it should be the
	load graph. This means attributes that are specified by attribute nodes of the entity graph are treated as
	FetchType.EAGER, and attributes that aren’t specified are treated according to their specified or default FetchType
	in the mapping.

	This is the declaration of this graph and the default fetch plan of the entity class:

		@NamedEntityGraphs({
			@NamedEntityGraph							// Default “Item” entity graph
		})
		@Entity
		public class Item {
			 @NotNull
			 @ManyToOne(fetch = FetchType.LAZY)
			 protected User seller;
			 
			 @OneToMany(mappedBy = "item")
			 protected Set<Bid> bids = new HashSet<>();
			 
			 @ElementCollection
			 protected Set<String> images = new HashSet<>();
			 // ...
		}

	Entity graphs in metadata have names and are associated with an entity class; they’re usually declared in annotations
	on top of an entity class. You can put them in XML if you like. If you don’t give an entity graph a name, it gets the
	simple name of its owning entity class, which here is Item. If you don’t specify any attribute nodes in the graph,
	like the empty entity graph in the last example, the defaults of the entity class
	are used. In Item, all associations and collections are mapped lazy; this is the default fetch plan. Hence, what
	 you’ve done so far makes little difference, and the find() operation without any hints will produce the same
	 result: the Item instance is loaded, and the seller, bids, and images aren’t.

	Let’s say you want to write an entity graph that changes the lazy default of Item#seller to eager fetching, when enabled:

		@NamedEntityGraphs({
		 @NamedEntityGraph(
			 name = "ItemSeller",
			 attributeNodes = {
				@NamedAttributeNode("seller")
			}
		 )
		})
		@Entity
		public class Item {
			// ...
		}
	Now enable this graph by name when you want the Item and the seller eagerly loaded:

		Map<String, Object> properties = new HashMap<>();
		properties.put(
			 "javax.persistence.loadgraph",
			 em.getEntityGraph("ItemSeller")
		);
		Item item = em.find(Item.class, ITEM_ID, properties);
		// select i.*, u.*
		// from ITEM i
		// inner join USERS u on u.ID = i.SELLER_ID
		// where i.ID = ?

	If you don’t want to hardcode the graph in annotations, build it with the API instead
	So far you’ve seen only properties for the find() operation. Entity graphs can also be enabled for queries, as hints


Summary
 
	 A fetch profile combines a fetch plan (what data should be loaded) with a fetch strategy (how the data should be
	loaded), encapsulated in reusable metadata or code.
	 You created a global fetch plan and defined which associations and collections should be loaded into memory at
	all times. You defined the fetch plan based on use cases, how to access associated entities and iterate through
	collections in your application, and which data should be available in detached state.
	 You learned to select the right fetching strategy for your fetch plan. Your goal is to minimize the number of
	SQL statements and the complexity of each SQL statement that must be executed. You especially want to avoid the
	n+1 selects and Cartesian product issues we examined in detail, using various optimization strategies.
	 You explored Hibernate fetch profiles and entity graphs, the fetch profiles in JPA. 



Filtering data

	In this chapter, you see many different strategies for filtering data as it passes through the Hibernate engine. When
	Hibernate loads data from the database, you can transparently restrict the data seen by the application with a filter.
	When Hibernate stores data in the database, you can listen to such an event and execute some secondary routines: for
	example, write an audit log or assign a tenant identifier to the record.

	We explore the following data-filtering features and APIs:
	 In section 13.1, you learn to react to state changes of an entity instance and cascade the state change to associated
	entities. For example, when a User is saved, Hibernate can transitively and automatically save all related BillingDetails.
	When an Item is deleted, Hibernate can delete all Bid instances associated with that Item. You can enable this standard
	JPA feature with special attributes in your entity association and collection mappings
	 The Java Persistence standard includes life cycle callbacks and event listeners. An event listener is a class you
	write with special methods, called by Hibernate when an entity instance changes state: for example, after Hibernates
	loads it or is about to delete it from the database. These callback methods can also be on your entity classes and
	 marked with special annotations. This gives you an opportunity to execute custom side effects when a transition occurs.
	 Hibernate also has several proprietary extension points that allow interception of life cycle events at a lower level
	 within its engine, which we discuss in section 13.2.
	 A common side effect is writing an audit log; such a log typically contains information about the data that was
	changed, when the change was made, and who made the modification. A more sophisticated auditing system might require
	storing several versions of data and temporal views; you might want to ask Hibernate to load data “as it was last week.”
	This being a complex problem, we introduce Hibernate Envers in section 13.3, a subproject dedicated to versioning and
	auditing in JPA applications.
	 In section 13.4, you see that data filters are also available as a proprietary Hibernate API. These filters add
	custom restrictions to SQL SELECT statements executed by Hibernate. Hence, you can effectively define a custom limited
	view of the data in the application tier. For example, you could apply a filter that restricts loaded data by sales region
	, or any other authorization criteria.

	Major new feature in JPA 2
		 Injection of dependencies through CDI is now supported in JPA entity event
		listener classes.


Cascading state transitions

	When an entity instance changes state—for example, when it was transient and becomes persistent—associated entity
	instances may also be included in this state transition. This cascading of state transitions isn’t enabled by default;
	each entity instance has an independent life cycle. But for some associations between entities, you may want to implement
	fine-grained life cycle dependencies.

	For example, in section 7.3, you created an association between the Item and Bid entity classes. In this case, not
	only did you make the bids of an Item automatically persistent when they were added to an Item, but they were also
	 automatically deleted when the owning Item was deleted. You effectively made Bid an entity class that was dependent
	on another entity, Item.
	The cascading settings you enabled in this association mapping were CascadeType.PERSIST and CascadeType.REMOVE


Available cascading options

	Table 13.1 summarizes all available cascading options in Hibernate. Note how each is linked with an EntityManager
	or Session operation.

	If you’re curious, you’ll find more cascading options defined in the org.hibernate.annotations.CascadeType enumeration. 


Transitive detachment and merging
	
	Let’s say you want to retrieve an Item and its bids from the database and work with this data in detached state.
	The Bid class maps this association with an @ManyToOne. It’s bidirectional with this @OneToMany collection mapping in Item:

		@Entity
		public class Item {
			 @OneToMany(
				mappedBy = "item",
				cascade = {CascadeType.DETACH, CascadeType.MERGE}
			 )
			 protected Set<Bid> bids = new HashSet<Bid>();
			 // ...
		}

	Transitive detachment and merging is enabled with the DETACH and MERGE cascade types. Now you load the Item and
	initialize its bids collection:

		Item item = em.find(Item.class, ITEM_ID);
		assertEquals(item.getBids().size(), 2);
		em.detach(item);

	The EntityManager#detach() operation is cascaded: it evicts the Item instance from the persistence context as well
	as all bids in the collection. If the bids aren’t loaded, they aren’t detached. (Of course, you could have closed
	the persistence context, effectively detaching all loaded entity instances.)
	In detached state, you change the Item#name, create a new Bid, and link it with the Item:

		item.setName("New Name");
		Bid bid = new Bid(new BigDecimal("101.00"), item);
		item.getBids().add(bid);

	Because you’re working with detached entity state and collections, you have to pay extra attention to identity and
	equality. As explained in section 10.3, you should override the equals() and hashCode() methods on the Bid entity
	class:Two Bid instances are equal when they have the same amount and are linked with the same Item. After you’re
	done with your modifications in detached state, the next step is to store the changes. Using a new persistence
	context, merge the detached Item and let Hibernate detect the changes:

		Item mergedItem = em.merge(item);														// 1. merge item
		// select i.*, b.*
		// from ITEM i
		// left outer join BID b on i.ID = b.ITEM_ID
		// where i.ID = ?
		
		for (Bid b : mergedItem.getBids()) {
		 assertNotNull(b.getId());															// 2. bid has identifier value
		}
		em.flush();																			//3. detects name change
		// update ITEM set NAME = ? where ID = ?
		// insert into BID values (?, ?, ?, ...)


		1. Hibernate merges the detached item. First it checks whether the persistence context already contains an Item
		with the given identifier value. In this case, there isn’t any, so the Item is loaded from the database. Hibernate
		is smart enough to know that it will also need the bids during merging, so it fetches them right away in the same
		SQL query. Hibernate then copies the detached item values onto the loaded instance, which it returns to you in
		persistent state. The same procedure is applied to every Bid, and Hibernate will detect that one of the bids is new
		2. Hibernate made the new Bid persistent during merging. It now has an identifier value assigned.
		3. When you flush the persistence context, Hibernate detects that the name of the Item changed during merging. The
		new Bid will also be stored

	Cascaded merging with collections is a powerful feature; consider how much code you would have to write without
	Hibernate to implement this functionality.

TODO Continue from here
Listening to and intercepting events

	In this section, we discuss three different APIs for custom event listeners and persistence life cycle
	interceptors available in JPA and Hibernate. You can
		 Use the standard JPA life cycle callback methods and event listeners.
		 Write a proprietary org.hibernate.Interceptor and activate it on a Session.
		 Use extension points of the Hibernate core engine with the org.hibernate.event SPI


JPA event listeners and callbacks	

	Let’s start with the standard JPA callbacks. They offer easy access to persist, load, and remove life
	cycle events.

	Let’s say you want to send a notification item to a system administrator whenever a new entity instance
	is stored. First, write a life cycle event listener with a callback
	method, annotated with @PostPersist, as shown in the following listing.

		public class PersistEntityListener {						// Entity listener constructor
			 @PostPersist																													// Makes notfyAdmin() a callback method
			 public void notifyAdmin(Object entityInstance) {
				 User currentUser = CurrentUser.INSTANCE.get();
				 Mail mail = Mail.INSTANCE;
				 mail.send(
					 "Entity instance persisted by "
					 + currentUser.getUsername()
					 + ": "
					 + entityInstance
				 );
			 }
		}

		An entity listener class must have either no constructor or a public no-argument constructor.
		It doesn’t have to implement any special interfaces. An entity listener is stateless; the JPA
		engine automatically creates and destroys it.

		You may annotate any method of an entity listener class as a callback method for persistence life
		cycle events. The notifyAdmin() method is invoked after a new entity
		instance is stored in the database.

		Because event listener classes are stateless, it can be difficult to get more contextual information
		when you need it. Here, you want the currently logged-in user and access to the
		item system to send a notification. A primitive solution is to use thread-local variables and
		singletons; you can find the source for CurrentUser and Mail in the example code.

	A callback method of an entity listener class has a single Object parameter: the entity instance
	involved in the state change. If you only enable the callback for a particular entity type, you may
	declare the argument as that specific type. The callback method may have any kind of access; it doesn’t
	have to be public. It must not be static or final and return nothing. If a callback method throws
	an unchecked RuntimeException,
	Hibernate will abort the operation and mark the current transaction for rollback.

	Injection in event listener classes
	You often need access to contextual information and APIs when implementing an event listener. The
	previous example needs the currently logged-in user and an item API. A simple solution based on
	thread-locals and singletons might not be sufficient in larger and more complex applications. JPA
	also standardizes integration with CDI, so an entity listener class may rely on injection and the
	@Inject annotation to access dependencies. The CDI container provides the contextual information
	when the listener class is called. Note that even with CDI, you can’t inject the current
	EntityManager to access the database in an event listener. We discuss a different solution for
	accessing the database in a (Hibernate) event listener later in this chapter.

	You may only use each callback annotation once in an entity listener class; that is, only one method
	may be annotated @PostPersist.

	See table 13.2 for a summary of all available callback annotations :
	@PostLoad @PrePersist @PostPersist @PreUpdate, @PostUpdate @PreRemove, @PostRemove

	An entity listener class must be enabled for any entity you’d like to intercept, such as this Item:

		@Entity
		@EntityListeners(
			PersistEntityListener.class
		)
		public class Item {
			// ...
		}

	The @EntityListeners annotation accepts an array of listener classes, if you have several
	interceptors. If several listeners define callback methods for the same event, Hibernate
	invokes the listeners in the declared order

!!!
	You don’t have to write a separate entity listener class to intercept life cycle events. You can,
	for example, implement the notifyAdmin() method on the User entity class:

		@Entity
		@Table(name = "USERS")
		public class User {
		
		 // ..........
		
		 @PostPersist
		 public void notifyAdmin(){
			 User currentUser = CurrentUser.INSTANCE.get();
			 Mail mail = Mail.INSTANCE;
			 mail.send(
				 "Entity instance persisted by "
				 + currentUser.getUsername()
				 + ": "
				 + this
			 );
		 }
		 // ...
		}
	Note that callback methods on an entity class don’t have any arguments: the “current” entity involved
	in the state changes is this.
	You can also add callback methods on an entity superclass for the entire hierarchy. If, for a
	particular entity subclass, you want to disable the superclass’s callbacks, annotate the subclass
	with @ExcludeSuperclassListeners or map it in XML metadata with <exclude-superclass-listeners>.

	Be aware that enabling entity listeners is additive. If you enable and/or bind entity listeners in
	XML metadata and annotations, Hibernate will call them all in the following order:
		1 Default listeners for the persistence unit, in the order as declared in XML metadata.
		2 Listeners declared on an entity with @EntityListeners, in the given order.
		3 Callback methods declared in entity superclasses are first, starting with the most generic
		superclass. Callback methods on the entity class are last.


Implementing Hibernate interceptors

	Let’s assume that you want to write an audit log of data modifications in a separate database table.
	For example, you may record information about creation and update events for each Item. The audit log
	includes the user, the date and time of the event, what type of event occurred, and the identifier
	of the Item that was changed. Audit logs are often handled using database triggers. On the other
	hand, it’s sometimes better for the application to take responsibility, especially if portability
	between different databases is required.

	You need several elements to implement audit logging. First, you have to mark the entity classes
	for which you want to enable audit logging. Next, you define what information to log, such as the
	user, date, time, and type of modification. Finally, you tie it all together with an
	org.hibernate.Interceptor that automatically creates the audit trail.

		public interface Auditable {
			public Long getId();
		}
	This interface requires that a persistent entity class expose its identifier with a getter method;
	you need this property to log the audit trail. Enabling audit logging for a particular persistent
	class is then trivial. You add it to the class declaration, such as for Item:

		@Entity
		public class Item implements Auditable {
			// ...
		}
	Now, create a new persistent entity class, AuditLogRecord, with the information you want to log in
	your audit database table:
		@Entity
		public class AuditLogRecord {
			 @Id
			 @GeneratedValue(generator = "ID_GENERATOR")
			 protected Long id;

			 @NotNull
			 protected String item;

			 @NotNull
			 protected Long entityId;

			 @NotNull
			 protected Class entityClass;

			 @NotNull
			 protected Long userId;

			 @NotNull
			 @Temporal(TemporalType.TIMESTAMP)
			 protected Date createdOn = new Date();
			 // ...
		}

	You want to store an instance of AuditLogRecord whenever Hibernate inserts or updates an Item in the
	database. A Hibernate interceptor can handle this automatically. Instead of implementing all
	methods in org.hibernate.Interceptor, extend the EmptyInterceptor and override only the methods
	you need, as shown next

		public class AuditLogInterceptor extends EmptyInterceptor {
			 protected Session currentSession;												// 1 Accesses database
			 protected Long currentUserId;
			 protected Set<Auditable> inserts = new HashSet<Auditable>();
			 protected Set<Auditable> updates = new HashSet<Auditable>();
			 
			 public void setCurrentSession(Session session) {
				this.currentSession = session;
			 }
			 
			 public void setCurrentUserId(Long currentUserId) {
				this.currentUserId = currentUserId;
			 }
			 
			public boolean onSave(Object entity, Serializable id,  Object[] state, String[] propertyNames, Type[] types) throws CallbackException {		//2 Called when instance is made persistent 
				if (entity instanceof Auditable)
					inserts.add((Auditable)entity);
				return false;	
			 }
			 
			 public boolean onFlushDirty(Object entity, Serializable id, Object[] currentState, Object[] previousState, String[] propertyNames, Type[] types) throws CallbackException {	// 3 Called if instance is dirty
				if (entity instanceof Auditable)
					updates.add((Auditable)entity);
				return false;
				}
			 // ...
		}

		1. You need to access the database to write the audit log, so this interceptor needs a Hibernate
		Session. You also want to store the identifier of the currently logged-in user in each audit log
		record. The inserts and updates instance variables are collections where this interceptor
		will hold its internal state.
		2. This method is called when an entity instance is made persistent.
		3. This method is called when an entity instance is detected as dirty during flushing of the
		persistence context.

	The interceptor collects the modified Auditable instances in inserts and updates. Note that in
	onSave(), there may not be an identifier value assigned to the given entity instance. Hibernate
	guarantees to set entity identifiers during flushing, so the actual audit log trail is written in
	the postFlush() callback, which is shown below
	
		public class AuditLogInterceptor extends EmptyInterceptor {
			 // ...
			 public void postFlush(Iterator iterator) throws CallbackException {
				 Session tempSession =
					 currentSession.sessionWithOptions()
					 .transactionContext()
					 .connection()
					 .openSession();
				 try {
					for (Auditable entity : inserts) {
						 tempSession.persist(
							new AuditLogRecord("insert", entity, currentUserId)
						 );
					 }
					 for (Auditable entity : updates) {
						tempSession.persist(new AuditLogRecord("update", entity, currentUserId));
					 }
					 tempSession.flush();
				 } finally {
					tempSession.close();
					inserts.clear();
					updates.clear();
				 }
			 }
		}

	This method is called after flushing of the persistence context is complete. Here, you write the
	audit log records for all insertions and updates you collected earlier
	You can’t access the original persistence context: the Session that is currently executing this
	interceptor. The Session is in a fragile state during interceptor calls. Hibernate lets you create
	a new Session that inherits some information from the original Session with the sessionWithOptions()
	method. The new temporary Session works with the same transaction and database connection as the
	original Session.

	You’re now ready to enable this interceptor with a Hibernate property when creating an EntityManager:

		EntityManagerFactory emf = JPA.getEntityManagerFactory();
		Map<String, String> properties = new HashMap<String, String>();
		properties.put(
			 org.hibernate.jpa.AvailableSettings.SESSION_INTERCEPTOR,
			 AuditLogInterceptor.class.getName()
		);
		EntityManager em = emf.createEntityManager(properties);
	
	This EntityManager now has an enabled AuditLogInterceptor, but the interceptor must also be configured
	with the current Session and logged-in user identifier. This involves some typecasts to access the
	Hibernate API:
		Session session = em.unwrap(Session.class);
		AuditLogInterceptor interceptor = (AuditLogInterceptor) ((SessionImplementor) session).getInterceptor();
		interceptor.setCurrentSession(session);
		interceptor.setCurrentUserId(CURRENT_USER_ID);

	The EntityManager is now ready for use, and an audit trail will be written whenever you store or modify
	an Item instance with it.

	Hibernate interceptors are flexible, and, unlike JPA event listeners and callback methods, you have
	access to much more contextual information when an event occurs. Having said that, Hibernate allows
	you to hook even deeper into its core with the extensible event system it’s based on (see next chapter)


The core event system

	The Hibernate core engine is based on a model of events and listeners. For example, if Hibernate needs
	to save an entity instance, it triggers an event. Whoever listens to this kind of event can catch
	it and handle saving the data. Hibernate therefore implements all of its core functionality as a set
	of default listeners, which can handle all Hibernate events.

	Hibernate is open by design: you can write and enable your own listeners for Hibernate events. You
	can either replace the existing default listeners or extend them and execute a side effect or
	additional procedure. Replacing the event listeners is rare; doing so implies that your own listener
	implementation can take care of a piece of Hibernate core functionality.

	Essentially, all the methods of the Session interface (and its narrower cousin, the EntityManager)
	correlate to an event. The find() and load() methods trigger a LoadEvent, and by default this event
	is processed with the DefaultLoadEventListener.

	A custom listener should implement the appropriate interface for the event it wants to process and/or
	extend one of the convenience base classes provided by Hibernate, or any of the default event
	listeners. Here’s an example of a custom load event listener.

		public class SecurityLoadListener extends DefaultLoadEventListener {
		 public void onLoad(LoadEvent event, LoadEventListener.LoadType loadType) throws HibernateException {
			boolean authorized =  MySecurity.isAuthorized(event.getEntityClassName(), event.getEntityId());
			if (!authorized)
				throw new MySecurityException("Unauthorized access");
			super.onLoad(event, loadType);
		 }
		}

	This listener performs custom authorization code. A listener should be considered effectively a singleton,
	meaning it’s shared between persistence contexts and thus shouldn’t save any transaction-related state as
	instance variables. For a list of all events and listener interfaces in native Hibernate, see the API Javadoc
	of the org.hibernate.event package.

	You enable listeners for each core event in your persistence.xml, in a <persistenceunit>:
		<properties>
			<property name="hibernate.ejb.event.load" value="org.jpwh.test.filtering.SecurityLoadListener"/>
		</properties>

	You can find a list of all event types in org.hibernate.event.spi.EventType. The value of the property can be
	a comma-separated list of listener class names; Hibernate will call each listener in the specified order.


Auditing and versioning with Hibernate Envers

	Envers is a project of the Hibernate suite dedicated to audit logging and keeping multiple versions of data in
	the database. This is similar to version control systems you may already be familiar with, such as Subversion
	and Git.

	With Envers enabled, a copy of your data is automatically stored in separate database tables when you add,
	modify, or delete data in the main tables of the application. Envers internally uses the Hibernate event SPI
	you saw in the previous section. Envers listens to Hibernate events, and when Hibernate stores changes in the
	database, Envers creates a copy of the data and logs a revision in its own tables


Enabling audit logging

	Envers is available without further configuration as soon as you put its JAR file on your classpath
	 (or, as shown in the example code of this book, include it as a Maven dependency). You enable audit logging
	 selectively for an entity class with the @org.hibernate.envers.Audited annotation.

		@Entity
		@org.hibernate.envers.Audited
		public class Item {
			 @NotNull
			 protected String name;
			 
			 @OneToMany(mappedBy = "item")
			 @org.hibernate.envers.NotAudited
			 protected Set<Bid> bids = new HashSet<Bid>();
			 
			 @ManyToOne(fetch = FetchType.LAZY)
			 @JoinColumn(name = "SELLER_ID", nullable = false)
			 protected User seller;
			 // ...
		}

	You’ve now enabled audit logging for Item instances and all properties of the entity. To disable audit logging
	for a particular property, annotate it with @NotAudited. In this case, Envers ignores the bids but audits the
	seller. You also have to enable auditing with @Audited on the User class.

	Hibernate will now generate (or expect) additional database tables to hold historical data for each Item and User.
	Figure 13.1 shows the schema for these tables.


Creating an audit trail
 
	Envers transparently writes the audit trail for this sequence of transactions by logging three change sets.
	To access this historical data, you first have to obtain the number of the revision, representing the change
	set you’d like to access.


Finding revisions					(no time to add here examples)
Accessing historical data			(no time to add here examples)


Dynamic data filters
 
	The first use case for dynamic data filtering relates to data security. A User in CaveatEmptor may have a
	ranking property, which is a simple integer:
		@Entity
		@Table(name = "USERS")
		public class User {
			 @NotNull
			 protected int rank = 0;
			 // ...
		}

	Now assume that users can only bid on items that other users offer with an equal or lower rank. In business
	terms, you have several groups of users that are defined by an arbitrary rank (a number), and users can trade
	only with people who have the same or lower rank. To implement this requirement, you’d have to customize
	all queries that load Item instances from the database. You’d check whether the Item#seller you want to load
	has an equal or lower rank than the currently logged-in user. Hibernate can do this work for you with a
	dynamic filter.


Defining dynamic filters

	First, you define your filter with a name and the dynamic runtime parameters it accepts. You can place the
	Hibernate annotation for this definition on any entity class of your domain model or in a package-info.java
	metadata file:
		@org.hibernate.annotations.FilterDef(
			name = "limitByUserRank",
			parameters = {
				@org.hibernate.annotations.ParamDef(
					name = "currentUserRank", type = "int"
				)
			}
		)

	This example names this filter limitByUserRank; note that filter names must be unique in a persistence unit.
	It accepts one runtime argument of type int. If you have several filter definitions, declare them within
	@org.hibernate.annotations.FilterDefs.

	The filter is inactive now; nothing indicates that it’s supposed to apply to Item instances. You must
	apply and implement the filter on the classes or collections you want to filter.


Applying the filter

	You want to apply the defined filter on the Item class so that no items are visible if the logged-in user doesn’t
	have the necessary rank:

		@Entity
		@org.hibernate.annotations.Filter(
		 name = "limitByUserRank",
		 condition =
			 ":currentUserRank >= (" +
			 "select u.RANK from USERS u " +
			 "where u.ID = SELLER_ID" +
			 ")"
		)
		public class Item {
			// ...
		}

	The condition is an SQL expression that’s passed through directly to the database system, so you can use any SQL
	operator or function. It must evaluate to true if a record should pass the filter. In this example, you use a
	subquery to obtain the rank of the seller of the item. Unqualified columns, such as SELLER_ID, refer to the
	table mapped to the entity class. If the currently logged-in user’s rank isn’t greater than or equal to
	the rank returned by the subquery, the Item instance is filtered out. You can apply several filters by grouping
	them in an @org.hibernate.annotations.Filters.

	A defined and applied filter, if enabled for a particular unit of work, filters out any Item instance that
	doesn’t pass the condition. Let’s enable it.


Enabling the filter

	You’ve defined a data filter and applied it to a persistent entity class. It’s still not filtering anything—it
	must be enabled and parameterized in the application for a particular unit of work, with the Session API:

		org.hibernate.Filter filter = em.unwrap(Session.class).enableFilter("limitByUserRank");
		filter.setParameter("currentUserRank", 0);


	You enable the filter by name; the method returns a Filter on which you set the runtime arguments dynamically.
	You must set the parameters you’ve defined; here it’s set to rank 0. This example then filters out Items sold
	by a User with a higher rank in this Session.

	Now, every JPQL or criteria query that you execute on the filtered persistence context restricts the returned
	Item instances:
		List<Item> items = em.createQuery("select i from Item i").getResultList();
		// select * from ITEM where 0 >= (select u.RANK from USERS u where u.ID = SELLER_ID)
	Note how Hibernate dynamically appends the SQL restriction conditions to the statement generated

!!!
	When you first experiment with dynamic filters, you’ll most likely run into an issue with retrieval by identifier.
	You might expect that em.find(Item.class, ITEM_ID) will be filtered as well. This is not the case, though:
	Hibernate doesn’t apply filters to retrieval by identifier operations. One of the reasons is that data-filter
	conditions are SQL fragments, and lookup by identifier may be resolved completely in memory, in the
	first-level persistence context cache. Similar reasoning applies to filtering of manyto-one or one-to-one
	associations. If a many-to-one association was filtered (for example, by returning null if
	you called anItem.getSeller()), the multiplicity of the association would change! You won’t know if the item
	has a seller or if you aren’t allowed to see it.


Filtering collection access

	Until now, calling someCategory.getItems() has returned all Item instances that are referenced by that Category.
	This can be restricted with a filter applied to a collection:

		@Entity
		public class Category {
			@OneToMany(mappedBy = "category")
			@org.hibernate.annotations.Filter(
				name = "limitByUserRank",
				condition =
					":currentUserRank >= (" +
					"select u.RANK from USERS u " +
					"where u.ID = SELLER_ID" +
					")"
			)
   		    protected Set<Item> items = new HashSet<Item>();
			// ...
		}

	If you now enable the filter in a Session, all iteration through a collection of Category#items is filtered:
		filter.setParameter("currentUserRank", 0);
		Category category = em.find(Category.class, CATEGORY_ID);
		assertEquals(category.getItems().size(), 1);

	If the current user’s rank is 0, only one Item is loaded when you access the collection. Now, with a rank of
	100, you see more data:
		filter.setParameter("currentUserRank", 100);
		category = em.find(Category.class, CATEGORY_ID);
		assertEquals(category.getItems().size(), 2);


Summary

	 Cascading state transitions are predefined reactions to life cycle events in the persistence engine.
	 You learned about listening to and intercepting events. You implement event listeners and interceptors to
	add custom logic when Hibernate loads and stores data. We introduced JPA’s event listener callbacks and
	Hibernate’s Interceptor extension point, as well as the Hibernate core event system.
	 You can use Hibernate Envers for audit logging and keeping multiple versions of data in the database (like
	the version control systems). Using Envers, a copy of your data is automatically stored in separate database
	tables when you add, modify, or delete data in application tables. Envers groups all data modifications as a
	change set, in a transaction, with a revision number. You can then query Envers to retrieve historical data.
	 Using dynamic data filters, Hibernate can automatically append arbitrary SQL restrictions to queries it
	generates.




============================================================================================
Part 4 - Writing queries
============================================================================================	

In part 4, we introduce data-query features and cover query languages and APIs in detail. Not all chapters in this
part are written in a tutorial style; we expect you’ll browse this part of the book frequently when building an
application and looking up a solution for a particular query problem. After reading this part, you’ll be
able to get any data you want out of your database using various querying techniques, customizing access as needed.


Creating and executing queries

	If you’ve been using handwritten SQL for a number of years, you may be concerned that ORM will take away some
	of the expressiveness and flexibility you’re used to. This isn’t the case with Hibernate and Java Persistence.
	With Hibernate’s and Java Persistence’s powerful query facilities, you can express almost everything you commonly
	(or even uncommonly) need to express in SQL, but in object-oriented terms—using classes and properties of classes.
	Moreover, you can always fall back to SQL strings and let Hibernate do the heavy lifting of handling the query
	result.

	 Common to all APIs, a query must be prepared in application code before execution. There are three distinct steps:
		1. Create the query, with any arbitrary selection, restriction, and projection of data that you want to retrieve.
		2. Prepare the query: bind runtime arguments to query parameters, set hints, and set paging options. You can
		reuse the query with changing settings.
		3. Execute the prepared query against the database and retrieve the data. You can control how the query is
		executed and how data should be retrieved into memory (all at once or piecemeal, for example).


Creating queries

	JPA represents a query with a javax.persistence.Query or javax.persistence.TypedQuery instance. You create
	queries with the EntityManager#createQuery() method and its variants. You can write the query in the Java
	Persistence Query Language (JPQL), construct it with the CriteriaBuilder and CriteriaQuery APIs, or use
	plain SQL. (There is also javax.persistence.StoredProcedureQuery, covered in section 17.4.)


The JPA query interfaces

	Say you want to retrieve all Item entity instances from the database. With JPQL, this simple query string looks
	quite a bit like the SQL you know:
		Query query = em.createQuery("select i from Item i");

	The JPA provider returns a fresh Query; so far, Hibernate hasn’t sent any SQL to the database. Remember that
	further preparation and execution of the query are separate steps.

!!!	JPQL is compact and will be familiar to anyone with SQL experience. Instead of table and column names, JPQL
	relies on entity class and property names. Except for these class and property names, JPQL is case-insensitive,
	so it doesn’t matter whether you write SeLEct or select

	JPQL (and SQL) query strings can be simple Java literals in your code, as you saw in the previous example.
	Alternatively, especially in larger applications, you can move the query strings out of your data-access code and
	into annotations or XML. A query is then accessed by name with EntityManager#createNamedQuery(). We discuss
	externalized queries separately later in this chapter; there are many options to consider

	A significant disadvantage of JPQL surfaces as problems during refactoring of the domain model: if you rename
	the Item class, your JPQL query will break. (Some IDEs can detect and refactor JPQL strings, though.)

!!!!
    JPA and query languages: HQL vs. JPQL
    Before JPA existed (and even today, in some documentation), the query language in
    Hibernate was called HQL. The differences between JPQL and HQL are insignificant
    now. Whenever you provide a query string to any query interface in Hibernate, either
    with the EntityManager or Session, it’s a JPQL/HQL string. The same engine
    parses the query internally. The fundamental syntax and semantics are the same,
    although Hibernate, as always, supports some special constructs that aren’t standardized in JPA. We’ll tell you
    when a particular keyword or clause in an example only works in Hibernate. To simplify your life, think JPQL
    whenever you see HQL.

	You can make query construction with CriteriaBuilder and CriteriaQuery APIs completely type-safe. JPA also calls
	this query by criteria:
		CriteriaBuilder cb = em.getCriteriaBuilder();
		// Also available on EntityManagerFactory:
		// CriteriaBuilder cb = entityManagerFactory.getCriteriaBuilder();
		CriteriaQuery criteria = cb.createQuery();
		criteria.select(criteria.from(Item.class));
		Query query = em.createQuery(criteria);

	First you get a CriteriaBuilder from your EntityManager by calling getCriteriaBuilder(). If you don’t have an
	EntityManager ready, perhaps because you want to create the query independently from a particular persistence context,
	you may obtain the CriteriaBuilder from the usually globally shared EntityManagerFactory
	You then use the builder to create any number of CriteriaQuery instances. Each CriteriaQuery has at least one root
	class specified with from(); in the last example, that’s Item.class. This is called selection; we’ll talk more
	about it in the next chapter. The shown query returns all Item instances from the database.
	The CriteriaQuery API will appear seamless in your application, without string manipulation. It’s the best choice
	when you can’t fully specify the query at development time and the application must create it dynamically at runtime.
	Imagine that you have to implement a search mask in your application, with many check boxes, input fields,
	and switches the user can enable. You must dynamically create a database query from the user’s chosen search options.
	With JPQL and string concatenation, such code would be difficult to write and maintain.

	If you need to use features specific to your database product, your only choice is native SQL. You can directly
	execute SQL in JPA and let Hibernate handle the result for you, with the EntityManager#createNativeQuery() method:
		Query query = em.createNativeQuery( "select * from ITEM", Item.class );

	After execution of this SQL query, Hibernate reads the java.sql.ResultSet and creates a List of managed Item entity
	instances. Of course, all columns necessary to construct an Item must be available in the result, and an error is
	thrown if your SQL query doesn’t return them properly.

	In practice, the majority of the queries in your application will be trivial—easily expressed in JPQL or with
	a CriteriaQuery. Then, possibly during optimization, you’ll find a handful of complex and performance-critical
	queries. You may have to use special and proprietary SQL keywords to control the optimizer of your DBMS product.
	Most developers then write SQL instead of JPQL and move such complex queries into an XML file, where, with
	the help of a DBA, you change them independently from Java code. Hibernate can still handle the query result for
	you; hence you integrate SQL into your JPA application. There is nothing wrong with using SQL in Hibernate; don’t
	let some kind of ORM “purity” get in your way. When you have a special case, don’t try to hide it, but rather
	expose and document it properly so the next engineer will understand what’s going on.


Typed query results

	Let’s say you want to retrieve only a single Item with a query, given its identifier value:
		
		Query query = em.createQuery("select i from Item i where i.id = :id").setParameter("id", ITEM_ID);
		Item result = (Item) query.getSingleResult();

	In this example, you see a preview of parameter binding and query execution. The important bit is the return value
	of the getSingleResult() method. It’s java.lang.Object, and you have to cast it to an Item. If you provide the
	class of your return value when creating the query, you can skip the cast. This is the job of the
	javax.persistence.TypedQuery interface:
		
		TypedQuery<Item> query = em.createQuery("select i from Item i where i.id = :id", Item.class).setParameter("id", ITEM_ID);
		Item result = query.getSingleResult();			// no cast needed
		
	Query by criteria also supports the TypedQuery interface:
	
		CriteriaBuilder cb = em.getCriteriaBuilder();
		CriteriaQuery<Item> criteria = cb.createQuery(Item.class);
		Root<Item> i = criteria.from(Item.class);
		criteria.select(i).where(cb.equal(i.get("id"), ITEM_ID));
		TypedQuery<Item> query = em.createQuery(criteria);
		Item result = query.getSingleResult();									// no cast needed


TODO Continue from here
Hibernate’s query interfaces

	Hibernate is older than even the first version of JPA, so it also has its own query APIs.
	Hibernate’s own query representations are org.hibernate.Query and org.hibernate.SQLQuery. As usual, they offer more
	than is standardized in JPA, at the cost of portability. They’re also much older than JPA, so there is some feature duplication

	Your starting point for Hibernate’s query API is the Session:

		Session session = em.unwrap(Session.class);
		org.hibernate.Query query = session.createQuery("select i from Item i");
		// Proprietary API: query.setResultTransformer(...);

	Hibernate also has its own SQL result-mapping facility, with org.hibernate.SQLQuery. This example relies on
	placeholders in the SQL string to map columns of the java.sql.ResultSet to entity properties.
		
		Session session = em.unwrap(Session.class);
		org.hibernate.SQLQuery query = session.createSQLQuery("select {i.*} from ITEM {i}").addEntity("i", Item.class);
	
	Hibernate also has an older, proprietary org.hibernate.Criteria query API:

		Session session = em.unwrap(Session.class);
		org.hibernate.Criteria query = session.createCriteria(Item.class);							 /** @deprecated */
		query.add(org.hibernate.criterion.Restrictions.eq("id", ITEM_ID));
		Item result = (Item) query.uniqueResult();


Preparing queries

	A query has several aspects: it defines what data should be loaded from the database and the restrictions that apply,
	such as the identifier of an Item or the name of a User. When you write a query, you shouldn’t code these arguments
	into the query string using string concatenation. You should use parameter placeholders instead and then
	bind the argument values before execution. This allows you to reuse the query with different argument values while
	keeping you’re safe from SQL injection attacks. Depending on your user interface, you frequently also need paging.
	You limit the number of rows returned from the database by your query. For example, you may
	want to return only result rows 1 to 20 because you can only show so much data on each screen, then a bit later you
	want rows 21 to 40, and so on.  Let’s start with parameter binding.

!!!
Protecting against SQL injection attacks

	Without runtime parameter binding, you’re forced to write bad code:
		
		String searchString = getValueEnteredByUser();													// Never do this!
		Query query = em.createQuery("select i from Item i where i.name = '" + searchString + "'");
	
	You should never write code like this, because a malicious user could craft a search string to execute code on the
	database you didn’t expect or want—that is, by entering the value of searchString in a search dialog box as
	foo' and callSomeStoredProcedure() and 'bar' = 'bar.
	As you can see, the original searchString is no longer a simple search for a string but also executes a stored
	procedure in the database! The quote characters aren’t escaped; hence the call to the stored procedure is another
	valid expression in the query. If you write a query like this, you open a major security hole in your application
	by allowing the execution of arbitrary code on your database. This is an SQL injection attack. Never pass unchecked
	values from user input to the database! Fortunately, a simple mechanism prevents this mistake

	The JDBC API includes functionality for safely binding values to SQL parameters. It knows exactly what characters in
	the parameter value to escape so the previous vulnerability doesn’t exist. For example, the database driver escapes
	the single-quote characters in the given searchString and no longer treats them as control characters but as a part
	of the search string value. Furthermore, when you use parameters, the database can efficiently cache precompiled
	prepared statements, improving performance significantly.

!!!
	There are two approaches to parameter binding: named and positional parameters. JPA support both options, but you
	can’t use both at the same time for a particular query.


Binding named parameters

	With named parameters, you can rewrite the query in the previous section as follows:
		
		String searchString = // ...
		Query query = em.createQuery("select i from Item i where i.name = :itemName").setParameter("itemName", searchString);

!!!!!!!!!!!			
	The colon followed by a parameter name indicates a named parameter, here itemName. In a second step, you bind a
	value to the itemName parameter. This code is cleaner, much safer, and performs better, because you can reuse a
	single compiled SQL statement if only parameter values change.

	You can get a Set of Parameters from a Query, either to obtain more information about each parameter (such as name
	or required Java type) or to verify that you’ve bound all parameters properly before execution:
		
		for (Parameter<?> parameter : query.getParameters()) {
			assertTrue(query.isBound(parameter));
		}

	The setParameter() method is a generic operation that can bind all types of arguments. It only needs a little help
	for temporal types:

		Date tomorrowDate = // ...
		Query query = em.createQuery("select i from Item i where i.auctionEnd > :endDate").setParameter("endDate", tomorrowDate, TemporalType.TIMESTAMP);
	
	Hibernate needs to know whether you want only the date or time or the full timestamp bound.

	 For convenience, an entity instance can also be passed to the setParameter() method:

		Item someItem = // ...
		Query query = em.createQuery("select b from Bid b where b.item = :item").setParameter("item", someItem);

	Hibernate binds the identifier value of the given Item. You later see that b.item is a shortcut for b.item.id.


Using positional parameters

	A rarely used and less safe option for value binding is positional query parameters. If you prefer, you can use
	positional parameters instead of named parameters:
		
		Query query = em.createQuery("select i from Item i where i.name like ?1 and i.auctionEnd > ?2");
		query.setParameter(1, searchString);
		query.setParameter(2, tomorrowDate, TemporalType.TIMESTAMP);

!!!
	In this example, the positional parameter markers are indexed ?1 and ?2. You may know this type of parameter
	placeholder from JDBC, but without the numbers and only the question marks. JPA requires that you enumerate
	the placeholders, starting with 1.

	Our recommendation is to avoid positional parameters. They may be more convenient if you build complex queries
	programmatically, but the CriteriaQuery API is a much better alternative for that purpose.

TODO CONTINUE FROM HERE
Paging through large result sets

    A commonly used technique to process large result sets is paging. Users may see the
    result of their search request (for example, for specific items) as a page. This page
    shows a limited subset (say, 10 items) at a time, and users can navigate to the next and
    previous pages manually to view the rest of the result.

	After binding parameters to your query, you may want to enable pagination if you can’t display all results at once.

	A commonly used technique to process large result sets is paging. Users may see the result of their search request
	(for example, for specific items) as a page. This page shows a limited subset (say, 10 items) at a time, and
	users can navigate to the next and previous pages manually to view the rest of the result. The Query interface
	supports paging of the query result. In this query, the requested page starts in the middle of the result set:

		Query query = em.createQuery("select i from Item i");
		query.setFirstResult(40).setMaxResults(10);
	
	Starting from the fortieth row, you retrieve the next 10 rows. The call to setFirstResults(40) starts the result
	set at row 40. The call to setMaxResults(10) limits the query result set to 10 rows returned by the database. Because
	there is no standard way to express paging in SQL, Hibernate knows the tricks to make this work efficiently on
	your particular DBMS.  It’s crucially important to remember that paging operates at the SQL level, on result rows. 

	Hibernate will rewrite your SQL query to include the necessary keywords and clauses for limiting the number of
	returned rows to the page you specified.


Executing queries

	Once you’ve created and prepared a Query, you’re ready to execute it and retrieve the result into memory. Retrieving
	the entire result set into memory in one go is the most common way to execute a query; we call this listing.
	Some other options are available that we also discuss next, such as scrolling and iterating.


Listing all results

	The getResultList() method executes the Query and returns the results as a java.util.List:

		Query query = em.createQuery("select i from Item i");
		List<Item> items = query.getResultList();

	Hibernate executes one or several SQL SELECT statements immediately, depending on your fetch plan. If you map any
	associations or collections with FetchType.EAGER, Hibernate must fetch them in addition to the data you want
	retrieved with your query. All data is loaded into memory, and any entity instances that Hibernate retrieves are
	in persistent state and managed by the persistence context.


Getting a single result

	You may execute a query that returns a single result with the getSingleResult() method:
		TypedQuery<Item> query = em.createQuery("select i from Item i where i.id = :id", Item.class).setParameter("id", ITEM_ID);
		Item item = query.getSingleResult();

	Now, the ugly bits: if there are no results, getSingleResult() throws a NoResultException.
	If there’s more than one result, getSingleResult() throws a NonUniqueResultException.


Scrolling with database cursors

	Plain JDBC provides a feature called scrollable result sets. This technique uses a cursor that the database
	management system holds. The cursor points to a particular row in the result of a query, and the application can
	move the cursor forward and backward. You can even directly jump to a row with the cursor.

	One of the situations where you should scroll through the results of a query instead of loading them all into
	memory involves result sets that are too large to fit into memory. Usually you try to restrict the result further
	by tightening the conditions in the query. Sometimes this isn’t possible, maybe because you need all the data but
	want to retrieve it in several steps. We’ll show such a batch-processing routine in section 20.1.

	JPA doesn’t standardize scrolling through results with database cursors, so you need the
	org.hibernate.ScrollableResults interface available on the proprietary org.hibernate.Query:

		Session session = em.unwrap(Session.class);
		org.hibernate.Query query = session.createQuery("select i from Item i order by i.id asc");
		org.hibernate.ScrollableResults cursor = query.scroll(org.hibernate.ScrollMode.SCROLL_INSENSITIVE);		// 1. Opens cursor
		cursor.setRowNumber(2);																					// 2. Jumps to third row
		Item item = (Item) cursor.get(0);																	// 3. Gets column value
		cursor.close();																								// 4. Closes cursor

	Start by creating an org.hibernate.Query and opening a cursor 1. You then ignore the first two result rows, jump to
	the third row 2, and get that row's first “column” value 3. There are no columns in JPQL, so this is the first
	projection element: here, "i" in the select clause. More examples of projection are available in the next chapter.
	Always close the cursor 4 before you end the database transaction!

	In the previous example, ScrollMode.SCROLL_INSENSITIVE means the cursor isn’t sensitive to changes made in the database,
	effectively guaranteeing that no dirty reads, unrepeatable reads, or phantom reads can slip into your result set
	while you scroll. Other available modes are SCROLL_SENSITIVE and FORWARD_ONLY. A sensitive cursor exposes you to
	committed modified data while the cursor is open; and with a forward-only cursor, you can’t jump to an absolute
	position in the result. Note that the Hibernate persistence context cache still provides repeatable read for entity
	instances even with a sensitive cursor, so this setting can only affect modified scalar values you project in the
	result set.

!!!!
	Be aware that some JDBC drivers don’t support scrolling with database cursors properly, although it might seem to
	work. With MySQL drivers, for example, the drivers always retrieve the entire result set of a query into memory
	immediately; hence you only scroll through the result set in application memory. To get real row-by-row streaming
	of the result, you have to set the JDBC fetch size of the query to Integer .MIN_VALUE (as explained in section
	14.5.4) and only use ScrollMode.FORWARD_ONLY. Check the behavior and documentation of your DBMS and JDBC driver
	before using cursors.


Iterating through a result
 
!!!!!!!!!!!!!!!!
	Let’s say you know that most of the entity instances your query will retrieve are already present in memory. They
	 may be in the persistence context or in the second-level shared cache (see section 20.2). In such a case, it
	 might make sense to iterate the query result with the proprietary org.hibernate.Query API:

			Session session = em.unwrap(Session.class);
			org.hibernate.Query query = session.createQuery("select i from Item i");
			Iterator<Item> it = query.iterate(); // select ID from ITEM
			while (it.hasNext()) {
				Item next = it.next(); // select * from ITEM where ID = ?
				// ...
			}
			Hibernate.close(it);

!!!!!!
	When you call query.iterate(), Hibernate executes your query and sends an SQL SELECT to the database. But Hibernate
	slightly modifies the query and, instead of retrieving all columns from the ITEM table, only retrieves the
	identifier/primary key values.
	Then, every time you call next() on the Iterator, an additional SQL query is triggered and the rest of the ITEM
	row is loaded. Obviously, this will cause an n+1 selects problem unless Hibernate can avoid the additional
	queries on next(). This will be the case if Hibernate can find the item’s data in either the persistence context
	cache or the second-level cache.

	The Iterator returned by iterate() must be closed. Hibernate closes it automatically when the EntityManager or
	Session is closed. If your iteration procedure exceeds the maximum number of open cursors in your database, you
	can close the Iterator manually with Hibernate.close(iterator).

	Iteration is rarely useful, considering that in the example all auction items would have to be in the caches to make
	this routine perform well. Like scrolling with a cursor, you can’t combine it with dynamic fetching and join fetch
	clauses; Hibernate will throw an exception if you try.


Naming and externalizing queries

	So far, the code examples have all embedded query string literals in Java code. This isn’t unreasonable for simple
	queries, but when you begin considering complex queries that must be split over multiple lines, it gets a bit
	unwieldy. Instead, you can give each query a name and move it into annotations or XML files.

	Externalizing query strings lets you store all queries related to a particular persistent class (or a set of
	classes) with the other metadata for that class. Alternatively, you can bundle your queries into an XML file,
	independent of any Java class. This technique is often preferred in larger applications; hundreds of queries are
	easier to maintain in a few well-known places rather than scattered throughout the code base in various classes
	accessing the database. You reference and access an externalized query by its name.


Calling a named query

	The EntityManager#getNamedQuery() method obtains a Query instance for a named query:
		Query query = em.createNamedQuery("findItems");

	You can also obtain a TypedQuery instance for a named query:
		TypedQuery<Item> query = em.createNamedQuery("findItemById", Item.class);
	Hibernate’s query API also supports accessing named queries:
		org.hibernate.Query query = session.getNamedQuery("findItems");

	Named queries are global—that is, the name of a query is a unique identifier for a particular persistence unit
	or org.hibernate.SessionFactory. How and where they’re defined, in XML files or annotations, is no concern of
	your application code. On startup, Hibernate loads named JPQL queries from XML files and/or annotations and parses
	them to validate their syntax. (This is useful during development, but you may want to disable this validation in
	production, for a faster bootstrap, with the persistence unit configuration property hibernate.query.startup_check.)
	 Even the query language you use to write a named query doesn’t matter. It can be JPQL or SQL.


Defining queries in XML metadata

	You can place a named query in any JPA <entity-mappings> element in your orm.xml metadata. In larger applications,
	we recommend isolating and separating all named queries into their own file. Alternatively, you may want the same
	XML mapping file to define the queries and a particular class

	The <named-query> element defines a named JPQL query:

		<entity-mappings version="2.1" xmlns="http://xmlns.jcp.org/xml/ns/persistence/orm"  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="						http://xmlns.jcp.org/xml/ns/persistence/orm http://xmlns.jcp.org/xml/ns/persistence/orm_2_1.xsd">
			<named-query name="findItems">
				<query><![CDATA[select i from Item i]]></query>
			</named-query>
		</entity-mappings>

!!!!
	You should wrap the query text into a CDATA instruction so any characters in your query string that may accidentally
	be considered XML (such as the less than operator) don’t confuse the XML parser. We omit CDATA from most other
	examples for clarity.

	Named queries don’t have to be written in JPQL. They may even be native SQL queries—and your Java code doesn’t
	need to know the difference:

		<named-native-query name="findItemsSQL" result-class="org.jpwh.model.querying.Item">
			<query>select * from ITEM</query>
		</named-native-query>

	This is useful if you think you may want to optimize your queries later by fine-tuning the SQL. It’s also a good
	solution if you have to port a legacy application to JPA/Hibernate, where SQL code can be isolated from the
	hand-coded JDBC routines. With named queries, you can easily port the queries one by one to mapping files.


Defining queries with annotations

	If you don’t like XML files, you can bundle and name your queries in Java annotation metadata.

	JPA supports named queries with the @NamedQuery and @NamedNativeQuery annotations. You can only place these
	annotations on a mapped class. Note that the query name must again be globally unique in all cases; no class or
	package name is automatically prefixed to the query name:

		@NamedQueries({
			 @NamedQuery(
				name = "findItemById",
				query = "select i from Item i where i.id = :id"
			 )
		})
		@Entity
		public class Item {
			 // ...
		}

	The class is annotated with an @NamedQueries containing an array of @NamedQuery. A single query can be declared
	directly; you don’t need to wrap it in @NamedQueries. If you have an SQL instead of a JPQL query, use the
	@NamedNativeQuery annotation.

	Unfortunately, JPA’s named query annotations only work when they’re on a mapped class. You can’t put them into a
	package-info.java metadata file. Hibernate has some proprietary annotations for that purpose:

		@org.hibernate.annotations.NamedQueries({
			 @org.hibernate.annotations.NamedQuery(
				 name = "findItemsOrderByName",
				 query = "select i from Item i order by i.name asc"
			 )
		})
		package org.jpwh.model.querying;

	If neither XML files nor annotations seem to be the right place for defining your named queries, you might want to
	construct them programmatically.


Defining named queries programmatically

	You can “save” a Query as a named query with the EntityManagerFactory#addNamedQuery() method:

	Query findItemsQuery = em.createQuery("select i from Item i");
	em.getEntityManagerFactory().addNamedQuery("savedFindItemsQuery", findItemsQuery);
	Query query = em.createNamedQuery("savedFindItemsQuery");		// Later, with the same EntityManagerFactory

	This registers your query with the persistence unit, the EntityManagerFactory, and make it reusable as a named
	query. The saved Query doesn’t have to be a JPQL statement; you can also save a criteria or native SQL query.
	Typically, you register your queries once, on startup of your application.

!!!	
	We leave it to you to decide whether you want to use the named query feature. But we consider query strings in the
	application code (unless they’re in annotations) to be the second choice; you should always externalize query strings
	if possible. In practice, XML files are probably the most versatile option.


Setting a timeout

	You can control how long to let a query run by setting a timeout:
		
		Query query = em.createQuery(queryString).setHint("javax.persistence.query.timeout", 60000);		// 1 minute

	With Hibernate, this method has the same semantics and consequences as the setQueryTimeout() method on the
	JDBC Statement API.


Setting the flush mode

	Let’s assume that you make modifications to persistent entity instances before executing a query. For example,
	you modify the name of managed Item instances. These modifications are only present in memory, so Hibernate by
	default flushes the persistence context and all changes to the database before executing your query. This
	guarantees that the query runs on current data and that no conflict between the query result and the in-memory
	instances can occur.

	You can disable flushing of the persistence context before a query with the org.hibernate.flushMode hint on a
	Query and the value org.hibernate.FlushMode.COMMIT. Fortunately, JPA has a standard setFlushMode() method on the
	EntityManager and Query API, and FlushModeType.COMMIT is also standardized. So, if you want to disable flushing
	only before a particular query, use the standard API:
		Query query = em.createQuery(queryString).setFlushMode(FlushModeType.COMMIT);
	
	With the flush mode set to COMMIT, Hibernate won’t flush the persistence context before executing the query.
	The default is AUTO.


Setting read-only mode

	You can tell Hibernate that it should consider all entity instances returned by a query as read-only (although
	not detached) with a hint:

		Query query = em.createQuery(queryString).setHint(org.hibernate.annotations.QueryHints.READ_ONLY,true);
		
	All Item instances returned by this query are in persistent state, but Hibernate doesn’t enable snapshot for
	automatic dirty checking in the persistence context. Hibernate doesn’t persist any modifications automatically
	unless you disable read-only mode with session.setReadOnly(item, false).


Setting a fetch size

	The fetch size is an optimization hint for the database driver. This hint may not result in any performance
	improvement if the driver doesn’t implement this functionality. If it does, it can improve the communication
	between the JDBC client and the database by retrieving many rows in one batch when the client (Hibernate) operates
	on a query result (that is, on a ResultSet).

		Query query = em.createQuery(queryString).setHint(org.hibernate.annotations.QueryHints.FETCH_SIZE,50);


Setting an SQL comment

!!!!
	When you optimize an application, you often have to read complex SQL logs. We highly recommend that you enable
	the property hibernate.use_sql_comments in your persistence.xml configuration. Hibernate will then add an
	auto-generated comment to each SQL statement it writes to the logs.

		Query query = em.createQuery(queryString).setHint(org.hibernate.annotations.QueryHints.COMMENT,"Custom SQL comment");

	The hints you’ve been setting so far are all related to Hibernate or JDBC handling. Many developers (and DBAs)
	consider a query hint to be something completely different. In SQL, a query hint is an instruction in the SQL
	statement for the optimizer of the DBMS. For example, if the developer or DBA thinks the execution plan selected
	by the database optimizer for a particular SQL statement isn’t the fastest, they use a hint to force a different
	execution plan. Hibernate and Java Persistence don’t support arbitrary SQL hints with an API; you’ll have to fall
	back to native SQL and write your own SQL statement—you can of course execute that statement with the provided APIs.

	In addition, of course, hints can be set on named queries externalized into a Hibernate XML metadata file:
		<?xml version="1.0"?>
		<hibernate-mapping xmlns="http://www.hibernate.org/xsd/orm/hbm">
		 <query name="findItemsOrderByAuctionEndHibernateWithHints"
			 cache-mode="ignore"
			 comment="Custom SQL comment"
			 fetch-size="50"
			 read-only="true"
			 timeout="60">
			 select i from Item i order by i.auctionEnd asc
		 </query>
		</hibernate-mapping>


Summary

	 You learned how to create and execute queries. To create queries, you use the JPA query interfaces and process
	typed query results. You also saw Hibernate’s own query interfaces.
	 You then looked at how to prepare queries, taking care to protect against SQL injection attacks. You learned how
	to use bound and positional parameters, and you paged through large result sets.
	 Instead of embedding JPQL in Java sources, you can name and externalize queries. You saw how to call a named query
	and different ways to define queries: in XML metadata, with annotations, and programmatically.
	 We discussed the query hints you can give Hibernate: setting a timeout, flush mode, read-only mode, fetch size,
	and SQL comment. As with JPQL, you saw how to name and externalize query hints.




The query languages
	
	Queries are the most interesting part of writing good data access code. A complex query may require a long time to
	get right, and its impact on the performance of the application can be tremendous. On the other hand, writing
	queries becomes much easier with more experience, and what seemed difficult at first is only a matter of knowing
	the available query languages
	
	This chapter covers the query languages available in JPA: JPQL and the criteria query API. We always show the same
	query example with both languages/API, where the result of the queries is equivalent.

	We expect that you won’t read this chapter just once but will rely on it as a reference to look up the correct
	syntax for a particular query when coding your application. Hence, our writing style is less verbose, with many
	small code examples for different use cases.

!!!
	Let’s start with some query terminology. You apply SELECTION to define where the data should be retrieved from,
	RESTRICTION to match records to a given criteria, and PROJECTION to select the data you want returned from a query.
	You’ll find this chapter organized in this manner.

	When we talk about queries in this chapter, we usually mean SELECT statements: operations that retrieve data from the database. JPA also supports UPDATE, DELETE, and even INSERT ... SELECT statements in JPQL, criteria, and SQL flavors, which we’ll discuss in section 20.1. We won’t repeat those bulk operations here and will focus on SELECT statements. We start with some basic selection examples.


Selection

	First, when we say selection, we don’t mean the SELECT clause of a query. We aren’t talking about the SELECT
	statement as such, either. We are referring to selecting a relation variable—or, in SQL terms, the FROM clause.
	It declares where data for your query should come from: simplifying, which tables you “select” for a query. Alternatively,
	with classes instead of table names in JPQL:

		from Item
	
	The following query (just a FROM clause) retrieves all Item entity instances. Hibernate generates the following SQL:
		
		select i.ID, i.NAME, ... from ITEM i

	The equivalent criteria query can be built with the from() method, passing in the entity name:
	
		CriteriaQuery criteria = cb.createQuery(Item.class);
		criteria.from(Item.class);


Assigning aliases and query roots

	Adding a SELECT clause to a JPQL query requires assignment of an alias to the queried class in the FROM clause,
	such that you can reference it in other parts of the query:
		
		select i from Item as i
	
	The following query is now JPA-compliant. The as keyword is always optional. The following is equivalent: 
		
		select i from Item i

	You assign the alias i to queried instances of the Item class. Think of this as being a bit like the temporary
	variable declaration in the following Java code:
		
		for(Iterator i = result.iterator(); i.hasNext();) {
			Item item = (Item) i.next();
			// ...
		}


Polymorphic queries

	JPQL, as an object-oriented query language, supports polymorphic queries—queries for instances of a class and all
	instances of its subclasses, respectively. Consider the following queries:
		select bd from BillingDetails bd
		criteria.select(criteria.from(BillingDetails.class));

	These queries return all instances of the type BillingDetails, which is an abstract class. In this case, each
	instance is of a subtype of BillingDetails: CreditCard or BankAccount. If you want only instances of a particular
	 subclass, you may use this:
		select cc from CreditCard cc
		criteria.select(criteria.from(CreditCard.class));

	The class named in the FROM clause doesn’t even need to be a mapped persistent class; any class will do. The following
	query returns all persistent objects:
		select o from java.lang.Object o

	Yes, you can select all the tables of your database with such a query and retrieve all data into memory! This also
	works for arbitrary interfaces—for example, selecting all serializable types:
		select s from java.io.Serializable 

	The bad news is that JPA doesn’t standardize polymorphic JPQL queries with arbitrary interfaces. They work in
	Hibernate, but portable applications should only reference mapped entity classes in the FROM clause
	(such as BillingDetails or CreditCard). The from() method in the criteria query API only accepts mapped entity
	types


Restriction

	You’re now done with the first step of writing a query, the selection. You picked the tables from which to
	query data. Next, you’d probably like to limit the rows you want to retrieve with a restriction

	Usually, you don’t want to retrieve all instances of a class from the database. You must be able to express
	constraints on the data returned by the query. We call this restriction.
	The WHERE clause declares restriction conditions in SQL and JPQL, and the where() method is the equivalent in
	the criteria query API.  This is a typical WHERE clause that restricts the results to all Item instances with a
	given name:
		
		select i from Item i where i.name = 'Foo'
		
		CriteriaBuilder cb = em.getCriteriaBuilder();
		Root<Item> i = criteria.from(Item.class);
		criteria.select(i).where(cb.equal(i.get("name"), "Foo"));

	The query expresses the constraint in terms of a property, name, of the Item class. The SQL generated by these
	queries is :
		select i.ID, i.NAME, ... from ITEM i where i.NAME = 'Foo'

	Other common literals in JPQL are true and false:
		select u from User u where u.activated = true
		Root<User> u = criteria.from(User.class);
		criteria.select(u).where(cb.equal(u.get("activated"), true));

	SQL (JPQL and criteria queries) expresses restrictions with ternary logic. The WHERE clause is a logical
	expression that evaluates to true, false, or null.

	What is ternary logic?
	A row is included in an SQL query result if and only if the WHERE clause evaluates to true. In Java,
	nonNullObject == null evaluates to false, and null == null evaluates to true. In SQL,
	NOT_NULL_COLUMN = null and null = null both evaluate to null, not true. Thus, SQL needs special operators,
	IS NULL and IS NOT NULL, to test whether a value is null.
	Ternary logic is a way of handling expressions you may apply to nullable column values. Treating null not as a
	special marker but as a regular value is an SQL extension of the familiar binary logic of the relational model.
	Hibernate has to support this ternary logic with ternary operators in JPQL and criteria queries.


Comparison expressions

	JPQL and the criteria API support the same basic comparison operators as SQL. Here are a few examples that
	should look familiar if you know SQL.

		select b from Bid b where b.amount between 99 and 110

		Root<Bid> b = criteria.from(Bid.class);
		criteria.select(b).where(
			cb.between(
				b.<BigDecimal>get("amount"),
				new BigDecimal("99"), new BigDecimal("110")
			)
		);


		select i from Item i where i.buyNowPrice is null
		
		Root<Item> i = criteria.from(Item.class);
		criteria.select(i).where(
			cb.isNull(i.get("buyNowPrice"))
		);

	
	The LIKE operator allows wildcard searches, where the wildcard symbols are % and _, as in SQL: The expression
	john% restricts the result to users with a username starting with “john”
		
		select u from User u where u.username like 'john%'
		
		Root<User> u = criteria.from(User.class);
		criteria.select(u).where(
			cb.like(u.<String>get("username"), "john%")
		);

	The percentage symbol stands for any sequence of characters; the underscore can be used to wildcard
	a single character.

	We summarize all operators, including some we haven’t shown so far, and their precedence from top to bottom,
	in table 15.1.


Expressions with collections

	All expressions in the previous sections had only single-valued path expressions: user.username,
	item.buyNowPrice, and so on. You can also write path expressions that end in collections, and apply some
	operators and functions.  For example, let’s assume you want to restrict your query result to Category
	instances that have an element in their items collection:

		select c from Category c where c.items is not empty
		
		Root<Category> c = criteria.from(Category.class);
		criteria.select(c).where(cb.isNotEmpty(c.<Collection>get("items")));


Calling functions

	An extremely powerful feature of the query languages is the ability to call functions in the WHERE clause.
	The following queries call the lower() function for case-insensitive searching:
		
		select i from Item i where lower(i.name) like 'ba%'
		
		Root<Item> i = criteria.from(Item.class);
		criteria.select(i).where(cb.like(cb.lower(i.<String>get("name")), "ba%"));

	Look at the summary of all available functions in table 15.2. For criteria queries, the equivalent methods are
	in CriteriaBuilder, with slightly different name formatting (using camelCase and no underscores).


Ordering query results

	All query languages provide some mechanism for ordering query results. JPQL provides an ORDER BY clause,
	similar to SQL.
	The following query returns all users, ordered by username, ascending by default:
		select u from User u order by u.username

	You specify ascending or descending order with the asc and desc keywords:
		select u from User u order by u.username desc

	With the criteria query API, you must specify ascending and descending order with asc() or desc():
		Root<User> u = criteria.from(User.class);
		criteria.select(u).orderBy(cb.desc(u.get("username")));

	You may order by multiple properties:
		select u from User u order by u.activated desc, u.username asc


Projection

!!!
	In simple terms, selection and restriction in a query is the process of declaring which tables and rows
	you want to query. Projection is defining the “columns” you want returned to the application: the data you need.
	 The SELECT clause in JPQL performs projections.


Projection of entities and scalar values

	For example, consider the following queries:
		select i, b from Item i, Bid b
		
		Root<Item> i = criteria.from(Item.class);
		Root<Bid> b = criteria.from(Bid.class);
		criteria.select(cb.tuple(i, b));

	As promised earlier, this criteria query shows how you can add several Roots by calling the from() method several
	times. To add several elements to your projection, either call the tuple() method of CriteriaBuilder, or the
	shortcut multiselect().
	You’re creating a Cartesian product of all Item and Bid instances. 

	The queries return ordered pairs of Item and Bid entity instances. The query returns a List of Object[]. At
	index 0 is the Item, and at index 1 is the Bid:
		List<Object[]> result = query.getResultList();
		Set<Item> items = new HashSet();
		Set<Bid> bids = new HashSet();
		for (Object[] row : result) {
			 assertTrue(row[0] instanceof Item);
			 items.add((Item) row[0]);
			 assertTrue(row[1] instanceof Bid);
			 bids.add((Bid)row[1]);
		}

	Because this is a product, the result contains every possible combination of Item and Bid rows found in the
	two underlying tables. Obviously, this query isn’t useful, but you shouldn’t be surprised to receive a
	collection of Object[] as a query result. Hibernate manages all Item and Bid entity instances in persistent
	state, in the persistence context. Note how the HashSets filter out duplicate Item and Bid instances.

TODO Continue from here
Using dynamic instantiation

	Let’s say you have a reporting screen in your application where you need to show some data in a list. You want
	to show all auction items and when each auction ends. You don’t want to load managed Item entity instances,
	because no data will be modified: you only read data.
	First, write a class called ItemSummary with a constructor that takes a Long for the item’s identifier,
	a String for the item’s name, and a Date for the item’s auction end timestamp:
		public class ItemSummary {
			 public ItemSummary(Long itemId, String name, Date auctionEnd) {
			 // ...
			 }
			 // ...
		}

	We sometimes call these kinds of classes data transfer objects (DTOs), because their main purpose is to shuttle
	data around in the application. The ItemSummary class isn’t mapped to the database, and you can add arbitrary
	methods (getter, setter, printing of values) as needed by your reporting user interface.

	Hibernate can directly return instances of ItemSummary from a query with the new keyword in JPQL and the
	construct() method in criteria:

		select new org.jpwh.model.querying.ItemSummary(i.id, i.name, i.auctionEnd) from Item i

		Root<Item> i = criteria.from(Item.class);
		criteria.select(
			cb.construct(ItemSummary.class, i.get("id"), i.get("name"), i.get("auctionEnd"))
		);
	
	In the result list of this query, each element is an instance of ItemSummary. Note that in JPQL, you must use
	 a fully qualified class name, which means including the package name. Also note that nesting constructor
	 calls isn’t supported: you can’t write new ItemSummary(..., new UserSummary(...)).


Getting distinct results

	When you create a projection in a query, the elements of the result aren’t guaranteed to be unique. For example,
	item names aren’t unique, so the following query may return the same name more than once:
		select i.name from Item i
	
		CriteriaQuery<String> criteria = cb.createQuery(String.class);
		criteria.select(criteria.from(Item.class).<String>get("name"));

	It’s difficult to see how it could be meaningful to have two identical rows in a query result, so if you think
	duplicates are likely, you normally apply the DISTINCT keyword or distinct() method:
	
		CriteriaQuery<String> criteria = cb.createQuery(String.class);
		criteria.select(criteria.from(Item.class).<String>get("name"));
		criteria.distinct(true);
		
	The filtering occurs at the database level.


Grouping

	JPA standardizes several features of SQL that are most commonly used for reporting—although they’re also used for
	other things. In reporting queries, you write the SELECT
	clause for projection and the GROUP BY and HAVING clauses for aggregation.

	Just like in SQL, any property or alias that appears outside of an aggregate function in the SELECT clause must
	also appear in the GROUP BY clause. Consider the next query, which counts the number of users with each last name:

		select u.lastname, count(u) from User u group by u.lastname
		
		Root<User> u = criteria.from(User.class);
		criteria.multiselect(u.get("lastname"), cb.count(u));
		criteria.groupBy(u.get("lastname"));

	Sometimes you want to restrict the result further by selecting only particular values of a group. Use the WHERE
	clause to perform the relational operation of restriction on rows. The HAVING clause performs restriction upon
	groups.  For example, the next query counts users with each last name that begins with “D”:

		select u.lastname, count(u) from User u group by u.lastname having u.lastname like 'D%'

		Root<User> u = criteria.from(User.class);
		criteria.multiselect(u.get("lastname"), cb.count(u));
		criteria.groupBy(u.get("lastname"));
		criteria.having(cb.like(u.<String>get("lastname"), "D%"));



Joins
	
	It’s time to look at some more complex options. For many engineers, the most difficult to understand but also one
	of most powerful benefits of the relational model is the ability to join arbitrary data.
	
	Join operations combine data in two (or more) relations. Joining data in a query also enables you to fetch
	several associated instances and collections in a single query: for example, to load an Item and all its bids in
	one round trip to the database. We now show you how basic join operations work and how to use them to write such dynamic
	fetching strategies. Let’s first look at how joins work in SQL queries, without JPA.


Joins with SQL

	Let’s start with the example we already mentioned: joining the data in the ITEM and BID tables, as shown in
	figure 15.1. The database contains three items: the first has three bids, the second has one bid, and the third
	has no bids. Note that we don’t show all columns; hence the dotted lines.

!!!
	What most people think of when they hear the word join in the context of SQL databases is an inner join. An inner
	join is the most important of several types of joins and the easiest to understand. Consider the SQL statement and
	result in figure 15.2. This SQL statement contains an ANSI-style inner join in the FROM clause.

		select i.*, b.* from ITEM i inner join BID b on i.ID = b.ITEM_ID
		
	If you join the ITEM and BID tables with an inner join, with the condition that the ID of an ITEM row must match
	the ITEM_ID value of a BID row, you get items combined with their bids in the result. Note that the result of this
	operation contains only items that have bids
	
	You can think of a join as working as follows: first you take a product of the two tables, by taking all possible
	combinations of ITEM rows with BID rows. Second, you filter these combined rows with a join condition: the expression
	in the ON clause. (Any good database engine has much more sophisticated algorithms to evaluate a join; it
	usually doesn’t build a memory-consuming product and then filter out rows.)

	It’s crucial to understand that the join condition can be any expression that evaluates to true. You can join data
	in arbitrary ways; you aren’t limited to comparisons of identifier values. For example, the join condition on
	i.ID = b.ITEM_ID and b.AMOUNT > 100 would only include rows from the BID table that also have an AMOUNT greater
	than 100. 

	If you want all items, not just the ones which have related bids, and NULL instead of bid data when there is no
	corresponding bid, then you write a (left) outer join, as shown in figure 15.3.

		select i.*, b.* from ITEM i left outer join BID b on i.ID = b.ITEM_ID

!!!!!
	In case of the left outer join, each row in the (left) ITEM table that never satisfies the join condition is
	also included in the result, with NULL returned for all columns of BID.

!!!
	Right outer joins are rarely used; developers always think from left to right and put the “driving” table
	of a join operation first. In figure 15.4, you can see the same result with BID instead of ITEM as the
	driving table, and a right outer join.
	
		select b.*, i.* from BID b right outer join ITEM i on b.ITEM_ID = i.ID


Join options in JPA	
	
	We now discuss JPA join options. Remember that Hibernate eventually translates all queries into SQL, so even if the
	syntax is slightly different, you should always refer to the illustrations shown in this section and verify that
	you understand what the resulting SQL and result set looks like.
	
	JPA provides four ways of expressing (inner and outer) joins in queries:
		 An implicit association join with path expressions
		 An ordinary join in the FROM clause with the join operator
		 A fetch join in the FROM clause with the join operator and the fetch keyword for eager fetching
		 A theta-style join in the WHERE clause


Implicit association joins
	
	In JPA queries, you don’t have to specify a join condition explicitly. Rather, you specify the name of a mapped
	Java class association. This is the same feature we’d prefer to have in SQL: a join condition expressed with a
	foreign key constraint name. Because you’ve mapped most, if not all, foreign key relationships of your database
	schema, you can use the names of these mapped associations in the query language. This is syntactical sugar,
	but it’s convenient.
	
	For example, the Bid entity class has a mapped many-to-one association named item, with the Item entity class.
	If you refer to this association in a query, Hibernate
	has enough information to deduce the join expression with a key column comparison. This helps make queries less
	verbose and more readable.
	
	Earlier in this chapter, we showed you property path expressions, using dot-notation: single-valued path
	expressions such as user.homeAddress.zipcode and collectionvalued path expressions such as item.bids. You can
	create a path expression in an implicit inner join query:
	
		select b from Bid b where b.item.name like 'Fo%'

		Root<Bid> b = criteria.from(Bid.class);
		criteria.select(b).where(
			cb.like(
				b.get("item").<String>get("name"),
				"Fo%"
			)
		);
!!!
	The path b.item.name creates an implicit join on the many-to-one associations from Bid to Item—the name of
	this association is item. Hibernate knows that you mapped this association with the ITEM_ID foreign key in the
	BID table and generates the SQL join condition accordingly. Implicit joins are always directed along many-to-one
	or one-to-one associations, never through a collection-valued association (you can’t write item.bids.amount).

	Multiple joins are possible in a single path expression. This query joins rows from the BID, the ITEM, and
	the USER tables:
	
		select b from Bid b where b.item.seller.username = 'johndoe'

		Root<Bid> b = criteria.from(Bid.class);
		criteria.select(b).where(
			cb.equal(
				b.get("item").get("seller").get("username"),
				"johndoe"
			)
		);
	
	SQL joins are important, and especially when optimizing queries, you need to be able to see at a glance exactly
	how many of them there are. Consider the following query:

		select b from Bid b where b.item.seller.username = 'johndoe' and b.item.buyNowPrice is not null

		Root<Bid> b = criteria.from(Bid.class);
		criteria.select(b).where(
			cb.and(
				cb.equal(
					b.get("item").get("seller").get("username"),
					"johndoe"
				),
				cb.isNotNull(b.get("item").get("buyNowPrice"))
			)
		);
	
	How many joins are required to express such a query in SQL? Even if you get the answer right, it takes more
	than a few seconds to figure out. The answer is two. The generated SQL looks something like this:
	
		select b.*
		 from BID b
			 inner join ITEM i on b.ITEM_ID = i.ID
			 inner join USER u on i.SELLER_ID = u.ID
			 where u.USERNAME = 'johndoe'
				and i.BUYNOWPRICE is not null;


Explicit joins

	You may want to limit the items returned by the query based on some criterion to apply to their bids. For
	example, you may want all items that have a bid of more than 100, which requires an inner join. Here, you
	aren’t interested in items that have no bids
	
	If you want to retrieve Item instances and restrict the result to items that have bids with a certain amount,
	you have to assign an alias to a joined association. Then you refer to the alias in a WHERE clause to restrict
	the data you want:
	
		select i from Item i join i.bids b where b.amount > 100
		
		Root<Item> i = criteria.from(Item.class);
		Join<Item, Bid> b = i.join("bids");
		criteria.select(i).where(
			cb.gt(b.<BigDecimal>get("amount"), new BigDecimal(100))
		);
	
	For example, the following query and retrieves items that have no bids, and items with bids of a minimum bid
	amount:
	
		select i, b from Item i left join i.bids b on b.amount > 100
		
		Root<Item> i = criteria.from(Item.class);
		Join<Item, Bid> b = i.join("bids", JoinType.LEFT);
		b.on( cb.gt(b.<BigDecimal>get("amount"), new BigDecimal(100)) );
		criteria.multiselect(i, b);
	
	The first thing that is new in this query is the LEFT keyword and JoinType.LEFT in the criteria query. Optionally
	you can write LEFT OUTER JOIN and RIGHT OUTER JOIN in JPQL, but we usually prefer the short form.


Dynamic fetching with joins

	All the queries you saw in the previous sections have one thing in common: the returned Item instances have a
	collection named bids. This @OneToMany collection, if mapped as FetchType.LAZY (the default for collections),
	isn’t initialized, and an additional SQL statement is triggered as soon as you access it. The same is true for
	all single-valued associations, like the @ManyToOne association seller of each Item. By
	default, Hibernate generates a proxy and loads the associated User instance lazily and only on demand.
	
	What options do you have to change this behavior? First, you can change the fetch plan in your mapping metadata and
	declare a collection or single-valued association as FetchType.EAGER. Hibernate then executes the necessary SQL
	to guarantee that the desired network of instances is loaded at all times. This also means a single JPA query may
	result in several SQL operations!
	As an example, the simple query "select i from Item i" may trigger additional SQL statements to load the bids of
	each Item, the seller of each Item, and so on.
	
	Eager fetching of associated data is possible with the FETCH keyword in JPQL and the fetch() method in the criteria query API:
	
		select i from Item i left join fetch i.bids
		
		Root<Item> i = criteria.from(Item.class);
		i.fetch("bids", JoinType.LEFT);
		criteria.select(i);
	
	
	You can also prefetch many-to-one or one-to-one associations with the same syntax:
		select distinct i from Item i
		 left join fetch i.bids b
		 join fetch b.bidder
		 left join fetch i.seller

		Root<Item> i = criteria.from(Item.class);
		Fetch<Item, Bid> b = i.fetch("bids", JoinType.LEFT);
		b.fetch("bidder");
		i.fetch("seller", JoinType.LEFT);
		criteria.select(i).distinct(true);


Theta-style joins

	In traditional SQL, a theta-style join is a Cartesian product together with a join condition in the WHERE clause,
	which is applied on the product to restrict the result. In JPA queries, the theta-style syntax is useful when your
	join condition isn’t a foreign key relationship mapped to a class association.
	
	You can then find all the Users and their LogRecords with the following theta-style join:
		select u, log from User u, LogRecord log where u.username = log.username
		
		Root<User> u = criteria.from(User.class);
		Root<LogRecord> log = criteria.from(LogRecord.class);
		criteria.where(
		 cb.equal(u.get("username"), log.get("username")));
		criteria.multiselect(u, log);


Subselects

	Subselects are an important and powerful feature of SQL. A subselect is a select query embedded in another query,
	usually in the SELECT, FROM, or WHERE clause.
	JPA supports subqueries in the WHERE clause. Subselects in the FROM clause aren’t supported because the query
	languages doesn’t have transitive closure. The result of a query may not be usable for further selection in a
	FROM clause. The query language also doesn’t support subselects in the SELECT clause, but you map can
	subselects to derived properties with @org.hibernate.annotations.Formula, as shown in section 5.1.3.
	Subselects can be either correlated with the rest of the query or uncorrelated.
	
	
Quantification

	The following quantifiers are standardized:
	 ALL—The expression evaluates to true if the comparison is true for all values in the result of the subquery.
	It evaluates to false if a single value of the subquery result fails the comparison test.
	 ANY—The expression evaluates to true if the comparison is true for some (any) value in the result of the
	subquery. If the subquery result is empty or no value satisfies the comparison, it evaluates to false.
	The keyword SOME is a synonym for ANY.
	 EXISTS—Evaluates to true if the result of the subquery consists of one or more values.	
	
	For example, the following query returns items where all bids are less or equal than 10:
	
		select i from Item i
			where 10 >= all (
				select b.amount from i.bids b
			)
	
		Root<Item> i = criteria.from(Item.class);
		Subquery<BigDecimal> sq = criteria.subquery(BigDecimal.class);
		Root<Bid> b = sq.from(Bid.class);
		
		sq.select(b.<BigDecimal>get("amount"));
		sq.where(cb.equal(b.get("item"), i));
		
		criteria.select(i);
		criteria.where(
			cb.greaterThanOrEqualTo(
				cb.literal(new BigDecimal(10)),
				cb.all(sq)
			)
		);	
	
	To retrieve all items that have bids, check the result of the subquery with EXISTS:
		select i from Item i
		 where exists (
			select b from Bid b where b.item = i
		 )
		 
		Root<Item> i = criteria.from(Item.class);
		Subquery<Bid> sq = criteria.subquery(Bid.class);
		Root<Bid> b = sq.from(Bid.class);
		sq.select(b).where(cb.equal(b.get("item"), i));
		criteria.select(i);
		criteria.where(cb.exists(sq));


Summary
	 If you knew SQL coming into this chapter, you’re now able to write a wide variety of queries in JPQL and with
	the criteria query API. If you aren’t comfortable with SQL, consult our reference section.
	 With selection, you pick the source(s) of your query: the “tables” you want to query. Then you apply restriction
	expressions to limit the queried “rows” to the relevant subset. The projection of your query defines the returned “columns”:
	the data retrieved by your query. You can also direct the database to aggregate and group data efficiently, before returning it.
	 We discussed joins: how you select, restrict, and combine data from several tables. A JPA application relies on
	joins to fetch associated entity instances and collections in a single database round-trip. This is a critical
	feature when you’re trying to reduce database load, and we recommend you repeat these examples to get a firm grasp
	of joining data and eager fetching.
	 You can nest queries inside each other as subselects.	



Advanced query options	
	
Transforming query results
	You can apply a result transformer to a query result so that you can filter or marshal the result with your
	 own procedure instead of the Hibernate default behavior. Hibernate’s default behavior provides a set of default
	 transformers that you can replace and/or customize.
	
	The result you’re going to transform is that of a simple query, but you need to access the native Hibernate
	API org.hibernate.Query through the Session, as shown in the following listing.

		Session session = em.unwrap(Session.class);
		org.hibernate.Query query = session.createQuery(
			 "select i.id as itemId, i.name as name, i.auctionEnd as auctionEnd from
			Item i"
		);

	Without any custom result transformation, this query returns a List of Object[]:
	
		List<Object[]> result = query.list();
		for (Object[] tuple : result) {
			 Long itemId = (Long) tuple[0];
			 String name = (String) tuple[1];
			 Date auctionEnd = (Date) tuple[2];
			 // ...
		}
	
	Each object array is a “row” of the query result. Each element of that tuple can be accessed by index: here
	index 0 is a Long, index 1 a String, and index 2 a Date.


Returning a list of lists

	Let’s say you want to use indexed access but are unhappy with the Object[] result. Instead of a list of
	 Object[]s, each tuple can also be represented as a List, using the ToListResultTransformer:

    //    @Deprecated method...setResultTransformer
		query.setResultTransformer(
			ToListResultTransformer.INSTANCE
		);
		
		List<List> result = query.list();
		for (List list : result) {
			 Long itemId = (Long) list.get(0);
			 String name = (String) list.get(1);
			 Date auctionEnd = (Date) list.get(2);
			 // ...
		}

	This is a minor difference but a convenient alternative if code in other layers of your application already
	works with lists of lists.


Returning a list of maps

	The next transformer converts the query result to a Map for each tuple, where the query projection assigns
	aliases mapped to the projection elements.

	The AliasToEntityMapResultTransformer returns a List of java.util.Map, one map per “row.” The aliases in the
	query are itemId, name, and auctionEnd:

		query.setResultTransformer(
			AliasToEntityMapResultTransformer.INSTANCE
		);
		List<Map> result = query.list();
		assertEquals(
			 query.getReturnAliases(),
			 new String[]{"itemId", "name", "auctionEnd"}
		);
		for (Map map : result) {
			 Long itemId = (Long) map.get("itemId");
			 String name = (String) map.get("name");
			 Date auctionEnd = (Date) map.get("auctionEnd");
			 // ...
		}


Mapping aliases to bean properties

	In section 15.3.2, we showed how a query can return instances of a JavaBean dynamically by calling the
	ItemSummary constructor. In JPQL, you achieve this with the new operator. For criteria queries, you use the
	construct() method. The ItemSummary class must have a constructor that matches the projected query result.

	Alternatively, if your JavaBean doesn’t have the right constructor, you can still instantiate and populate its
	values through setters and/or fields with the AliasToBeanResultTransformer. The following example transforms
	 the query result shown in listing 16.1:

		query.setResultTransformer(
			new AliasToBeanResultTransformer(ItemSummary.class)
		);
		List<ItemSummary> result = query.list();
		for (ItemSummary itemSummary : result) {
			Long itemId = itemSummary.getItemId();
			String name = itemSummary.getName();
			Date auctionEnd = itemSummary.getAuctionEnd();
			// ...
		}
	
	You create the transformer with the JavaBean class you want to instantiate, here ItemSummary. Hibernate
	requires that this class either has no constructor or a public noargument constructor.

	When transforming the query result, Hibernate looks for setter methods and fields with the same names as the
	aliases in the query. The ItemSummary class must either have the fields itemId, name, andauctionEnd, or
	the setter methods setItemId(), setName(), and setAuctionEnd(). The fields or setter method parameters
	must be of the right type. If you have fields that map to some query aliases and setter methods for
	the rest, that’s fine too. 


Writing a ResultTransformer

	The built-in transformers in Hibernate aren’t sophisticated; there isn’t much difference between result
	tuples represented as lists, maps, or object arrays. Implementing the ResultTransformer interface is trivial,
	though, and custom conversion of query results can tighten the integration between the layers of code in your
	application. If your user interface code already knows how to render a table of List<ItemSummary>, let
	Hibernate return it directly from a query

	The ResultTransformer interface requires that you implement the methods transformTuple() and transformList():

	query.setResultTransformer(
	 new ResultTransformer() {
		 @Override
		 public Object transformTuple(Object[] tuple, String[] aliases) {
			 Long itemId = (Long) tuple[0];
			 String name = (String) tuple[1];
			 Date auctionEnd = (Date) tuple[2];
			 
			 assertEquals(aliases[0], "itemId");						// Access query aliases if needed
			 assertEquals(aliases[1], "name");
			 assertEquals(aliases[2], "auctionEnd");
			 
			 return ItemSummaryFactory.newItemSummary(
				itemId, name, auctionEnd
			 );
		 }
		 
		 @Override
		 public List transformList(List collection) {				// Modifies result list
			return Collections.unmodifiableList(collection);
		 }
	 }
	);

	For each result “row,” an Object[] tuple must be transformed into the desired result value for that row.
	Here you access each projection element by index in the tuple array and then call the ItemSummaryFactory
	to produce the query result value. Hibernate passes the method the aliases found in the query, for each
	tuple element. You don’t need the aliases in this transformer, though.

	You can wrap or modify the result list after transforming the tuples. Here you make the returned List
	unmodifiable: ideal for a reporting screen where nothing should change the data.


Filtering collections

	In chapter 7, you saw reasons you should (or rather, shouldn’t) map a collection in your Java domain model.
	The main benefit of a collection mapping is easier access to data: you can call item.getImages() or
	item.getBids() to access all images and bids associated with an Item. You don’t have to write a JPQL or
	criteria query; Hibernate will execute the query for you when you start iterating through the collection elements.

	The most obvious problem with this automatic data access is that Hibernate will always write the same query,
	retrieving all images or bids for an Item. You can customize the order of collection elements, but even that
	is a static mapping. What would you do to render two lists of bids for an Item, in ascending and descending
	order by creation date? You could go back to writing and executing custom queries and not call item.getBids();
	the collection mapping might not even be necessary

	Instead, you can use a Hibernate proprietary feature, collection filters, that makes writing these queries
	easier, using the mapped collection. Let’s say you have a persistent Item instance in memory, probably
	loaded with the EntityManager API. You want to list all bids made for this Item but further restrict the
	result to bids made by a particular User. You also want the list sorted in descending order by Bid#amount.

		Item item = em.find(Item.class, ITEM_ID);
		User user = em.find(User.class, USER_ID);

!!
// @Deprecated....method createFilter is deprecated...  apparently this was replaced with the @Filter annotation

		org.hibernate.Query query = session.createFilter(
			 item.getBids(),
			 "where this.bidder = :bidder order by this.amount desc"
		);
		query.setParameter("bidder", user);
		List<Bid> bids = query.list();

	The session.createFilter() method accepts a persistent collection and a JPQL query fragment. This query fragment
	doesn’t require a select or from clause; here it only has a restriction with the where clause and an order
	by clause.


------------------------------------------------------------------------------------------------------------------------------------
							QUICK READ
							QUICK READ
							QUICK READ
							QUICK READ
							QUICK READ
-----------------------------------------------------------------------------------------------------------------------------------


@Deprecated !!!! this is not used anymore
The Hibernate criteria query API

	Using the org.hibernate.Criteria and org.hibernate.Example interfaces, you can build queries programmatically
	by creating and combining org.hibernate.criterion.* instances. You see how to use these APIs and how to
	express selection, restriction, joins, and projection

Selection and ordering
Restriction
Projection and aggregation
Joins
Subselects

Summary
	 You used the ResultTransformer API to write custom code to process a query result, returning a list of lists
	and a list of maps, and mapping aliases to bean properties.
	 We covered Hibernate’s collection-filtering interfaces as well as making better use of mapped persistent
	collections.
	 You explored the older Hibernate Criteria query facility and when you might use it instead of the standardized
	 criteria queries in JPA. We covered all the relational and Hibernate goodies using this API: selection and
	 ordering, restriction, projection and aggregation, joins, subselects, and example queries


Customizing SQL

	In this chapter, we cover customizing and embedding SQL in a Hibernate application. SQL was created in the
	1970s, but ANSI didn’t standardized it until 1986. Although each update of the SQL standard has seen new (and
	many controversial) features, every DBMS product that supports SQL does so in its own unique way. The burden of
	portability is again on the database application developers. This is where Hibernate helps: its built-in query
	languages produce SQL that depends on the configured database dialect. Dialects also help produce all other
	automatically generated SQL (for example, when Hibernate has to retrieve a collection on demand). With a simple
	dialect switch, you can run your application on a different DBMS.
	Hibernate generates all SQL statements for you, for all create, read, update, and delete (CRUD) operations.
	Sometimes, though, you need more control than Hibernate and the Java Persistence API provide: you need to work
	at a lower level of abstraction. With Hibernate, you can write your own SQL statements:

	Fall back to the JDBC API, and work directly with the Connection, PreparedStatement, and ResultSet interfaces.
	Hibernate provides the Connection, so you don’t have to maintain a separate connection pool, and your SQL
	statements execute within the same (current) transaction.

	Write plain SQL SELECT statements, and either embed them within your Java code or externalize them (in XML files
	or annotations) as named queries. You execute these SQL queries with the Java Persistence API, just like a
	regular JPQL query. Hibernate can then transform the query result according to your mapping. This also works
	with stored procedure calls.

	You can also write your own Data Manipulation Language (DML) statements, such as UPDATE, INSERT, and DELETE. You
	might even call a stored procedure to perform a CRUD operation.


Falling back to JDBC

	Sometimes you want Hibernate to get out of the way and directly access the database through the JDBC API. To
	do so, you need a java.sql.Connection interface to write and execute your own PreparedStatement and direct
	access to your statement ResultSet. Because Hibernate already knows how to obtain and close database
	connections, it can provide your application with a Connection and release it when you’re done.


Customizing CRUD operations

	The first custom SQL you write loads an entity instance of the User class. All the following code examples
	show the same SQL that Hibernate executes automatically by default, without much customization—this helps
	you understand the mapping technique more quickly. You can customize retrieval of an entity instance with
	 a loader.


Enabling custom loaders

	Hibernate has two requirements when you override an SQL query to load an entity instance:
		 Write a named query that retrieves the entity instance. We show an example in SQL, but as always,
		 you can also write named queries in JPQL. For an SQL query, you may need a custom result mapping, as shown
		 earlier in this chapter.
		 Activate the query on an entity class with @org.hibernate.annotations.Loader. This enables the query
		as the replacement for the Hibernate-generated query.


Calling stored procedures
 
	Stored procedures are common in database application development. Moving code closer to the data and executing
	it inside the database has distinct advantages. You end up not duplicating functionality and logic in each
	program that accesses the data. A different point of view is that a lot of business logic shouldn’t be
	duplicated, so it can be applied all the time. This includes procedures that guarantee the integrity of
	the data: for example, constraints that are too complex to implement declaratively. You’ll usually also find
	triggers in a database that contain procedural integrity rules.

	Stored procedures have advantages for all processing on large amounts of data, such as reporting and
	statistical analysis. You should always try to avoid moving large data sets on your network and between
	your database and application servers, so a stored procedure is the natural choice for mass data operations.


Returning a result set

	You can create the following procedure in MySQL. It returns a result set containing all rows of the ITEM table:
		create procedure FIND_ITEMS()
		begin
		 select * from ITEM;
		end

		StoredProcedureQuery query = em.createStoredProcedureQuery("FIND_ITEMS", Item.class);
		List<Item> result = query.getResultList();
		for (Item item : result) {
			// ...
		}


Summary
	 You saw how to fall back to the JDBC API when necessary. Even for custom SQL queries, Hibernate can do the
	heavy lifting and transform the ResultSet into instances of your domain model classes, with flexible mapping
	options, including customizing result mapping. You can also externalize native queries for a cleaner setup.
	 We discussed how to override and provide your own SQL statements for regular create, read, update, and
	delete (CRUD) operations, as well as for collection operations.
	 You can enable custom loaders and use eager fetching in such loaders.
	 You learned how to call database stored procedures directly and integrate them into Hibernate. You explored
	processing a single result set as well as multiple result sets and update counts. You set stored procedure
	(input and output) parameters and learned how to return a database cursor. You also saw how to use stored
	procedures for CRUD.



============================================================================================
Part 5 - Building applications
============================================================================================	

	In part 5, the last part of this book, we’ll discuss the design and implementation of layered and
	conversation-aware Java database applications. We’ll discuss the most common design patterns used with
	Hibernate, such as the Data Access Object (DAO). You’ll see how you can test your Hibernate application
	easily and what other best practices are relevant if you work with ORM software in web and client/server
	applications in general.

	After reading this part, you’ll have gathered the architectural-level knowledge to put together an
	application and let it scale as it succeeds.


Designing client/server applications

	Most JPA developers build client/server applications with a Java-based server accessing the database
	tier through Hibernate. Knowing how the EntityManager and system transactions work, you could probably
	come up with your own server architecture. You’d have to figure out where to create the EntityManager,
	when and how to close it, and how to set transaction boundaries.

	You may be wondering what the relationship is between requests and responses from and to your client,
	and the persistence context and transactions on the server. Should a single system transaction handle
	each client request? Can several consecutive requests hold a persistence context open? How does detached
	entity state fit into this picture? Can you and should you serialize entity data between client and
	server? How will these decisions affect your client design?


Creating a persistence layer
 
	Although JPA already provides a certain level of abstraction, there are several reasons you should consider
	hiding JPA calls behind a facade:
		 A custom persistence layer can provide a higher level of abstraction for data access operations. Instead
		of basic CRUD and query operations as exposed by the EntityManager, you can expose higher-level operations,
		such as getMaximumBid(Item i) and findItems(User soldBy) methods. This abstraction is the primary reason to
		create a persistence layer in larger applications: to support reuse of the same data-access operations.
		 The persistence layer can have a generic interface without exposing implementation details. In other
		words, you can hide the fact that you’re using Hibernate (or Java Persistence) to implement the data-access
		operations from any client of the persistence layer. We consider persistence-layer portability an
		unimportant concern because full object/relational mapping solutions like Hibernate already provide
		database portability. It’s highly unlikely that you’ll rewrite your persistence layer with different
		software in the future and still not want to change any client code. Furthermore, Java Persistence is
		a standardized and fully portable API; there is little harm in occasionally exposing it to clients of
		the persistence layer.

!!!
    The persistence layer can unify data-access operations. This concern relates to portability, but from a slightly
    different angle. Imagine that you have to deal with mixed
    data-access code, such as JPA and JDBC operations. By unifying the facade that clients
    see and use, you can hide this implementation detail from the client. If you have to
    deal with different types of data stores, this is a valid reason to write a persistence layer.

    If you consider portability and unification to be side effects of creating a persistence layer, your primary
    motivation is achieving a higher level of abstraction and improve the maintainability and reuse of data-access
    code. These are good reasons, and we encourage you to create a persistence layer with a generic facade in all but
    the simplest applications. But always first consider using JPA directly without any additional layering. Keep
    it as simple as possible, and create a lean persistence layer on top of JPA when you realize you’re duplicating
    the same query and persistence operations.

!!!
    You see in this chapter and the next how to avoid the repetitive code often associated with persistence-layer
    components without using any additional tools.

A generic data access object pattern
	
	The DAO design pattern originated in Sun’s Java Blueprints more than 15 years ago; it’s had a long history.
	A DAO class defines an interface to persistence operations relating to a particular entity; it advises you to
	group together code that relates to the persistence of that entity. Given its age, there are many
	variations of the DAO pattern.

    We designed the persistence layer with two parallel hierarchies: interfaces on one
    side, implementations on the other. The basic instance-storage and -retrieval operations are grouped in a generic
    super-interface and a superclass that implements these operations with a particular persistence solution (using
    Hibernate, of course). The generic interface is extended by interfaces for particular entities that require
    additional business-related data-access operations. Again, you may have one or several implementations of an
    entity DAO interface

!!!
    There are a bunch of finder methods. These typically return managed (in persistent state) entity instances, but
    they may also return arbitrary data-transfer objects such as ItemBidSummary. Finder methods are your biggest code
    duplication issue; you may end up with dozens if you don’t plan carefully. The first step is to try to make them as
    generic as possible and move them up in the hierarchy, ideally into the top-level interface. Consider the
    findByName() method in the ItemDAO: you’ll probably have to add more options for item searches soon, or you may
    want the result presorted by the database, or you may implement some kind of paging feature.

    Methods such as makePersistent() and makeTransient() change an entity instance’s state (or the state of many
    instances at once, with cascading enabled). A client can expect that updates are executed automatically (flushed) by
    the persistence engine when an entity instance is modified (there is no performUpdate() method). You’d write a
    completely different DAO interface if your persistence layer were statement-oriented: for example, if you weren’t
    using Hibernate to implement it, but rather only plain JDBC

!!
    The persistence layer facade we introduce here doesn’t expose any Hibernate or Java Persistence interface to the
    client, so theoretically you can implement it with any software without making changes to the client code.
    You may not want or need persistence-layer portability, as explained earlier. In that case, you should consider
    exposing Hibernate or Java Persistence interfaces—for example, you could allow clients to access the JPA
    CriteriaBuilder and then have a generic findBy(CriteriaQuery) method. This decision is up to you; you may decide
    that exposing Java Persistence interfaces is a safer choice than exposing Hibernate interfaces. You should know,
    however, that although it’s possible to change the implementation of the persistence layer from one JPA provider
    to another, it’s almost impossible to rewrite a persistence layer that is state-oriented with plain JDBC statements.

Implementing the generic interface

    This generic implementation needs two things to work: an EntityManager and an entity class. A subclass must
    provide the entity class as a constructor argument. The EntityManager, however, can be provided either by a
    runtime container that understands the @PersistenceContext injection annotation (for example, any standard
    Java EE container) or through setEntityManager().

    An important decision is how you implement the makePersistent() method. Here we’ve chosen EntityManager#merge()
    because it’s the most versatile. If the given argument is a transient entity instance, merging will return a
    persistent instance. If the argument is a detached entity instance, merging will also return a persistent instance.
    This provides clients with a consistent API without worrying about the state of an entity
    instance before calling makePersistent(). But the client needs to be aware that the
    returned value of makePersistent() is always the current instance and that the argument it has given must now
    be thrown away. (basically the client needs to use the returned instance, not the one provided in the argument)

    You’ve now completed building the basic machinery of the persistence layer and the generic interface it exposes
    to the upper layer of the system. In the next step, you create entity-related DAO interfaces and implementations
    by extending the generic interface and implementation.

Implementing entity DAOs

    First you must make choices about how callers will access the DAOs. You also need to think about the life cycle
    of a DAO instance. With the current design, the DAO classes are stateless except for the EntityManager member

    Caller threads can share a DAO instance. In a multithreaded Java EE environment, for example, the automatically
    injected EntityManager is effectively thread-safe, because internally it’s often implemented as a proxy that
    delegates to some thread- or transaction-bound persistence context. Of course, if you call setEntityManager() on
    a DAO, that instance can’t be shared and should only be used by one (for example, integration/unit test) thread.

Testing the persistence layer

	In our experience, it doesn’t usually make sense to test the persistence layer separately. You could instantiate
	your domain DAO classes and provide a mock EntityManager. Such a unit test would be of limited value and quite a
	lot of work to write.
	Instead, we recommend that you create integration tests, which test a larger part of the application stack and
	involve the database system. All the rest of the examples in this chapter are from such integration tests; they
	simulate a client calling the server application, with an actual database back end. Hence, you’re testing what’s
	important: the correct behavior of your services, the business logic of the domain model they rely on,
	and database access through your DAOs, all together.

	A more interesting problem is preparing test data for integration tests. Most meaningful tests require that some
	data exists in the database. You want to load that test data into the database before your test runs, and each
	test should work with a clean and well-defined data set so you can write reliable assertions.

	 Based on our experience, here are three common techniques to import test data:
		 Your test fixture executes a method before every test to obtain an EntityManager. You manually instantiate
		your test data entities and persist them with the EntityManager API. The major advantage of this strategy is
		that you test quite a few of your mappings as a side effect. Another advantage is easy programmatic access to
		test data. For example, if you need the identifier value of a particular test Item in your test code, it’s
		already there in Java because you can pass it back from your data-import method. The disadvantage is that
		test data can be hard to maintain, because Java code isn’t a great data format. You can clear test data
		from the database by dropping and re-creating the schema after every test using Hibernate’s schema-export
		feature. All integration tests in this book so far have used this approach; you can find the test data-import
		procedure next to each test in the example code.
		 Arquillian can execute a DbUnit (http://dbunit.sourceforge.net) data-set import before every test run.
		 DbUnit offers several formats for writing data sets, including the commonly used flat XML syntax.
		 This isn’t the most compact format but is easy to read and maintain. The examples in this chapter use
		 this approach. You can find Arquillian’s @UsingDataSet on the test classes with a path to the XML file
		 to import. Hibernate generates and drops the SQL schema, and Arquillian, with the help of DbUnit, loads
		 the test data into the database. If you like to keep your test data independent of tests, this may be the
		 right approach for you. If you don’t use Arquillian, manually importing a data set is pretty easy with
		 DbUnit—see the SampleDataImporter in this chapter’s examples. We deploy this importer when running the
		 example applications during development, to have the same data available for interactive use as in
		 automated tests.
		 In section 9.1.1, you saw how to execute custom SQL scripts when Hibernate starts. The load script
		executes after Hibernate generates the schema; this is a great utility for importing test data with plain
		INSERT SQL statements. The examples in the next chapter use this approach. The major advantage is that you
	    can copy/paste the INSERT statements from an SQL console into your test fixture and vice versa. Furthermore,
		if your database supports the SQL row value constructor syntax, you can write compact multirow insertion
		statements like insert into MY_TABLE (MY_COLUMN) values (1), (2), (3), ....

		We leave it up to you to pick a strategy. This is frequently a matter of taste and how much test data you have to maintain. Note that we’re talking about test data for integration tests, not performance or scalability tests. If you need large amounts of
		(mostly random) test data for load testing, consider data-generation tools such as Benerator (http://databene.org/databene-benerator.html)


Building a stateless server

	The application will be a stateless server application, which means no application state will be managed on the
	server between client requests. The application will be simple, in that it supports only two use cases: editing
	an auction item and placing a bid for an item.

!!!
    Consider these workflows to be conversations: units of work from the perspective of the application user. The
    point of view of application users isn’t necessarily the same that we as developers have on the system; developers
    usually consider one system transaction to be a unit of work. We now focus on this mismatch and how a user’s
    perspective influences the design of server and client code. We start with the first conversation: editing an item.

!!
    The service (or controller, if you like) is still completely unaware of any business logic—it doesn’t need to
    know whether a new bid must be higher or lower than the last one. The service implementation must provide the
    currentHighestBid and currentLowestBid when calling the Item#isValid() method. This is what we hinted
    at earlier: that you may want to add operations to the BidDAO. You could write database queries to find those
    bids in the most efficient way possible without loading all the item bids into memory and sorting them there.


Analyzing the stateless application

!!!
    You’ve implemented code to support conversations: units of work from the perspective of the users. The users
    expect to perform a series of steps in a workflow and that each step will be only temporary until they finalize
    the conversation with the last step. That last step is usually a final request from the client to the server,
    ending the conversation. This sounds a lot like transactions, but you may have to perform several system
    transactions on the server to complete a particular conversation. The question is how to provide atomicity across
    several requests and system transactions.

    Conversations by users can be of arbitrary complexity and duration. More than one client request in a conversation’s
    flow may load detached data. Because you’re in control of the detached instances on the client, you can easily
    make a conversation atomic if you don’t merge, persist, or remove any entity instances on the server until
    the final request in your conversation workflow. It’s up to you to somehow queue modifications and manage detached
    data where the list of items is held during user thinktime. Just don’t call any service operation from the client
    that makes permanent changes on the server until you’re sure you want to “commit” the conversation.

    You should enable versioning of the Item entity for multiuser applications, as explained in the section “Enabling
    versioning” in chapter 11. When entity modifications are merged in AuctionService#storeItem(), Hibernate
    increments the Item’s  version (it doesn’t if the Item wasn’t modified, though). Hence, if another user has
    changed the name of an Item concurrently, Hibernate will throw an exception when the system transaction is
    committed and the persistence context is flushed. The first user to commit their conversation always wins with
    this optimistic strategy. The second user should be shown the usual error message: “Sorry, someone else modified
    the same data; please restart your conversation.”

!!!
    What you’ve created is a system with a rich client or thick client; the client isn’t a dumb input/output terminal
    but an application with an internal state independent of the server (recall that the server doesn’t hold any
    application state). One of the advantages of such a stateless server is that any server can handle any client
    request. If a server fails, you can route the next request to a different server, and the conversation process
    continues. The servers in a cluster share nothing; you can easily scale up your system horizontally by adding more
    servers. Obviously, all application servers still share the database system, but at least you only have to worry
    about scaling up one tier of servers.

    Keeping changes after race conditions
    In acceptance testing, you’ll probably discover that users don’t like to restart conversations when a race
    condition is detected. They may demand pessimistic locking: while user A edits an item, user B shouldn’t even be
    allowed to load it into an editor dialog. The fundamental problem isn’t the optimistic version checks at the
    end of a conversation; it’s that you lose all your work when you start a conversation from scratch.
    Instead of rendering a simple concurrency error message, you could offer a dialog that allows the user to keep
    their now invalid changes, merge them manually with the modifications made by the other user, and then save the
    combined result. Be warned, though: implementing this feature can be time-consuming, and Hibernate doesn’t help
    much.

    The downside is that you need to write rich-client applications, and you have to deal with network communication
    and data-serialization issues. Complexity shifts from the server side to the client side, and you have to
    optimize communication between the client and server.

!!!
    If, instead of on an EJB client, your (JavaScript) client has to work on several web browsers or even as a native
    application on different (mobile) operating systems, this can certainly be a challenge. We recommend this
    architecture if your rich client runs in popular web browsers, where users download the latest version of the
    client application every time they visit your website. Rolling out native clients on several platforms,
    and maintaining and upgrading the installations, can be a significant burden even in medium-sized intranets where
    you control the user’s environment.

!!!
    Next, you implement the same use cases again, but with a very different strategy.
    The server will now hold the conversational state of the application, and the client will only be a dumb
    input/output device. This is an architecture with a thin client and a stateful server.


Building a stateful server

!!!
    With a thin client, it’s the server’s job to transform data for output into a display format understood by the
    thin client—for example, into HTML pages rendered by a web browser. The client transmits user input operations
    directly to the server—for example, as simple HTML form submissions. The server is responsible for decoding and
    transforming the input into higher-level domain model operations. We keep this part simple for now, though,
    and use only remote method invocation with an EJB client.

    The server must then also hold conversational data, usually stored in some kind of
    server-side session associated with a particular client. Note that a client’s session has a
    larger scope than a single conversation; a user may perform several conversations during a session. If the user
    walks away from the client and doesn’t complete a conversation, temporary conversation data must be removed on
    the server at some point. The server typically handles this situation with timeouts; for example, the server may
    discard a client’s session and all the data it contains after a certain period of inactivity.
    This sounds like a job for EJB stateful session beans, and, indeed, they’re ideal for this kind of architecture
    if you’re in need of a standardized solution.

!!!
    Even if you have a stateful server architecture, there will be many short conversations in your application that
    don’t require any state to be held on the server. This is both normal and important: holding state on the server
    consumes resources. If you implemented the getSummaries() operation with a stateful session bean, you’d waste
    resources. You’d only use the stateful bean for a single operation, and then it would consume memory until the
    container expired it. Stateful server architecture doesn’t mean you can only use stateful server-side components.

    You use @PersistenceContext to declare that this stateful bean needs an EntityManager and that the container
    should extend the persistence context to span the same duration as the life cycle of the stateful session bean.
    This extended mode is an option exclusively for stateful EJBs. Without it, the container will create and close
    a persistence context whenever a transaction commits. Here, you want the persistence context to stay open beyond
    any transaction boundaries and remain attached to the stateful session bean instance

    The ItemDAO also needs an EntityManager; remember that it has a @PersistenceContext annotation without any
    options. The rules for persistence context propagation in EJB calls you’ve seen before still apply: Hibernate
    propagates the persistence context along with the transaction context. The persistence context rides along into
    the ItemDAO with the transaction that was started for the startConversation() method call.


    The important aspect you must understand is how persistence context and transaction handling work in EJBs.

!!!
    NOTE There are additional rules for EJBs about persistence-context propagation between different types of
    components. They’re extremely complex, and we’ve never seen any good use cases for them in practice. For example,
    you probably wouldn’t call a stateful EJB from a stateless EJB. Another complication is disabled or optional
    EJB method transactions, which also influence how the persistence context is propagated along with component
    calls. We explained these propagation rules in the previous edition of this book. We
    recommend that you try to work only with the strategies shown in this chapter, and keep things simple


Analyzing the stateful application

     At some point, usually when the last request in a conversation occurs, you commit
    the conversation and write the changes to the database. The conversation is atomic if
    you don’t join the extended EntityManager with a transaction until the last event in
    the conversation. No dirty checking and flushing will occur if you only read data in
    unsynchronized mode

    The loaded Item and other data become stale, however, if the user needs a long time to trigger the next request.
    You may want to refresh() some managed entity instances during the conversation if you need updates from the
    database, as explained in section 10.2.6. Alternatively, you can refresh to undo an operation during a
    conversation. For example, if the user changes the Item#name in a dialog but then decides to undo this, you
    can refresh() the persistent Item instance to retrieve the “old” name from the database.

    The stateful server architecture may be more difficult to scale horizontally. If a server
    fails, the state of the current conversation and indeed the entire session is lost. Replicating sessions on
    several servers is a costly operation, because any modification of session data on one server involves network
    communication to (potentially all) other servers.

    On the other hand, stateful servers can act as a first line of caches with their extended persistence contexts
    in user sessions. Once an Item has been loaded for a particular user conversation, that Item won’t be loaded
    again from the database in the same conversation. This can be a great tool to reduce the load on your database
    servers (the most expensive tier to scale).

    There are alternative implementations of thin clients and stateful servers, of
    course. You can use regular request-scoped persistence contexts and manage detached
    (not persistent) entity instances on the server manually. This is certainly possible with detaching and merging but
     can be much more work. One of the main advantages of the extended persistence context, transparent lazy loading
     even across requests, would no longer be available either.

!!!
     Thin client systems typically produce more load on servers than rich clients do.
     Every time the user interacts with the application, a client event results in a network
     request. This can even happen for every mouse click in a web application. Only the
     server knows the state of the current conversation and has to prepare and render all
     information the user is viewing. A rich client, on the other hand, can load raw data
     needed for a conversation in one request, transform it, and bind it locally to the user
     interface as needed. A dialog in a rich client can queue modifications on the client
     side and fire a network request only when it has to make changes persistent at the end
     of a conversation.

!!!
     An additional challenge with thin clients is parallel conversations by one user: what
     happens if a user is editing two items at the same time—for example, in two web
     browser tabs? This means the user has two parallel conversations with the server. The
     server must separate data in the user session by conversation. Client requests during a
     conversation must therefore contain some sort of conversation identifier so you can
     select the correct conversation state from the user’s session for each request. This happens automatically
     with EJB clients and servers but probably isn’t built into your favorite web application framework (unless it’s
     JSF and CDI, as you’ll see in the next chapter).

      One significant benefit of a stateful server is less reliance on the client platform; if
     the client is a simple input/output terminal with few moving parts, there is less chance
     for things to go wrong. The only place you have to implement data validation and
     security checks is the server. There are no deployment issues to deal with; you can roll
     out application upgrades on servers without touching clients.
     Today, there are few advantages to thin client systems, and stateful server installations are declining.
     This is especially true in the web application sector, where easy scalability is frequently a major concern.

     Summary
      You implemented simple conversations—units of work, from the perspective of your application user.
      You saw two server and client designs, with stateless and stateful servers, and
     learned how Hibernate fits into both these architectures.
      You can work with either detached entity state or an extended conversation scoped persistence context

TODO
Building web applications

    In this chapter, you see how Hibernate works in a typical web application environment. There are dozens of web
    application frameworks for Java, so we apologize if we don’t cover your favorite combination here. We discuss
    JPA in the standard Java Enterprise Edition environment, in particular combined with standards Contexts
    and Dependency Injection (CDI), JavaServer Faces (JSF), and Java API for RESTful
    web services (JAX-RS). As always, we show patterns you can apply in other proprietary environments.

!!!
    Next, you write a fully functional JSF application on top of the persistence layer
    and look at the Java EE conversation scope, where CDI and JSF work together to provide a simple stateful model
    for server-side components. If you didn’t like the stateful EJBs with the extended persistence context in the
    last chapter, maybe these conversation examples with detached entity state on the server are what you’re looking for.
!!!
    Finally, if you prefer writing web applications with rich clients, a stateless server,
    and frameworks such as JAX-RS, GWT, or AngularJS, we show you how to customize
    serialization of JPA entity instances into XML and JSON formats.

Integrating JPA with CDI

!!!
    The CDI standard offers a type-safe dependency injection and component life-cycle management system in a
    Java EE runtime environment. You saw the @Inject annotation in the previous chapter and used it to wire the
    ItemDAO and BidDAO components together with the service EJB classes.
    The JPA @PersistenceContext annotation you used inside the DAO classes is just another special injection case:
    you tell the runtime container to provide and automatically handle an EntityManager instance.
    This is a "container-managed EntityManager".
    There are some strings attached, though, such as the persistence-context propagation and transaction rules we
    discussed in the previous chapter. Such rules are convenient when all of your service and DAO classes are EJBs,
    but if you don’t employ EJBs, you may not want to follow these rules.
    With an "application-managed EntityManager", you can create your own persistence context management, propagation,
    and injection rules. You now rewrite the DAO classes as simple CDI managed beans, which are just like
    EJBs: plain Java classes with extra annotations. You want to @Inject an EntityManager
    and drop the @PersistenceContext annotation, and thus have full control over the persistence context.

Producing an EntityManager

!!!!!
    A producer in CDI parlance is a factory used to customize creation of an instance and
    tell the runtime container to call a custom routine whenever the application needs an
    instance based on the declared scope. For example, the container will create an application-scoped instance only
    once during the life cycle of the application. The container creates a request-scoped instance once for every
    request handled by a server and a session-scoped instance once for every session a user has with a server

!!!
    The CDI specification maps the abstract notions of request and session to servlet
    requests and sessions. Remember that both JSF and JAX-RS build on top of servlets, so
    CDI works well with those frameworks. In other words, don’t worry much about this: in
    a Java EE environment, all the integration work has already been done for you.

    Let’s create a producer of request-scoped EntityManager instances:

    @javax.enterprise.context.ApplicationScoped
    public class EntityManagerProducer {
        @PersistenceUnit
        private EntityManagerFactory entityManagerFactory;

        @javax.enterprise.inject.Produces
        @javax.enterprise.context.RequestScoped
        public EntityManager create() {
            return entityManagerFactory.createEntityManager();
        }

        public void dispose(@javax.enterprise.inject.Disposes EntityManager entityManager) {
            if (entityManager.isOpen())
                entityManager.close();
        }
    }

    The Java EE runtime gives you the persistence unit configured in persistence.xml (with  @PersistenceUnit),
    which is also an application-scoped component. (If you use CDI standalone and outside
    a Java EE environment, you can instead use the static Persistence.createEntityManagerFactory() bootstrap.)

    When a request is over and the request context is being destroyed, the CDI container
    calls this method (dispose) to get rid of an EntityManager instance. You created this application managed
    persistence context  (see section 10.1.2), so it’s your job to close it.

    You now have a producer of application-managed EntityManagers and request scoped persistence contexts. Next,
    you must find a way to let the EntityManager know about your system transactions


Joining the EntityManager with transactions

!!!
    When your server must handle a servlet request, the container creates the EntityManager automatically when it
    first needs it for injection. Remember that an EntityManager you manually create will only join a system
    transaction automatically if the transaction is already in progress. Otherwise, it will be unsynchronized: you’ll
    read data in auto-commit mode, and Hibernate won’t flush the persistence context.
    It’s not always obvious when the container will call the EntityManager producer or exactly when during a request
    the first EntityManager injection takes place. Of course, if you process a request without a system transaction,
    the EntityManager you get is always unsynchronized. Hence, you must ensure that the EntityManager knows
    you have a system transaction.
    You add a method for this purpose to the persistence layer, in the super-interface: void joinTransaction();

!!!
    You should call this method on any DAO before storing data, when you’re sure you’re
    in a transaction. If you forget, Hibernate will throw a TransactionRequiredException
    when you try to write data, indicating that the EntityManager has been created before
    a transaction was started and that nobody told it about the transaction. If you want to
    exercise your CDI skills, you could try to implement this aspect with a CDI decorator or
    interceptor.


Injecting an EntityManager

!!!
    When your application needs an ItemDAO, the CDI runtime will call your EntityManagerProducer and then call the
    ItemDAOImpl constructor. The container will reuse  the same EntityManager for any injection in any DAO during a
    particular request.

    What scope then is ItemDAO? Because you don’t declare a scope for the implementation class, it’s dependent.
    An ItemDAO is created whenever someone needs it, and the ItemDAO instance is then in the same context and scope
    as its caller and belongs to that calling object. This is a good choice for the persistence layer API, because
    you delegate scoping decisions to the upper layer with the services calling the persistence layer.

    You’re now ready to @Inject an ItemDAO field in a service class.


Paging and sorting data

     Because the query returns much more information than can be displayed on a single page, you only show a subset
    of the data. You only render a certain number of rows in a data table and give users the options to go to the
    next, previous, first, or last page of rows. Users also expect the application to preserve sorting
    when they switch pages.
     Users want to be able to click a column header in the table and sort the rows of the table by the values of
    this column. Typically, you can sort in either ascending or descending order; this can be switched by subsequent
    clicks the column header

!!!
    Page browsing can be implemented in two variations: using the offset or the seek
    technique. Let’s look first at the differences and what you want to implement.

    Behind this page are database queries with an offset and a limit condition, based on
    row numbers. The Java Persistence API for this are Query#setFirstResult() and
    Query#setMaxResults(), which we discussed in section 14.2.4. You write a query and
    then let Hibernate wrap the offset and limit clauses around it, depending on your
    database SQL dialect.

    Now consider the alternative: paging with a seek method, as shown in figure 19.2.
    Here you don’t offer users the option to jump to any page by offset; you only allow
    them to seek forward, to the next page. This may seem restrictive, but you’ve probably
    seen or even implemented such a paging routine when you needed infinite scrolling.
    You can, for example, automatically load and display the next page of data when the
    user reaches the bottom of the table/screen

    The seek method relies on a special additional restriction in the query retrieving the
    data. When the next page has to be loaded, you query for all items with a name
    “greater than [Coffee Machine]”. You seek forward not by offset of result rows with
    setFirstResult(), but by restricting the result based on the ordered values of some
    key. If you’re unfamiliar with seek paging (sometimes called keyset paging), we’re sure
    you won’t find this difficult once you see the queries later in this section.

    Let’s compare the advantages and disadvantages of both techniques. You can of
    course implement an endless-scrolling feature with offset paging or direct-page navigation with the seek
    technique; but they each have their strength and weaknesses:

     The offset method is great when users want to jump to pages directly. For example, many search engines offer
        the option to jump directly to page 42 of a query result or directly to the last page. Because you can easily
        calculate the offset and limit of a range of rows based on the desired page number, you can implement
        this with little effort. With the seek method, providing such a page-jumping UI
        is more difficult; you must know the value to seek. You don’t know what item
        name the client displayed just before page 42, so you can’t seek forward to
        items with names “greater than X.” Seeking is best suited for a UI where users
        only move forward or backward page by page through a data list or table, and if
        you can easily remember the last value shown to the user.
     A great use case for seeking is paging based on anchor values that you don’t
        have to remember. For example, all customers whose names start with C could
        be on one page and those starting with D on the next page. Alternatively, each
        page shows auction items that have reached a certain threshold value for their highest bid amount.
     The offset method performs much worse if you fetch higher page numbers. If
        you jump to page 5,000, the database must count all the rows and prepare 5,000
        pages of data before it can skip 4,999 of them to give you the result. A common
        workaround is to restrict how far a user can jump: for example, only allowing
        direct jumps to the first 100 pages and forcing the user to refine the query
        restrictions to get a smaller result. The seek method is usually faster than the offset method, even on low
        page numbers. The database query optimizer can skip directly to the start of the desired page and efficiently
        limit the index range to scan. Records shown on previous pages don’t have to be considered or counted.
     The offset method may sometimes show incorrect results. Although the result
        may be consistent with what’s in the database, your users may consider it incorrect. When applications insert
        new records or delete existing records while the user is browsing, anomalies may occur. Imagine the user
        looking at page 1 and someone adding a new record that would appear on page 1. If the user now
        retrieves page 2, some record they may have seen on page 1 is pushed forward
        onto page 2. If a record on page 1 was deleted, the user may miss data on page
        2 because a record was pulled back to page 1. The seek method avoids these
        anomalies; records don’t mysteriously reappear or vanish.

Paging in the persistence layer


Building JSF applications

Request-scoped services

    The @Transactional annotation is new in Java EE 7 (from JTA 1.2) and similar to @TransactionAttribute on EJB
    components. Internally, an interceptor wraps the method call in a system transaction context, just like an
    EJB method.

Conversation-scoped services

!!!
    You can declare beans and produced instances in Java EE as @ConversationScoped.
    This special scope is manually controllable with the standard javax.enterprise.context.Conversation API. To
    understand conversation scope, think about the examples in the previous chapter.
    The shortest possible conversation, a unit of work from the perspective of the application user, is a single
    request/response cycle. A user sends a request, and the server starts a conversation context to hold the data for
    that request. When the response is returned, this short-running transient conversation context ends and is
    closed. Transient conversation scope is therefore the same as the request scope; both contexts have the same
    life cycle. This is how the CDI specification maps the conversation scope to servlet requests and therefore also
    JSF and JAX-RS requests.

    With the Conversation API, you can promote a conversation on the server and make it long-running and no longer
    transient. If you promote the conversation during a request, the same conversation context is then available to
    the next request. You can use the context to transport data from one request to the next. Usually you’d
    need the session context for this, but the data from several parallel conversations (a user opens two browser
    tabs) would have to be manually isolated in the session. This is what the conversation context provides: automatic
    data isolation in a controllable context, which is more convenient than the regular session context.

    You implement the conversation context with server-side sessions, of course, and store the data in the user’s
    session. To isolate and identify conversations within a session, each conversation has an identifier value; you
    must transmit that value with every request. The parameter name cid has even been standardized for this purpose.
    Each conversation also has an individual timeout setting so the server can free resources if a user stops sending
    requests. This allows the server to clean up expired conversation state automatically without waiting until the
    entire user session expires.


THE “EDIT ITEM” CONVERSATION WORKFLOW

    Meanwhile, the application waits for the user to trigger an event; we call this user think-time

    Note that rendering this page won’t start a long-running conversation context on the
    server. This would waste resources, because you don’t know yet whether the user
    wants to follow through and work on the item details. You begin the long-running
    conversation context and hold state across requests on the server when the user clicks
    the Next button for the first time. If form validation passes, the server stores the conversational data in the
    user’s session and advances the user to the Edit Images page

    The EntityManager field of the DAOs isn’t an actual instance of a persistence context at runtime. The field
    holds a reference to some EntityManager: some current persistence context. Remember that CDI produces and
    injects the dependency through the constructor. Because you declared it request-scoped, at runtime a special
    proxy is injected that only looks like a real EntityManager. This proxy delegates all
    calls to an actual EntityManager it finds in the current request context. The proxy is
    serializable and doesn’t hold a reference to an EntityManager after a request is complete. It can then be easily
    serialized; and when it’s deserialized, maybe even on a different JVM, it will continue to do its work and obtain
    a request-scoped EntityManager whenever called. Therefore, you’re not serializing the entire persistence context—
    only a proxy that can look up the current request-scoped persistence context

    This may sound strange at first, but it’s how CDI works: if a request-scoped bean is
    injected into a conversation-, session-, or application-scoped bean, an indirect reference is required. If you
    try to call the EntityManager proxy through a DAO when no request context is active (say, in a servlet’s init()
    method), you’ll get a ContextNotActiveException because the proxy can’t obtain a current EntityManager. The CDI
    specification also defines that such proxies may be passivated (serialized), even when the component they
    represent may not be.


BEGINNING LONG-RUNNING CONVERSATIONS

    The action method is called after all the Item details have been set on the EditItemService#item property by the
    JSF engine. You make the transient conversation longrunning with an individual timeout setting. This timeout is
    obviously shorter than or equal to the timeout of the user’s session; larger values don’t make sense. The server
    preserves the state of the service in the user’s session and renders an automatically
    generated conversation identifier as a hidden field on any action form on the Edit
    Images page. If you need it, you can also obtain the identifier value of the conversation with
    Conversation#getId(). You can even set your own identifier value in the  Conversation#begin() method call.

!!!
    One of the most important issues when building a stateful server system is memory
    consumption and how many concurrent user sessions the system can handle. You
    must be careful with conversational data. Always question whether you must hold data
    you get from the user or data you load from the database for the entire conversation.
    When the user clicks Submit Item, the conversation ends, and all transient and
    detached entity instances must be stored.

    This completes our example of stateful services with JSF, CDI, and a JPA persistence
    layer. We think JSF and CDI are great in combination with JPA; you get a well-tested
    and standardized programming model with little overhead, both in terms of resource
    consumption and lines of code.

!!!
    We now continue with CDI but, instead of JSF, introduce JAX-RS combined with JPA
    in a stateless server design for any rich clients. One of the challenges you face in this
    architecture is serializing data.


Serializing domain model data

!!!!
    When we first talked about writing persistence-capable classes in section 3.2.3, we briefly mentioned that the
    classes don’t have to implement java.io.Serializable.
    You can apply this marker interface when needed.
    One of the cases when this was necessary so far was in chapter 18. Domain model
    instances were transmitted between EJB client and server systems, and they were automatically serialized on one
    end into some wire format and deserialized on the other end. This worked flawlessly and without customization
    because both client and server are Java virtual machines, and the Hibernate libraries were available on both systems.
    The client used Remote Method Invocation (RMI) and the standardized Java serialization format (a stream of bytes
    representing Java objects).

!!!
    If your client isn’t running in a Java virtual machine, you probably don’t want to receive a stream of bytes
    representing Java objects from the server. A common scenario is a stateless server system that handles a rich
    client, such as a JavaScript application running in a web browser or a mobile device application. To implement
    this, you typically create a Web API on the server that speaks HTTP and transmit either XML or
    JSON payloads, which clients must then parse. The clients also send XML or JSON data when changes must be stored
    on the server, so your server must be able to produce and consume the desired media types.


    Designing RESTful hypermedia-driven applications
    Many people call a system with HTTP remote communication and JSON or XML media types RESTful. But one of the
    most important aspects of an architecture with representational state transfer is
    that client and server exchange hypermedia documents.
    These hypermedia documents contain the data and the affordances (actions) available on that data: Thus, the
    name Hypermedia As The Engine Of Application State (HATEOAS). Because this is a book about Hibernate and not
    Web API design, we can only show you how to build a simple HTTP API that exchanges basic XML documents,
    which isn’t RESTful and doesn’t use hypermedia.
    When you design your own API, consider the H Factor of your chosen media type (see http://amundsen.com/hypermedia/hfactor/)
    and study the excellent book RESTful Web APIs by Leonard Richardson, Mike Amundsen, and Sam Ruby (Richardson,
    2013). We recommend that you avoid using JSON, because it requires proprietary  extensions to improve its H-Factor.
    Designing your own XML-based hypermedia format, perhaps by extending the example shown in this chapter, isn’t
    much better. Our favorite media type is plain XHTML:
    it has a great H-Factor, and it’s easy to write and read with APIs available everywhere. Compressed, it can be
    more efficient than JSON, and it’s a joy to work with
    interactively when building and testing your API. Jon Moore presents a great example of such a design in
    “Building Hypermedia APIs with HTML” (www.infoq.com
    /presentations/web-api-html).


    You now write an HTTP server with the JAX-RS framework, producing and consuming
    XML documents. Although the examples are in XML, they’re equally applicable to
    JSON, and the fundamental problems we discuss are the same in both.



Writing a JAX-RS service

    Let’s start with the JAX-RS service. One service method delivers an XML document representing an Item entity
    instance when a client sends a GET HTTP request. Another service method accepts an XML document for updating
    an Item in a PUT request:

    When the server receives a request with the request path /item, the method on this
    service handles it. By default, the service instance is request-scoped, but you can apply
    CDI scoping annotations to change that

    The container uses the path segment after /item as an argument value for the call,
    such as /item/123. You map it to a method parameter with @PathParam.

    This method produces XML media; therefore, someone has to serialize the method’s
    returned value into XML. Be careful: this annotation isn’t the same producer annotation as in CDI.
    It’s in a different package!

    This method consumes XML media; therefore, someone has to deserialize the XML
    document and transform it into a detached Item instance.

!!!!
    The JAX-RS standard covers automatic marshalling for the most important media
    types and a whole range of Java types. A JAX-RS implementation, for example, must be
    able to produce and consume XML media for an application-supplied Java Architecture for XML Binding (JAXB)
    class. The domain model entity class Item must therefore become a JAXB class.


Applying JAXB mappings

    JAXB, much like JPA, works with annotations to declare a class’s capabilities. These
    annotations map the properties of a class to elements and attributes in an XML document.
    The JAXB runtime automatically reads and writes instances from and to XML
    documents. This should sound familiar to you by now; JAXB is a great companion for
    JPA-enabled domain models.


    @Entity
    @XmlRootElement                                         // Maps to XML
    @XmlAccessorType(XmlAccessType.FIELD)                   // use fileds to create xml, not getter methods
    public class Item implements Serializable {
        @Id
        @GeneratedValue(generator = Constants.ID_GENERATOR)
        @XmlAttribute
        protected Long id;

        @NotNull
        @Future(message = "{Item.auctionEnd.Future}")

        @XmlAttribute
        protected Date auctionEnd;


        @OneToMany(mappedBy = "item")
        @XmlElementWrapper(name = "bids")
        @XmlElement(name = "bid")
        protected Set<Bid> bids = new HashSet<>();


         // ...
    }

!!!
    The identifier and auction end date of the item become XML attributes, and all other
    properties are nested XML elements. You don’t have to put any JAXB annotations on
    description and initialPrice; they map to elements by default. Singular attributes
    of the domain model class are easy: they’re either XML attributes or nested XML elements with some text.
    What about entity associations and collections?

    There is some optimization potential here: if you say, “Always eagerly include bids”
    when your service returns an Item, then you should load them eagerly. Right now, several queries are necessary
    in JPA to load the Item and the default lazy-mapped Item#bids. The JAXB serializer automatically iterates through
    the collection elements when the response is prepared.

    When you don’t want to include a collection or a property in the XML document,
    use the @XmlTransient annotation:

    Collections are easy to handle, regardless of whether they’re collections of primitives,
    embeddables, or many-valued entity associations. Of course, you must be careful with
    circular references, such as each Bid having a (back) reference to an Item. At some
    point, you must make a break and declare a reference transient.

!!!
    The most difficult issue you face when serializing an entity instance loaded by
    Hibernate is internal proxies: the placeholders used for lazy loading of entity associations. In the Item class,
    this is the seller property, referencing a User entity.

Serializing Hibernate proxies

    This is the same data as the proxy: the entity class and the identifier value represented
    by the proxy. A client now knows that there is indeed a seller for this item and the
    identifier of that user; it can request this data if needed. If you receive this XML document on the server when
    a user updates an item, you can reconstruct a proxy from the entity class name and identifier value.
    You should write a model class that represents such an entity reference and map it
    to XML elements and attributes:


        @XmlRootElement
        @XmlAccessorType(XmlAccessType.FIELD)
        public class EntityReference {
             @XmlAttribute
             public Class type;
             @XmlAttribute
             public Long id;

             public EntityReference() { }

             public EntityReference(Class type, Long id) {
                this.type = type;
                this.id = id;
             }
        }

        public class Item implements Serializable {
             @NotNull
             @ManyToOne(fetch = LAZY)
             @XmlJavaTypeAdapter(EntityReferenceAdapter.class)
             protected User seller;
             // ...
        }

    You can use the EntityReferenceAdapter for any entity association property. It knows
    how to read and write an EntityReference from and to XML:

    public class EntityReferenceAdapter extends XmlAdapter<EntityReference, Object> {

        EntityManager em;

        public EntityReferenceAdapter() { }                    // 1

        public EntityReferenceAdapter(EntityManager em) {        // 2
            this.em = em;
        }

        @Override
        public EntityReference marshal(Object entityInstance) throws Exception {
             Class type = getType(entityInstance);                                  // 3
             Long id = getId(type, entityInstance);
             return new EntityReference(type, id);
        }

        @Override
        public Object unmarshal(EntityReference entityReference) throws Exception {
         if (em == null)
            throw new IllegalStateException( "Call Unmarshaller#setAdapter() and provide an EntityManager");

         return em.getReference(entityReference.type, entityReference.id );                 //4
         }
    }


    1 JAXB calls this constructor when it generates an XML document. In that case, you don’t need an EntityManager:
     the proxy contains all the information you need to write an EntityReference.
    2 JAXB must call this constructor when it reads an XML document. You need an EntityManager to get a Hibernate
    proxy from an EntityReference.
    3 When writing an XML document, take the Hibernate proxy and create a serializable representation. This calls
    internal Hibernate methods that we haven’t shown here.
    4 When reading an XML document, take the serialized representation and create a Hibernate proxy attached to
    the current persistence context.

    Finally, you need an extension for JAX-RS that will automatically initialize this adapter
    with the current request-scoped EntityManager when an XML document has to be
    unmarshalled on the server. You can find this EntityReferenceXMLReader extension
    in the example code for this book

!!!!
    There are a few remaining points we need to discuss. First, we haven’t talked about
    unmarshalling collections. Any <bids> element in the XML document will be deserialized when the service is
    called, and detached instances of Bid will be created from that data. You can access them on the detached
    Item#bids when your service runs. Nothing else will happen or can happen, though: the collection created during
    unmarshalling by JAXB isn’t one of the special Hibernate collections. Even if you had enabled cascaded merging
    of the Item#bids collection in your mapping, it would be ignored by EntityManager#merge().

    This is similar to the proxy problem you solved in the previous section. You would
    have to detect that you must create a special Hibernate collection when a particular
    property is unmarshalled in an XML document. You’d have to call some Hibernate
    internal APIs to create that magic collection. We recommend that you consider collections to be read-only;
    collection mappings in general are a shortcut for embedding
    data in query results when you send data to the client. When the client sends an XML
    document to the server, it shouldn’t include any <bids> element. On the server, you
    only access the collection on the persistent Item after it’s merged (ignoring the collection during merge).

    If you want to use JSON with Hibernate, you must write the same extension code we
    wrote for JAXB, but for your favorite JSON marshalling tool. You’ll have to customize
    proxy handling, how proxy data is sent to the client, and how proxy data is turned back
    into entity references with em.getReference(). You’ll certainly have to rely on some
    extension API of your framework, just as we did with JAXB, but the same pattern applies

    Summary
     You’ve seen many ways to integrate Hibernate and JPA in a web application environment. You enabled the
    EntityManager for CDI injection and improved the persistence layer with CDI and a generic sorting and paging
    solution for finder queries.
     You looked at JPA in a JSF web application: how to write request- and conversation-scoped services with a
    JPA persistence context.
     We discussed problems and solutions associated with entity data serialization, and how to resolve those in an
    environment with stateless clients and a JAX-RS server




Scaling Hibernate

     In this chapter, we show you how to avoid falling back to JDBC and how to execute bulk and batch operations with
     Hibernate and JPA.


Bulk statements in JPQL and criteria

!!!
    The Java Persistence Query Language is similar to SQL. The main difference between
    the two is that JPQL uses class names instead of table names and property names
    instead of column names. JPQL also understands inheritance—that is, whether you’re
    querying with a superclass or an interface. The JPA criteria query facility supports the
    same query constructs as JPQL but in addition offers type-safe and easy programmatic
    statement creation.

    The next statements we show you support updating and deleting data directly in
    the database without the need to retrieve them into memory

        Query query = em.createQuery("update Item i set i.active = true where i.seller = :s")
            .setParameter("s", johndoe);
        int updatedEntities = query.executeUpdate();
        assertEquals(updatedEntities, 2);                   // Entity instances, not “rows”

!!!!!!
    This UPDATE statement only affects the database; Hibernate doesn’t update any
    Item instance you’ve already retrieved into the (current) persistence context. In the
    previous chapters, we’ve repeated that you should think about state management of
    entity instances, not how SQL statements are managed. This strategy assumes that the
    entity instances you’re referring to are available in memory. If you update or delete
    data directly in the database, what you’ve already loaded into application memory,
    into the persistence context, isn’t updated or deleted

!!!!
    A pragmatic solution that avoids this issue is a simple convention: execute any
    direct DML operations first in a fresh persistence context. Then, use the EntityManager to load and store
    entity instances. This convention guarantees that the persistence context is unaffected by any statements
    executed earlier. Alternatively, you can selectively use the refresh() operation to reload the state of an
    entity instance in the persistence context from the database, if you know it’s been modified outside of the
    persistence context.

!!!
    Bulk JPQL/criteria statements and the second-level cache
    Executing a DML operation directly on the database automatically clears the optional
    Hibernate second-level cache. Hibernate parses your JPQL and criteria bulk operations and detects which cache
    regions are affected. Hibernate then clears the regions in the second-level cache. Note that this is a
    coarse-grained invalidation: although you may only update or delete a few rows in the ITEM table, Hibernate
    clears and invalidates all cache regions where it holds Item data.

!!!!
    Another benefit is that the JPQL UPDATE statement and a CriteriaUpdate work with
    inheritance hierarchies. The following statement marks all credit cards as stolen if the
    owner’s name starts with “J” :
        Query query = em.createQuery(
            "update CreditCard c set c.stolenOn = :now where c.owner like 'J%'"
        ).setParameter("now", new Date());
    Hibernate knows how to execute this update, even if several SQL statements have to be
    generated or some data needs to be copied into a temporary table; it updates rows in
    several base tables (because CreditCard is mapped to several superclass and subclass
    tables).

!!!! (hibernate feature only)
    Direct DML operations, by default, don’t affect any version or timestamp values in
    the affected entities (as standardized by JPA). But a Hibernate extension lets you
    increment the version number of directly modified entity instances:

        int updatedEntities = em.createQuery("update versioned Item i set i.active = true").executeUpdate();
!!!
    The version of each updated Item entity instance will now be directly incremented in
    the database, indicating to any other transaction relying on optimistic concurrency
    control that you modified the data. (Hibernate doesn’t allow use of the versioned
    keyword if your version or timestamp property relies on a custom org.hibernate.usertype.UserVersionType.)

    The same rules for UPDATE statements and CriteriaUpdate apply to DELETE and CriteriaDelete:


Bulk statements in SQL

    In the previous section, you saw JPQL UPDATE and DELETE statements. The primary
    advantage of these statements is that they work with class and property names and that
    Hibernate knows how to handle inheritance hierarchies and versioning when generating SQL. Because Hibernate
    parses JPQL, it also knows how to efficiently dirty-check and flush the persistence context before the query and
    how to invalidate second-level cache regions.
    If JPQL doesn’t have the features you need, you can execute native SQL bulk statements:

        Query query = em.createNativeQuery("update ITEM set ACTIVE = true where SELLER_ID = :sellerId")
            .setParameter("sellerId", johndoe.getId());
        int updatedEntities = query.executeUpdate();

!!!!!
    With JPA native bulk statements, you must be aware of one important issue: Hibernate
    will not parse your SQL statement to detect the affected tables. This means Hibernate
    doesn’t know whether a flush of the persistence context is required before the query
    executes. In the previous example, Hibernate doesn’t know you’re updating rows in
    the ITEM table. Hibernate has to dirty-check and flush any entity instances in the persistence context when you
    execute the query; it can’t only dirty-check and flush Item instances in the persistence context.
!!!! You must consider another issue if you enable the second-level cache (if you don’t,
    don’t worry): Hibernate has to keep your second-level cache synchronized to avoid
    returning stale data, so it will invalidate and clear all second-level cache regions when
    you execute a native SQL UPDATE or DELETE statement. This means your second-level
    cache will be empty after this query

INSERTING ENTITY INSTANCES IN BATCHES
    Every transient entity instance you pass to EntityManager#persist() is added to the
    persistence context cache, as explained in section 10.2.8. To prevent memory exhaustion, you flush() and
    clear() the persistence context after a certain number of insertions, effectively batching the inserts.

    You should set the hibernate.jdbc.batch_size property in the persistence unit to
    the same size as your batch, here 100. With this setting, Hibernate will batch the
    INSERT statements at the JDBC level, with PreparedStatement#addBatch().

Batching interleaved SQL statements
    A batch procedure persisting several different entity instances in an interleaved fashion, let’s say an Item,
    then a User, then another Item, another User, and so on, isn’t efficiently batched at the JDBC level. When
    flushing, Hibernate generates an insert into ITEM SQL statement, then an insert into USERS statement, then
    another insert into ITEM statement, and so on. Hibernate can’t execute a larger
    batch at once, given that each statement is different from the last. If you enable the
    property hibernate.order_inserts in the persistence unit configuration, Hibernate
    sorts the operations before trying to build a batch of statements. Hibernate then executes all INSERT statements
    for the ITEM table and all INSERT statements for the USERS table. Then, Hibernate can batch the statements at the
    JDBC level.


The Hibernate StatelessSession interface

Caching data
    In this section, we show you how to enable, tune, and manage the shared data caches
    in Hibernate. The shared data cache is not the persistence context cache, which
    Hibernate never shares between application threads. For reasons explained in section
    10.1.2, this isn’t optional. We call the persistence context a first-level cache. The shared
    data cache—the second-level cache—is optional,

    Hibernate has several types of shared caches available. You may use a cache to avoid a database hit
    whenever the following take place:
         The application performs an entity instance lookup by identifier (primary key);
            this may get a hit in the "entity data cache". Initializing an entity proxy on demand
            is the same operation and, internally, may hit the entity data cache instead of
            the database. The cache key is the identifier value of the entity instance, and
            the cache value is the data of the entity instance (its property values). The
            actual data is stored in a special disassembled format, and Hibernate assembles
            an entity instance again when it reads from the entity data cache.
         The persistence engine initializes a collection lazily; a "collection cache" may hold
            the elements of the collection. The cache key is the collection role: for example,
            “Item[1234]#bids” would be the bids collection of an Item instance with
            identifier 1234. The cache value in this case would be a set of Bid identifier values, the
            elements of the collection. (Note that this collection cache does not
            hold the Bid entity data, only the data’s identifier values!)
         The application performs an entity instance lookup by a unique key attribute.
            This is a special "natural identifier cache" for entity classes with unique properties:
            for example, User#username. The cache key is the unique property, such as the
            username, and the cached value is the User’s entity instance identifier.
         You execute a JPQL, criteria, or SQL query, and the result of the actual SQL
            query is already stored in the "query result cache". The cache key is the rendered
            SQL statement including all its parameter values, and the cache value is some
            representation of the SQL result set, which may include entity identifier values.

!!!!
    It’s critically important to understand that the entity data cache is the only type of
    cache that holds actual entity data values. The other three cache types only hold entity
    identifier information. Therefore, it doesn’t make sense to enable the natural identifier cache, for
    example, without also enabling the entity data cache. A lookup in the natural identifier cache will,
    when a match is found, always involve a lookup in the entity data cache

THE SECOND-LEVEL CACHE

!!!
    You can see the various elements of Hibernate’s caching system in figure 20.1. The
    first-level cache is the persistence context cache, which we discussed in section 10.1.2.
    Hibernate does not share this cache between threads; each application thread has its
    own copy of the data in this cache. Hence, there are no issues with transaction isolation and concurrency
    when accessing this cache.
    The second-level cache system in Hibernate may be process-scoped in the JVM or
    may be a cache system that can work in a cluster of JVMs. Multiple application threads
    may access the shared second-level caches concurrently.

    If an application does not have exclusive access to the database, shared caching should
    only be used for data that changes rarely and for which a small window of inconsistency is acceptable after an update.
    When another application updates the database, your cache may contain stale data until it expires. The
    other application may be a database-triggered stored procedure or even an ON DELETE or ON UPDATE foreign key
    option. There is no way for Hibernate’s cache system to know when another application or trigger updates the
    data in the database;

    Let’s assume for a moment that your application has exclusive access to the database. Even then, you must ask the
    same questions as a shared cache makes data retrieved from the database in one transaction visible to another
    transaction. What transaction isolation guarantees should the shared cache provide? The shared cache
    will affect the isolation level of your transactions, whether you read only committed
    data or if reads are repeatable

!!!
     Start this design process with a diagram of your domain model, and look at the entity classes. Good candidates for
     caching are classes that represent
         Data that changes rarely
         Noncritical data (for example, content-management data)
         Data that’s local to the application and not modified by other applications
    Bad candidates include
         Data that is updated often
         Financial data, where decisions must be based on the latest update
         Data that is shared with and/or written by other applications

    These aren’t the only rules we usually apply. Many applications have a number of classes with the following properties:
         A small number of instances (thousands, not millions) that all fit into memory
         Each instance referenced by many instances of another class or classes
         Instances that are rarely (or never) updated
    We sometimes call this kind of data reference data. Examples of reference data are Zip codes, locations, static
    text messages, and so on. Reference data is an excellent candidate for shared caching, and any application that
    uses reference data heavily will benefit greatly from caching that data

    You must exercise careful judgment for each class and collection for which you
    want to enable caching. You have to decide which concurrency strategy to use.


SELECTING A CACHE CONCURRENCY STRATEGY
    A cache concurrency strategy is a mediator: it’s responsible for storing items of data in
    the cache and retrieving them from the cache. This important role defines the transaction isolation semantics for
    that particular item. You’ll have to decide, for each persistent class and collection, which cache concurrency strategy
    to use if you want to enable the shared cache.
    The four built-in Hibernate concurrency strategies represent decreasing levels of strictness in terms of transaction
    isolation:
        TRANSACTIONAL—Available only in environments with a system transaction manager, this strategy guarantees full
            transactional isolation up to repeatable read, if supported by the cache provider. With this strategy, Hibernate
            assumes that the cache provider is aware of and participating in system transactions. Hibernate
            doesn’t perform any kind of locking or version checking; it relies solely on the
            cache provider’s ability to isolate data in concurrent transactions. Use this strategy for read-mostly data where
            it’s critical to prevent stale data in concurrent transactions, in the rare case of an update. This strategy also
            works in a cluster if the cache provider engine supports synchronous distributed caching.
         READ_WRITE—Maintains read committed isolation where Hibernate can use a
            time-stamping mechanism; hence, this strategy only works in a non-clustered
            environment. Hibernate may also use a proprietary locking API offered by the
            cache provider. Enable this strategy for read-mostly data where it’s critical to
            prevent stale data in concurrent transactions, in the rare case of an update. You
            shouldn’t enable this strategy if data is concurrently modified (by other applications) in the database.
         NONSTRICT_READ_WRITE—Makes no guarantee of consistency between the
            cache and the database. A transaction may read stale data from the cache. Use
            this strategy if data hardly ever changes (say, not every 10 seconds) and a window of inconsistency isn’t of
            critical concern. You configure the duration of the
            inconsistency window with the expiration policies of your cache provider. This
            strategy is usable in a cluster, even with asynchronous distributed caching. It
            may be appropriate if other applications change data in the same database.
         READ_ONLY—Suitable for data that never changes. You get an exception if you trigger an update. Use it for
            reference data only.

        With decreasing strictness come increasing performance and scalability. A clustered
        asynchronous cache with NONSTRICT_READ_WRITE can handle many more transactions
        than a synchronous cluster with TRANSACTIONAL


Enabling entity and collection caching

    The @Cacheable annotation enables the shared cache for this entity class, but a Hibernate annotation is necessary to
    pick the concurrency strategy. Hibernate stores and loads User entity data in the second-level cache, in a cache
    region named your.package.name.User. You can override the name with the region attribute of the @Cache annotation.
    (Alternatively, you can set a global region name prefix with the hibernate.cache.region_prefix property in the
    persistence unit.)

!!!!
    It’s critical to remember that the collection cache will not contain the actual Bid data.
    The collection cache only holds a set of Bid identifier values. Therefore, you must
    enable caching for the Bid entity as well. Otherwise, Hibernate may hit the cache
    when you start iterating through Item#bids, but then, due to cache misses, load each
    Bid separately from the database. This is a case where enabling the cache will result in
    more load on your database server


Setting cache modes

    JPA standardizes control of the shared cache with several cache modes. The following
    EntityManager#find() operation, for example, doesn’t attempt a cache lookup and
    hits the database directly:

            Map<String, Object> properties = new HashMap<String, Object>();
            properties.put("javax.persistence.cache.retrieveMode",
            CacheRetrieveMode.BYPASS);
            Item item = em.find(Item.class, ITEM_ID, properties);

    A more common usage of cache modes is the CacheStoreMode. By default, Hibernate puts entity data in the cache when you call EntityManager#persist(). It also
    puts data in the cache when you load an entity instance from the database. But if you
    store or load a large number of entity instances, you may not want to fill up the available cache. This is especially important for batch procedures, as we showed earlier in
    this chapter.
    You can disable storage of data in the shared entity cache for the entire unit of work by setting a CacheStoreMode
    on the EntityManager:

        em.setProperty("javax.persistence.cache.storeMode", CacheStoreMode.BYPASS);
        Item item = new Item(
            // ...
        );
        em.persist(item);               // will not be stored in cache

    Let’s look at the special cache mode CacheStoreMode.REFRESH. When you load an
    entity instance from the database with the default CacheStoreMode.USE, Hibernate
    first asks the cache whether it already has the data of the loaded entity instance. Then,
    if the cache already contains the data, Hibernate doesn’t put the loaded data into the
    cache. This avoids a cache write, assuming that cache reads are cheaper. With the
    REFRESH mode, Hibernate always puts loaded data into the cache without first querying the cache.


 Controlling the shared cache
    The standard JPA interface for controlling the caches is the Cache API:
        EntityManagerFactory emf = JPA.getEntityManagerFactory();
        Cache cache = emf.getCache();
        assertTrue(cache.contains(Item.class, ITEM_ID));
        cache.evict(Item.class, ITEM_ID);
        cache.evict(Item.class);
        cache.evictAll();
    This is a simple API, and it only allows you to access cache regions of entity data. You
    need the org.hibernate.Cache API to access the other cache regions, such as the collection and natural identifier
    cache regions

The query result cache
    The query result cache is by default disabled, and every JPA, criteria, or native SQL
    query you write always hits the database first. In this section, we show you why Hibernate disables the query cache
    by default and then how to enable it for particular queries when needed.

    You have to enable caching for a particular query. Without the org.hibernate.cachable hint, the result won’t be
    stored in the query result cache

    only the ID value of each ITEM record is stored in the query result cache. The property values of each Item are stored
    in the entity cache region.

!!!
    Now, when you execute the same query again, with the same argument values for
    its parameters, Hibernate first accesses the query result cache. It retrieves the identifier values of the ITEM
    records from the cache region for query results. Then, Hibernate looks up and assembles each Item entity instance
    by identifier from the entity cache region. If you query for entities and decide to enable caching, make sure you
    also enable regular data caching for these entities. If you don’t, you may end up with
    more database hits after enabling the query result cache!

!!!
    The majority of queries don’t benefit from result caching. This may come as a surprise. After all, it sounds like
    avoiding a database hit is always a good thing. There are two good reasons this doesn’t always work for arbitrary
    queries, compared to entity retrieval by identifier or collection initialization.
        First, you must ask how often you’re going to execute the same query repeatedly,
        with the same arguments. Granted, your application may execute a few queries repeatedly with exactly the same
        arguments bound to parameters and the same automatically generated SQL statement. We consider this a rare case,
         but when you’re certain you’re executing a query repeatedly, it becomes a good candidate for result set caching.
        Second, for applications that perform many queries and few inserts, deletes, or updates, caching query results
        can improve performance and scalability. On the other hand, if the application performs many writes, Hibernate
        won’t use the query result cache efficiently. Hibernate expires a cached query result set when there is any insert,
        update, or delete of any row of a table that appears in the cached query result. This
        means cached results may have a short lifetime, and even if you execute a query
        repeatedly, Hibernate won’t use cached results due to concurrent modifications of
        rows in the tables referenced by the query.

Summary
         You saw options you need to scale up your application and handle many concurrent users and larger data sets.
         With bulk UPDATE and DELETE operations, you can modify data directly in the database and still benefit from
        JPQL and criteria APIs without falling back to SQL.
         You learned about batch operations that let you work with large numbers of records in the application tier.
         We discussed the Hibernate caching system in detail: how you can selectively enable and optimize shared
            caching of entity, collection, and query result data.
         You configured Ehcache as a cache provider and learned how to peek under the hood with the Hibernate
            statistics API.


Useful Q&As:

1: https://stackoverflow.com/questions/26327274/do-you-need-a-database-transaction-for-reading-data
2: https://stackoverflow.com/questions/4270950/compile-time-vs-run-time-dependency-java/4270989
	Compile-time dependency: You need the dependency in your CLASSPATH to compile your artifact. They are produced because
	you have some kind of "reference" to the dependency hardcoded in your code, such as calling new for some class, extending
	or implementing something (either directly or indirectly), or a method call using the direct reference.method() notation.

	Run-time dependency: You need the dependency in your CLASSPATH to run your artifact. They are produced because you
	execute code that accesses the dependency (either in a hardcoded way or via reflection or whatever).
	
	